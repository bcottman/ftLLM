{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63eabc79",
   "metadata": {},
   "source": [
    "## QA finetuning using text from a website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211a10b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e60d2463",
   "metadata": {},
   "source": [
    "## Scrape a website for a text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c00ac9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "498e357d",
   "metadata": {},
   "source": [
    "## Transform  a text corpus into a question-answer pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa28e8b",
   "metadata": {},
   "source": [
    "###  transform a text corpus of statements into question answer pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7134167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "# Load the text corpus.\n",
    "with open(\"corpus.txt\", \"r\") as f:\n",
    "    corpus = f.read()\n",
    "\n",
    "# Split the text corpus into sentences.\n",
    "sentences = nltk.sent_tokenize(corpus)\n",
    "\n",
    "# Create a list of question answer pairs.\n",
    "qa_pairs = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    # Generate a question from the sentence.\n",
    "    question = \"What is \" + sentence.split()[0] + \"?\"\n",
    "\n",
    "    # Remove any punctuation from the question.\n",
    "    question = question.strip(\".,!?;:\")\n",
    "\n",
    "    # Add the question answer pair to the list of question answer pairs.\n",
    "    qa_pairs.append((question, sentence))\n",
    "\n",
    "# Shuffle the question answer pairs.\n",
    "random.shuffle(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d20e8d",
   "metadata": {},
   "source": [
    "n generate question-answer pairs from - Mojo is a programming language that is as easy to use as Python but with the performance of C++ and Rust. Furthermore, Mojo provides the ability to leverage the entire Python library ecosystem.\n",
    "\n",
    "Mojo achieves this feat by utilizing next-generation compiler technologies with integrated caching, multithreading, and cloud distribution technologies. Furthermore, Mojo’s autotuning and compile-time metaprogramming features allow you to write code that is portable to even the most exotic hardware.\n",
    "\n",
    "More importantly, Mojo allows you to leverage the entire Python ecosystem so you can continue to use tools you are familiar with. Mojo is designed to become a superset of Python over time by preserving Python’s dynamic features while adding new primitives for systems programming. These new system programming primitives will allow Mojo developers to build high-performance libraries that currently require C, C++, Rust, CUDA, and other accelerator systems. By bringing together the best of dynamic languages and systems languages, we hope to provide a unified programming model that works across levels of abstraction, is friendly for novice programmers, and scales across many use cases from accelerators through to application programming and scripting.\n",
    "\n",
    "This document is an introduction to the Mojo programming language, fit for consumption by Mojo programmers. It assumes knowledge of Python and systems programming concepts but it does not expect the reader to be a compiler nerd. At the moment, Mojo is still a work in progress and the documentation is targeted to developers with systems programming experience. As the language grows and becomes more broadly available, we intend for it to be friendly and accessible to everyone, including beginner programmers. It’s just not there today.\n",
    "\n",
    "Sure, I can generate question-answer pairs from the text you provided about Mojo. Here are a few examples:\n",
    "\n",
    "What is Mojo?\n",
    "Mojo is a programming language that is as easy to use as Python but with the performance of C++ and Rust.\n",
    "\n",
    "What are the benefits of using Mojo?\n",
    "Mojo provides the following benefits:\n",
    "\n",
    "It is as easy to use as Python.\n",
    "It has the performance of C++ and Rust.\n",
    "It allows you to leverage the entire Python library ecosystem.\n",
    "It is portable to even the most exotic hardware.\n",
    "What are the limitations of using Mojo?\n",
    "Mojo is still a work in progress and the documentation is targeted to developers with systems programming experience. As the language grows and becomes more broadly available, it is intended to be friendly and accessible to everyone, including beginner programmers.\n",
    "\n",
    "What are the future plans for Mojo?\n",
    "The future plans for Mojo include:\n",
    "\n",
    "Making it more user-friendly for beginners.\n",
    "Making it more portable to a wider range of hardware.\n",
    "Adding new features and capabilities.\n",
    "I hope this helps!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1674c12",
   "metadata": {},
   "source": [
    "### there are several Hugging Face RLHF models that can generate question-answer pairs from text using the model's API. \n",
    "\n",
    "T5 is a large language model that is trained on a massive dataset of text and code. It can generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way. T5 can also generate question-answer pairs from text using the model's API.\n",
    "\n",
    "BART is another large language model that is trained on a massive dataset of text and code. It can generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way. BART can also generate question-answer pairs from text using the model's API.\n",
    "\n",
    "QANet is a specialized language model that is trained on a dataset of question-answer pairs. It is particularly good at generating question-answer pairs from text using the model's API.\n",
    "SQuAD-BERT is a BERT-based model that is trained on a dataset of question-answer pairs from the SQuAD dataset. It is particularly good at generating question-answer pairs from text that are relevant to the SQuAD dataset using the model's API.\n",
    "To generate question-answer pairs from text using a Hugging Face RLHF model's API, you can use the following steps:\n",
    "\n",
    "Import the Hugging Face library.\n",
    "Create a Hugging Face model object.\n",
    "Set the model's prompt and max_length parameters.\n",
    "Call the model's generate() method.\n",
    "The model will generate a list of question-answer pairs.\n",
    "You can then use these question-answer pairs for a variety of purposes, such as:\n",
    "\n",
    "Answering questions from users.\n",
    "Generating quiz questions.\n",
    "Creating educational content.\n",
    "Improving the performance of a question-answering system.\n",
    "Here is an example of how to generate question-answer pairs from text using the T5 model's API:\n",
    "\n",
    "Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ad81c8",
   "metadata": {},
   "source": [
    "\n",
    "The maximum length of the max_length parameter for the HFT5 large language model API is 512 tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31135a78",
   "metadata": {},
   "source": [
    "here are a few ways to count the number of tokens in text input in the HF T5 large language model API. One way is to use the len() function. The len() function takes a string as input and returns the number of characters in the string. To count the number of tokens in a string, you can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad010e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# Create a T5 model object\n",
    "model = transformers.AutoModelForSeq2SeqLM(\"t5-base\")\n",
    "\n",
    "# Set the model's prompt and max_length parameters\n",
    "prompt = \"What is the capital of France?\"\n",
    "max_length = 128\n",
    "\n",
    "# Call the model's generate() method\n",
    "outputs = model.generate(prompt=prompt, max_length=max_length)\n",
    "\n",
    "# Count the number of tokens in the output\n",
    "num_tokens = len(outputs[0][\"generated_text\"])\n",
    "\n",
    "# Print the number of tokens\n",
    "print(\"Number of tokens:\", num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815454db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import transformers\n",
    "\n",
    "# Create a T5 model object\n",
    "model = transformers.AutoModelForSeq2SeqLM(\"t5-base\")\n",
    "\n",
    "# Set the model's prompt and max_length parameters\n",
    "prompt = \"What is the capital of France?\"\n",
    "max_length = 128\n",
    "\n",
    "# Call the model's generate() method\n",
    "outputs = model.generate(prompt=prompt, max_length=max_length)\n",
    "\n",
    "# Print the question and answer\n",
    "print(\"Question:\", outputs[0][\"generated_text\"])\n",
    "print(\"Answer:\", outputs[1][\"generated_text\"])\n",
    "\n",
    "\n",
    "# Question: What is the capital of France?\n",
    "# Answer: Paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31eea73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1adf89f7",
   "metadata": {},
   "source": [
    "# use question-answer pair to finetune StarCoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6288c9bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2c4ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f633b07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704bdf30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
