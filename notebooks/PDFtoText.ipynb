{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff6478ae",
   "metadata": {},
   "source": [
    "# finetune a LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608067b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T19:12:33.125685Z",
     "start_time": "2023-05-04T19:12:33.121785Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7775c62f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T16:36:06.000596Z",
     "start_time": "2023-05-19T16:36:05.840465Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.11\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c948735",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T16:36:06.009883Z",
     "start_time": "2023-05-19T16:36:06.005434Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/brucecottman/Documents/pyProjects/ftLLM/notebooks', '/usr/local/Cellar/apache-spark/2.0.1/libexec/python', '/Users/brucecottman/Documents/pyProjects/ftLLM/notebooks/$', '/usr/local/Cellar/python@3.10/3.10.11/Frameworks/Python.framework/Versions/3.10/lib/python310.zip', '/usr/local/Cellar/python@3.10/3.10.11/Frameworks/Python.framework/Versions/3.10/lib/python3.10', '/usr/local/Cellar/python@3.10/3.10.11/Frameworks/Python.framework/Versions/3.10/lib/python3.10/lib-dynload', '', '/Users/brucecottman/Documents/pyProjects/ftLLM/venv/lib/python3.10/site-packages', '/usr/local/lib/python3.10/site-packages']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import sys, os\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e69c68b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T16:36:12.749004Z",
     "start_time": "2023-05-19T16:36:06.012917Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# packages in environment at /Users/brucecottman/anaconda3:',\n",
       " '#',\n",
       " '# Name                    Version                   Build  Channel',\n",
       " 'abseil-cpp                20211102.0           h96cf925_1    conda-forge',\n",
       " 'alabaster                 0.7.12             pyhd3eb1b0_0  ',\n",
       " 'altair                    4.2.2              pyhd8ed1ab_0    conda-forge',\n",
       " 'anaconda-client           1.11.2          py310hecd8cb5_0  ',\n",
       " 'anaconda-navigator        2.4.0           py310hecd8cb5_0  ',\n",
       " 'anaconda-project          0.11.1          py310hecd8cb5_0  ',\n",
       " 'anyio                     3.5.0           py310hecd8cb5_0  ',\n",
       " 'appdirs                   1.4.4              pyhd3eb1b0_0  ',\n",
       " 'applaunchservices         0.3.0           py310hecd8cb5_0  ',\n",
       " 'appnope                   0.1.2           py310hecd8cb5_1001  ',\n",
       " 'appscript                 1.1.2           py310hca72f7f_0  ',\n",
       " 'argon2-cffi               21.3.0             pyhd3eb1b0_0  ',\n",
       " 'argon2-cffi-bindings      21.2.0          py310hca72f7f_0  ',\n",
       " 'arrow                     1.2.3           py310hecd8cb5_1  ',\n",
       " 'arrow-cpp                 11.0.0          py310hfdcd655_0  ',\n",
       " 'astroid                   2.14.2          py310hecd8cb5_0  ',\n",
       " 'astropy                   5.1             py310h4e76f89_0  ',\n",
       " 'asttokens                 2.0.5              pyhd3eb1b0_0  ',\n",
       " 'atomicwrites              1.4.0                      py_0  ',\n",
       " 'attrs                     22.1.0          py310hecd8cb5_0  ',\n",
       " 'automat                   20.2.0                     py_0  ',\n",
       " 'autopep8                  1.6.0              pyhd3eb1b0_1  ',\n",
       " 'aws-c-common              0.4.57               hb1e8313_1  ',\n",
       " 'aws-c-event-stream        0.1.6                h23ab428_5  ',\n",
       " 'aws-checksums             0.1.9                hb1e8313_0  ',\n",
       " 'aws-sdk-cpp               1.8.185              he271ece_0  ',\n",
       " 'babel                     2.11.0          py310hecd8cb5_0  ',\n",
       " 'backcall                  0.2.0              pyhd3eb1b0_0  ',\n",
       " 'backports                 1.1                pyhd3eb1b0_0  ',\n",
       " 'backports.functools_lru_cache 1.6.4              pyhd3eb1b0_0  ',\n",
       " 'backports.tempfile        1.0                pyhd3eb1b0_1  ',\n",
       " 'backports.weakref         1.0.post1                  py_1  ',\n",
       " 'bcrypt                    3.2.0           py310hca72f7f_1  ',\n",
       " 'beautifulsoup4            4.11.1          py310hecd8cb5_0  ',\n",
       " 'binaryornot               0.4.4              pyhd3eb1b0_1  ',\n",
       " 'black                     22.6.0          py310hecd8cb5_0  ',\n",
       " 'blas                      1.0                    openblas  ',\n",
       " 'bleach                    4.1.0              pyhd3eb1b0_0  ',\n",
       " 'blinker                   1.6.2              pyhd8ed1ab_0    conda-forge',\n",
       " 'blosc                     1.21.3               hcec6c5f_0  ',\n",
       " 'bokeh                     2.4.3           py310hecd8cb5_0  ',\n",
       " 'boltons                   23.0.0          py310hecd8cb5_0  ',\n",
       " 'boost-cpp                 1.70.0               hd59e818_1    conda-forge',\n",
       " 'bottleneck                1.3.5           py310h4e76f89_0  ',\n",
       " 'brotli                    1.0.9                hca72f7f_7  ',\n",
       " 'brotli-bin                1.0.9                hca72f7f_7  ',\n",
       " 'brotlipy                  0.7.0           py310hca72f7f_1002  ',\n",
       " 'brunsli                   0.1                  h23ab428_0  ',\n",
       " 'bzip2                     1.0.8                h1de35cc_0  ',\n",
       " 'c-ares                    1.18.1               hca72f7f_0  ',\n",
       " 'ca-certificates           2022.12.7            h033912b_0    conda-forge',\n",
       " 'cachetools                5.3.0              pyhd8ed1ab_0    conda-forge',\n",
       " 'catalogue                 2.0.8           py310h2ec42d9_1    conda-forge',\n",
       " 'cctools                   949.0.1             h9abeeb2_25  ',\n",
       " 'cctools_osx-64            949.0.1             hc7db93f_25  ',\n",
       " 'certifi                   2022.12.7          pyhd8ed1ab_0    conda-forge',\n",
       " 'cffi                      1.15.1          py310h6c40b1e_3  ',\n",
       " 'cfitsio                   3.470                hbd21bf8_7  ',\n",
       " 'chardet                   4.0.0           py310hecd8cb5_1003  ',\n",
       " 'charls                    2.2.0                h23ab428_0  ',\n",
       " 'charset-normalizer        2.0.4              pyhd3eb1b0_0  ',\n",
       " 'click                     8.0.4           py310hecd8cb5_0  ',\n",
       " 'cloudpickle               2.0.0              pyhd3eb1b0_0  ',\n",
       " 'clyent                    1.2.2           py310hecd8cb5_1  ',\n",
       " 'colorama                  0.4.6           py310hecd8cb5_0  ',\n",
       " 'colorcet                  3.0.1           py310hecd8cb5_0  ',\n",
       " 'comm                      0.1.2           py310hecd8cb5_0  ',\n",
       " 'conda                     23.3.1          py310hecd8cb5_0  ',\n",
       " 'conda-build               3.24.0          py310hecd8cb5_0  ',\n",
       " 'conda-content-trust       0.1.3           py310hecd8cb5_0  ',\n",
       " 'conda-pack                0.6.0              pyhd3eb1b0_0  ',\n",
       " 'conda-package-handling    2.0.2           py310hecd8cb5_0  ',\n",
       " 'conda-package-streaming   0.7.0           py310hecd8cb5_0  ',\n",
       " 'conda-repo-cli            1.0.41          py310hecd8cb5_0  ',\n",
       " 'conda-token               0.4.0              pyhd3eb1b0_0  ',\n",
       " 'conda-verify              3.4.2                      py_1  ',\n",
       " 'constantly                15.1.0          py310hecd8cb5_0  ',\n",
       " 'contourpy                 1.0.5           py310haf03e11_0  ',\n",
       " 'cookiecutter              1.7.3              pyhd3eb1b0_0  ',\n",
       " 'cryptography              39.0.1          py310hf6deb26_0  ',\n",
       " 'cssselect                 1.1.0              pyhd3eb1b0_0  ',\n",
       " 'curl                      7.87.0               h6c40b1e_0  ',\n",
       " 'cycler                    0.11.0             pyhd3eb1b0_0  ',\n",
       " 'cymem                     2.0.7           py310h7a76584_1    conda-forge',\n",
       " 'cython-blis               0.7.9           py310h936d966_1    conda-forge',\n",
       " 'cytoolz                   0.12.0          py310hca72f7f_0  ',\n",
       " 'dask                      2022.7.0        py310hecd8cb5_0  ',\n",
       " 'dask-core                 2022.7.0        py310hecd8cb5_0  ',\n",
       " 'dataclasses               0.8                pyhc8e2a94_3    conda-forge',\n",
       " 'datashader                0.14.4          py310hecd8cb5_0  ',\n",
       " 'datashape                 0.5.4           py310hecd8cb5_1  ',\n",
       " 'debugpy                   1.5.1           py310he9d5cce_0  ',\n",
       " 'decorator                 5.1.1              pyhd3eb1b0_0  ',\n",
       " 'defusedxml                0.7.1              pyhd3eb1b0_0  ',\n",
       " 'diff-match-patch          20200713           pyhd3eb1b0_0  ',\n",
       " 'dill                      0.3.6           py310hecd8cb5_0  ',\n",
       " 'distributed               2022.7.0        py310hecd8cb5_0  ',\n",
       " 'docstring-to-markdown     0.11            py310hecd8cb5_0  ',\n",
       " 'docutils                  0.18.1          py310hecd8cb5_3  ',\n",
       " 'entrypoints               0.4             py310hecd8cb5_0  ',\n",
       " 'et_xmlfile                1.1.0           py310hecd8cb5_0  ',\n",
       " 'executing                 0.8.3              pyhd3eb1b0_0  ',\n",
       " 'filelock                  3.9.0           py310hecd8cb5_0  ',\n",
       " 'flake8                    6.0.0           py310hecd8cb5_0  ',\n",
       " 'flask                     2.2.2           py310hecd8cb5_0  ',\n",
       " 'flit-core                 3.6.0              pyhd3eb1b0_0  ',\n",
       " 'fonttools                 4.25.0             pyhd3eb1b0_0  ',\n",
       " 'freetype                  2.12.1               hd8bbffd_0  ',\n",
       " 'fsspec                    2022.11.0       py310hecd8cb5_0  ',\n",
       " 'future                    0.18.3          py310hecd8cb5_0  ',\n",
       " 'gensim                    4.3.0           py310h3ea8b11_0  ',\n",
       " 'gettext                   0.21.0               h7535e17_0  ',\n",
       " 'gflags                    2.2.2             hb1e8313_1004    conda-forge',\n",
       " 'giflib                    5.2.1                h6c40b1e_3  ',\n",
       " 'gitdb                     4.0.10             pyhd8ed1ab_0    conda-forge',\n",
       " 'gitpython                 3.1.31             pyhd8ed1ab_0    conda-forge',\n",
       " 'glib                      2.69.1               hfff2838_2  ',\n",
       " 'glob2                     0.7                pyhd3eb1b0_0  ',\n",
       " 'glog                      0.5.0                h25b26a9_0    conda-forge',\n",
       " 'gmp                       6.2.1                he9d5cce_3  ',\n",
       " 'gmpy2                     2.1.2           py310hd5de756_0  ',\n",
       " 'greenlet                  2.0.1           py310hcec6c5f_0  ',\n",
       " 'grpc-cpp                  1.46.1               h64d96ca_1  ',\n",
       " 'gst-plugins-base          1.14.1               hcec6c5f_1  ',\n",
       " 'gstreamer                 1.14.1               h6c40b1e_1  ',\n",
       " 'h5py                      3.7.0           py310h6c517f8_0  ',\n",
       " 'hdf5                      1.10.6               h10fe05b_1  ',\n",
       " 'heapdict                  1.0.1              pyhd3eb1b0_0  ',\n",
       " 'holoviews                 1.15.4          py310hecd8cb5_0  ',\n",
       " 'huggingface_hub           0.10.1          py310hecd8cb5_0  ',\n",
       " 'hvplot                    0.8.2           py310hecd8cb5_0  ',\n",
       " 'hyperlink                 21.0.0             pyhd3eb1b0_0  ',\n",
       " 'icu                       58.2                 h0a44026_3  ',\n",
       " 'idna                      3.4             py310hecd8cb5_0  ',\n",
       " 'imagecodecs               2021.8.26       py310hf5cf8d7_2  ',\n",
       " 'imageio                   2.26.0          py310hecd8cb5_0  ',\n",
       " 'imagesize                 1.4.1           py310hecd8cb5_0  ',\n",
       " 'imbalanced-learn          0.10.1          py310hecd8cb5_0  ',\n",
       " 'importlib-metadata        4.11.3          py310hecd8cb5_0  ',\n",
       " 'importlib_metadata        4.11.3               hd3eb1b0_0  ',\n",
       " 'incremental               21.3.0             pyhd3eb1b0_0  ',\n",
       " 'inflection                0.5.1           py310hecd8cb5_0  ',\n",
       " 'iniconfig                 1.1.1              pyhd3eb1b0_0  ',\n",
       " 'intake                    0.6.7           py310hecd8cb5_0  ',\n",
       " 'intervaltree              3.1.0              pyhd3eb1b0_0  ',\n",
       " 'ipykernel                 6.19.2          py310h20db666_0  ',\n",
       " 'ipython                   8.10.0          py310hecd8cb5_0  ',\n",
       " 'ipython_genutils          0.2.0              pyhd3eb1b0_1  ',\n",
       " 'ipywidgets                7.6.5              pyhd3eb1b0_1  ',\n",
       " 'isort                     5.9.3              pyhd3eb1b0_0  ',\n",
       " 'itemadapter               0.3.0              pyhd3eb1b0_0  ',\n",
       " 'itemloaders               1.0.4              pyhd3eb1b0_1  ',\n",
       " 'itsdangerous              2.0.1              pyhd3eb1b0_0  ',\n",
       " 'jedi                      0.18.1          py310hecd8cb5_1  ',\n",
       " 'jellyfish                 0.9.0           py310hca72f7f_0  ',\n",
       " 'jinja2                    3.1.2           py310hecd8cb5_0  ',\n",
       " 'jinja2-time               0.2.0              pyhd3eb1b0_3  ',\n",
       " 'jmespath                  0.10.0             pyhd3eb1b0_0  ',\n",
       " 'joblib                    1.1.1           py310hecd8cb5_0  ',\n",
       " 'jpeg                      9e                   h6c40b1e_1  ',\n",
       " 'jq                        1.6               h9ed2024_1000  ',\n",
       " 'json5                     0.9.6              pyhd3eb1b0_0  ',\n",
       " 'jsonpatch                 1.32               pyhd3eb1b0_0  ',\n",
       " 'jsonpointer               2.1                pyhd3eb1b0_0  ',\n",
       " 'jsonschema                4.17.3          py310hecd8cb5_0  ',\n",
       " 'jupyter                   1.0.0           py310hecd8cb5_8  ',\n",
       " 'jupyter_client            7.3.4           py310hecd8cb5_0  ',\n",
       " 'jupyter_console           6.6.2           py310hecd8cb5_0  ',\n",
       " 'jupyter_core              5.2.0           py310hecd8cb5_0  ',\n",
       " 'jupyter_server            1.23.4          py310hecd8cb5_0  ',\n",
       " 'jupyterlab                3.5.3           py310hecd8cb5_0  ',\n",
       " 'jupyterlab_pygments       0.1.2                      py_0  ',\n",
       " 'jupyterlab_server         2.19.0          py310hecd8cb5_0  ',\n",
       " 'jupyterlab_widgets        1.0.0              pyhd3eb1b0_1  ',\n",
       " 'jxrlib                    1.1                  haf1e3a3_2  ',\n",
       " 'keyring                   23.4.0          py310hecd8cb5_0  ',\n",
       " 'kiwisolver                1.4.4           py310hcec6c5f_0  ',\n",
       " 'krb5                      1.19.4               hdba6334_0  ',\n",
       " 'langcodes                 3.3.0              pyhd8ed1ab_0    conda-forge',\n",
       " 'lazy-object-proxy         1.6.0           py310hca72f7f_0  ',\n",
       " 'lcms2                     2.12                 hf1fd2bf_0  ',\n",
       " 'ld64                      530                 h20443b4_25  ',\n",
       " 'ld64_osx-64               530                 h70f3046_25  ',\n",
       " 'ldid                      2.1.2                h2d21305_2  ',\n",
       " 'lerc                      3.0                  he9d5cce_0  ',\n",
       " 'libaec                    1.0.4                hb1e8313_1  ',\n",
       " 'libarchive                3.6.2                h65c5294_0  ',\n",
       " 'libbrotlicommon           1.0.9                hca72f7f_7  ',\n",
       " 'libbrotlidec              1.0.9                hca72f7f_7  ',\n",
       " 'libbrotlienc              1.0.9                hca72f7f_7  ',\n",
       " 'libclang                  12.0.0          default_hbc2896b_2  ',\n",
       " 'libcurl                   7.87.0               ha585b31_0  ',\n",
       " 'libcxx                    14.0.6               h9765a3e_0  ',\n",
       " 'libdeflate                1.17                 hb664fd8_0  ',\n",
       " 'libedit                   3.1.20221030         h6c40b1e_0  ',\n",
       " 'libev                     4.33                 h9ed2024_1  ',\n",
       " 'libevent                  2.1.10               h815e4d9_4    conda-forge',\n",
       " 'libffi                    3.4.2                hecd8cb5_6  ',\n",
       " 'libgfortran               5.0.0           11_3_0_hecd8cb5_28  ',\n",
       " 'libgfortran5              11.3.0              h9dfd629_28  ',\n",
       " 'libiconv                  1.16                 hca72f7f_2  ',\n",
       " 'liblief                   0.12.3               hcec6c5f_0  ',\n",
       " 'libllvm11                 11.1.0               h46f1229_6  ',\n",
       " 'libllvm12                 12.0.0               h9b2ccf5_3  ',\n",
       " 'libllvm14                 14.0.6               h91fad77_2  ',\n",
       " 'libnghttp2                1.46.0               ha29bfda_0  ',\n",
       " 'libopenblas               0.3.21               h54e7dc3_0  ',\n",
       " 'libpng                    1.6.39               h6c40b1e_0  ',\n",
       " 'libpq                     12.9                 h1c9f633_3  ',\n",
       " 'libprotobuf               3.20.3               hfff2838_0  ',\n",
       " 'libsodium                 1.0.18               h1de35cc_0  ',\n",
       " 'libspatialindex           1.9.3                h23ab428_0  ',\n",
       " 'libssh2                   1.10.0               h0a4fc7d_0  ',\n",
       " 'libthrift                 0.15.0               h054ceb0_0  ',\n",
       " 'libtiff                   4.5.0                hcec6c5f_2  ',\n",
       " 'libuv                     1.44.2               h6c40b1e_0  ',\n",
       " 'libwebp                   1.2.4                hf6ce154_1  ',\n",
       " 'libwebp-base              1.2.4                h6c40b1e_1  ',\n",
       " 'libxml2                   2.9.14               hbf8cd5e_0  ',\n",
       " 'libxslt                   1.1.35               h5b33f42_0  ',\n",
       " 'libzopfli                 1.0.3                hb1e8313_0  ',\n",
       " 'llvm-openmp               14.0.6               h0dcd299_0  ',\n",
       " 'llvmlite                  0.39.1          py310h8346a28_0  ',\n",
       " 'locket                    1.0.0           py310hecd8cb5_0  ',\n",
       " 'lxml                      4.9.1           py310h65b224f_0  ',\n",
       " 'lz4                       3.1.3           py310hca72f7f_0  ',\n",
       " 'lz4-c                     1.9.4                hcec6c5f_0  ',\n",
       " 'lzo                       2.10                 haf1e3a3_2  ',\n",
       " 'markdown                  3.4.1           py310hecd8cb5_0  ',\n",
       " 'markdown-it-py            2.2.0              pyhd8ed1ab_0    conda-forge',\n",
       " 'markupsafe                2.1.1           py310hca72f7f_0  ',\n",
       " 'matplotlib                3.7.0           py310hecd8cb5_0  ',\n",
       " 'matplotlib-base           3.7.0           py310ha533b9c_0  ',\n",
       " 'matplotlib-inline         0.1.6           py310hecd8cb5_0  ',\n",
       " 'mccabe                    0.7.0              pyhd3eb1b0_0  ',\n",
       " 'mdurl                     0.1.0              pyhd8ed1ab_0    conda-forge',\n",
       " 'mistune                   0.8.4           py310hca72f7f_1000  ',\n",
       " 'mock                      4.0.3              pyhd3eb1b0_0  ',\n",
       " 'mpc                       1.1.0                h6ef4df4_1  ',\n",
       " 'mpfr                      4.0.2                h9066e36_1  ',\n",
       " 'mpmath                    1.2.1                    pypi_0    pypi',\n",
       " 'msgpack-python            1.0.3           py310haf03e11_0  ',\n",
       " 'multipledispatch          0.6.0           py310hecd8cb5_0  ',\n",
       " 'munkres                   1.1.4                      py_0  ',\n",
       " 'murmurhash                1.0.9           py310h7a76584_1    conda-forge',\n",
       " 'mypy_extensions           0.4.3           py310hecd8cb5_1  ',\n",
       " 'navigator-updater         0.3.0           py310hecd8cb5_0  ',\n",
       " 'nbclassic                 0.5.2           py310hecd8cb5_0  ',\n",
       " 'nbclient                  0.5.13          py310hecd8cb5_0  ',\n",
       " 'nbconvert                 6.5.4           py310hecd8cb5_0  ',\n",
       " 'nbformat                  5.7.0           py310hecd8cb5_0  ',\n",
       " 'ncurses                   6.4                  hcec6c5f_0  ',\n",
       " 'nest-asyncio              1.5.6           py310hecd8cb5_0  ',\n",
       " 'networkx                  2.8.4           py310hecd8cb5_0  ',\n",
       " 'ninja                     1.10.2               hecd8cb5_5  ',\n",
       " 'ninja-base                1.10.2               haf03e11_5  ',\n",
       " 'nltk                      3.7                pyhd3eb1b0_0  ',\n",
       " 'notebook                  6.5.2           py310hecd8cb5_0  ',\n",
       " 'notebook-shim             0.2.2           py310hecd8cb5_0  ',\n",
       " 'nspr                      4.33                 he9d5cce_0  ',\n",
       " 'nss                       3.74                 h47edf6a_0  ',\n",
       " 'numba                     0.56.4          py310h3ea8b11_0  ',\n",
       " 'numexpr                   2.8.4           py310he50c29a_0  ',\n",
       " 'numpy                     1.23.5          py310he50c29a_0  ',\n",
       " 'numpy-base                1.23.5          py310h992e150_0  ',\n",
       " 'numpydoc                  1.5.0           py310hecd8cb5_0  ',\n",
       " 'oniguruma                 6.9.7.1              h9ed2024_0  ',\n",
       " 'openjpeg                  2.4.0                h66ea3da_0  ',\n",
       " 'openpyxl                  3.0.10          py310hca72f7f_0  ',\n",
       " 'openssl                   1.1.1t               hfd90126_0    conda-forge',\n",
       " 'orc                       1.7.4                h995b336_1  ',\n",
       " 'packaging                 22.0            py310hecd8cb5_0  ',\n",
       " 'pandas                    1.5.3           py310h3ea8b11_0  ',\n",
       " 'pandocfilters             1.5.0              pyhd3eb1b0_0  ',\n",
       " 'panel                     0.14.3          py310hecd8cb5_0  ',\n",
       " 'param                     1.12.3          py310hecd8cb5_0  ',\n",
       " 'parsel                    1.6.0           py310hecd8cb5_0  ',\n",
       " 'parso                     0.8.3              pyhd3eb1b0_0  ',\n",
       " 'partd                     1.2.0              pyhd3eb1b0_1  ',\n",
       " 'patch                     2.7.6             h1de35cc_1001  ',\n",
       " 'pathlib                   1.0.1              pyhd3eb1b0_1  ',\n",
       " 'pathspec                  0.10.3          py310hecd8cb5_0  ',\n",
       " 'pathy                     0.10.1             pyhd8ed1ab_0    conda-forge',\n",
       " 'patsy                     0.5.3           py310hecd8cb5_0  ',\n",
       " 'pcre                      8.45                 h23ab428_0  ',\n",
       " 'pep8                      1.7.1           py310hecd8cb5_1  ',\n",
       " 'pexpect                   4.8.0              pyhd3eb1b0_3  ',\n",
       " 'pickleshare               0.7.5           pyhd3eb1b0_1003  ',\n",
       " 'pillow                    9.4.0           py310hcec6c5f_0  ',\n",
       " 'pip                       22.3.1          py310hecd8cb5_0  ',\n",
       " 'pkginfo                   1.9.6           py310hecd8cb5_0  ',\n",
       " 'platformdirs              2.5.2           py310hecd8cb5_0  ',\n",
       " 'plotly                    5.9.0           py310hecd8cb5_0  ',\n",
       " 'pluggy                    1.0.0           py310hecd8cb5_1  ',\n",
       " 'ply                       3.11            py310hecd8cb5_0  ',\n",
       " 'pooch                     1.4.0              pyhd3eb1b0_0  ',\n",
       " 'poyo                      0.5.0              pyhd3eb1b0_0  ',\n",
       " 'preshed                   3.0.8           py310h7a76584_1    conda-forge',\n",
       " 'prometheus_client         0.14.1          py310hecd8cb5_0  ',\n",
       " 'prompt-toolkit            3.0.36          py310hecd8cb5_0  ',\n",
       " 'prompt_toolkit            3.0.36               hd3eb1b0_0  ',\n",
       " 'protego                   0.1.16                     py_0  ',\n",
       " 'protobuf                  3.20.3          py310h7a76584_1    conda-forge',\n",
       " 'psutil                    5.9.0           py310hca72f7f_0  ',\n",
       " 'ptyprocess                0.7.0              pyhd3eb1b0_2  ',\n",
       " 'pure_eval                 0.2.2              pyhd3eb1b0_0  ',\n",
       " 'py                        1.11.0             pyhd3eb1b0_0  ',\n",
       " 'py-lief                   0.12.3          py310hcec6c5f_0  ',\n",
       " 'pyarrow                   11.0.0          py310he65a03e_0  ',\n",
       " 'pyasn1                    0.4.8              pyhd3eb1b0_0  ',\n",
       " 'pyasn1-modules            0.2.8                      py_0  ',\n",
       " 'pycodestyle               2.10.0          py310hecd8cb5_0  ',\n",
       " 'pycosat                   0.6.4           py310hca72f7f_0  ',\n",
       " 'pycparser                 2.21               pyhd3eb1b0_0  ',\n",
       " 'pyct                      0.5.0           py310hecd8cb5_0  ',\n",
       " 'pycurl                    7.45.1                   pypi_0    pypi',\n",
       " 'pydantic                  1.10.7          py310h90acd4f_0    conda-forge',\n",
       " 'pydeck                    0.8.0              pyhd8ed1ab_0    conda-forge',\n",
       " 'pydispatcher              2.0.5           py310hecd8cb5_2  ',\n",
       " 'pydocstyle                6.3.0           py310hecd8cb5_0  ',\n",
       " 'pyerfa                    2.0.0           py310hca72f7f_0  ',\n",
       " 'pyflakes                  3.0.1           py310hecd8cb5_0  ',\n",
       " 'pygments                  2.11.2             pyhd3eb1b0_0  ',\n",
       " 'pyhamcrest                2.0.2              pyhd3eb1b0_2  ',\n",
       " 'pyjwt                     2.4.0           py310hecd8cb5_0  ',\n",
       " 'pylint                    2.16.2          py310hecd8cb5_0  ',\n",
       " 'pylint-venv               2.3.0           py310hecd8cb5_0  ',\n",
       " 'pyls-spyder               0.4.0              pyhd3eb1b0_0  ',\n",
       " 'pympler                   1.0.1              pyhd8ed1ab_0    conda-forge',\n",
       " 'pyobjc-core               9.0             py310h9205ec4_1  ',\n",
       " 'pyobjc-framework-cocoa    9.0             py310h9205ec4_0  ',\n",
       " 'pyobjc-framework-coreservices 9.0             py310h46256e1_0  ',\n",
       " 'pyobjc-framework-fsevents 9.0             py310hecd8cb5_0  ',\n",
       " 'pyodbc                    4.0.34          py310he9d5cce_0  ',\n",
       " 'pyopenssl                 23.0.0          py310hecd8cb5_0  ',\n",
       " 'pyparsing                 3.0.9           py310hecd8cb5_0  ',\n",
       " 'pyqt                      5.15.7          py310he9d5cce_0  ',\n",
       " 'pyqt5-sip                 12.11.0                  pypi_0    pypi',\n",
       " 'pyqtwebengine             5.15.7          py310he9d5cce_0  ',\n",
       " 'pyrsistent                0.18.0          py310hca72f7f_0  ',\n",
       " 'pysocks                   1.7.1           py310hecd8cb5_0  ',\n",
       " 'pytables                  3.7.0           py310h59775c6_1  ',\n",
       " 'pytest                    7.1.2           py310hecd8cb5_0  ',\n",
       " 'python                    3.10.9               h218abb5_1  ',\n",
       " 'python-dateutil           2.8.2              pyhd3eb1b0_0  ',\n",
       " 'python-fastjsonschema     2.16.2          py310hecd8cb5_0  ',\n",
       " 'python-libarchive-c       2.9                pyhd3eb1b0_1  ',\n",
       " 'python-lsp-black          1.2.1           py310hecd8cb5_0  ',\n",
       " 'python-lsp-jsonrpc        1.0.0              pyhd3eb1b0_0  ',\n",
       " 'python-lsp-server         1.7.1           py310hecd8cb5_0  ',\n",
       " 'python-slugify            5.0.2              pyhd3eb1b0_0  ',\n",
       " 'python-snappy             0.6.1           py310hcec6c5f_0  ',\n",
       " 'python-tzdata             2023.3             pyhd8ed1ab_0    conda-forge',\n",
       " 'python.app                3               py310hca72f7f_0  ',\n",
       " 'python_abi                3.10                    2_cp310    conda-forge',\n",
       " 'pytoolconfig              1.2.5           py310hecd8cb5_1  ',\n",
       " 'pytorch                   1.12.1          cpu_py310h64f2f56_1  ',\n",
       " 'pytz                      2022.7          py310hecd8cb5_0  ',\n",
       " 'pytz-deprecation-shim     0.1.0.post0     py310h2ec42d9_3    conda-forge',\n",
       " 'pyviz_comms               2.0.2              pyhd3eb1b0_0  ',\n",
       " 'pywavelets                1.4.1           py310h6c40b1e_0  ',\n",
       " 'pyyaml                    6.0             py310h6c40b1e_1  ',\n",
       " 'pyzmq                     23.2.0          py310he9d5cce_0  ',\n",
       " 'qdarkstyle                3.0.2              pyhd3eb1b0_0  ',\n",
       " 'qstylizer                 0.2.2           py310hecd8cb5_0  ',\n",
       " 'qt-main                   5.15.2               h719ae48_7  ',\n",
       " 'qt-webengine              5.15.9               h90a370e_4  ',\n",
       " 'qtawesome                 1.2.2           py310hecd8cb5_0  ',\n",
       " 'qtconsole                 5.4.0           py310hecd8cb5_0  ',\n",
       " 'qtpy                      2.2.0           py310hecd8cb5_0  ',\n",
       " 'qtwebkit                  5.212                h24dc246_4  ',\n",
       " 'queuelib                  1.5.0           py310hecd8cb5_0  ',\n",
       " 're2                       2022.04.01           h96cf925_0    conda-forge',\n",
       " 'readline                  8.2                  hca72f7f_0  ',\n",
       " 'regex                     2022.7.9        py310hca72f7f_0  ',\n",
       " 'requests                  2.28.1          py310hecd8cb5_0  ',\n",
       " 'requests-file             1.5.1              pyhd3eb1b0_0  ',\n",
       " 'requests-toolbelt         0.9.1              pyhd3eb1b0_0  ',\n",
       " 'rich                      13.2.0             pyhd8ed1ab_1    conda-forge',\n",
       " 'ripgrep                   13.0.0               hc2228c6_0  ',\n",
       " 'rope                      1.7.0           py310hecd8cb5_0  ',\n",
       " 'rtree                     1.0.1           py310hecd8cb5_0  ',\n",
       " 'ruamel.yaml               0.17.21         py310hca72f7f_0  ',\n",
       " 'ruamel.yaml.clib          0.2.6           py310hca72f7f_1  ',\n",
       " 'ruamel_yaml               0.17.21         py310hca72f7f_0  ',\n",
       " 'scikit-image              0.19.3          py310hcec6c5f_1  ',\n",
       " 'scikit-learn              1.2.1           py310hcec6c5f_0  ',\n",
       " 'scipy                     1.10.0          py310ha516a68_1  ',\n",
       " 'scrapy                    2.8.0           py310hecd8cb5_0  ',\n",
       " 'seaborn                   0.12.2          py310hecd8cb5_0  ',\n",
       " 'send2trash                1.8.0              pyhd3eb1b0_1  ',\n",
       " 'service_identity          18.1.0             pyhd3eb1b0_1  ',\n",
       " 'setuptools                65.6.3          py310hecd8cb5_0  ',\n",
       " 'shellingham               1.5.1              pyhd8ed1ab_0    conda-forge',\n",
       " 'sip                       6.6.2           py310he9d5cce_0  ',\n",
       " 'six                       1.16.0             pyhd3eb1b0_1  ',\n",
       " 'smart_open                5.2.1           py310hecd8cb5_0  ',\n",
       " 'smmap                     3.0.5              pyh44b312d_0    conda-forge',\n",
       " 'snappy                    1.1.9                he9d5cce_0  ',\n",
       " 'sniffio                   1.2.0           py310hecd8cb5_1  ',\n",
       " 'snowballstemmer           2.2.0              pyhd3eb1b0_0  ',\n",
       " 'sortedcontainers          2.4.0              pyhd3eb1b0_0  ',\n",
       " 'soupsieve                 2.3.2.post1     py310hecd8cb5_0  ',\n",
       " 'spacy                     3.3.1           py310h7ff4b7e_0  ',\n",
       " 'spacy-legacy              3.0.12             pyhd8ed1ab_0    conda-forge',\n",
       " 'spacy-loggers             1.0.4              pyhd8ed1ab_0    conda-forge',\n",
       " 'sphinx                    5.0.2           py310hecd8cb5_0  ',\n",
       " 'sphinxcontrib-applehelp   1.0.2              pyhd3eb1b0_0  ',\n",
       " 'sphinxcontrib-devhelp     1.0.2              pyhd3eb1b0_0  ',\n",
       " 'sphinxcontrib-htmlhelp    2.0.0              pyhd3eb1b0_0  ',\n",
       " 'sphinxcontrib-jsmath      1.0.1              pyhd3eb1b0_0  ',\n",
       " 'sphinxcontrib-qthelp      1.0.3              pyhd3eb1b0_0  ',\n",
       " 'sphinxcontrib-serializinghtml 1.1.5              pyhd3eb1b0_0  ',\n",
       " 'spyder                    5.4.1           py310hecd8cb5_0  ',\n",
       " 'spyder-kernels            2.4.1           py310hecd8cb5_0  ',\n",
       " 'sqlalchemy                1.4.39          py310hca72f7f_0  ',\n",
       " 'sqlite                    3.40.1               h880c91c_0  ',\n",
       " 'srsly                     2.4.6           py310h7a76584_0    conda-forge',\n",
       " 'stack_data                0.2.0              pyhd3eb1b0_0  ',\n",
       " 'statsmodels               0.13.5          py310h7b7cdfe_1  ',\n",
       " 'streamlit                 1.22.0             pyhd8ed1ab_2    conda-forge',\n",
       " 'sympy                     1.11.1          py310hecd8cb5_0  ',\n",
       " 'tabulate                  0.8.10          py310hecd8cb5_0  ',\n",
       " 'tapi                      1000.10.8            ha1b3eb9_0  ',\n",
       " 'tbb                       2021.7.0             ha357a0b_0  ',\n",
       " 'tbb4py                    2021.7.0        py310ha357a0b_0  ',\n",
       " 'tblib                     1.7.0              pyhd3eb1b0_0  ',\n",
       " 'tenacity                  8.0.1           py310hecd8cb5_1  ',\n",
       " 'terminado                 0.17.1          py310hecd8cb5_0  ',\n",
       " 'text-unidecode            1.3                pyhd3eb1b0_0  ',\n",
       " 'textdistance              4.2.1              pyhd3eb1b0_0  ',\n",
       " 'thinc                     8.0.15          py310h7ff4b7e_0  ',\n",
       " 'threadpoolctl             2.2.0              pyh0d69192_0  ',\n",
       " 'three-merge               0.1.1              pyhd3eb1b0_0  ',\n",
       " 'tifffile                  2021.7.2           pyhd3eb1b0_2  ',\n",
       " 'tinycss2                  1.2.1           py310hecd8cb5_0  ',\n",
       " 'tk                        8.6.12               h5d9f67b_0  ',\n",
       " 'tldextract                3.2.0              pyhd3eb1b0_0  ',\n",
       " 'tokenizers                0.11.4          py310h8776b5c_1  ',\n",
       " 'toml                      0.10.2             pyhd3eb1b0_0  ',\n",
       " 'tomli                     2.0.1           py310hecd8cb5_0  ',\n",
       " 'tomlkit                   0.11.1          py310hecd8cb5_0  ',\n",
       " 'toolz                     0.12.0          py310hecd8cb5_0  ',\n",
       " 'tornado                   6.1             py310hca72f7f_0  ',\n",
       " 'tqdm                      4.64.1          py310hecd8cb5_0  ',\n",
       " 'traitlets                 5.7.1           py310hecd8cb5_0  ',\n",
       " 'transformers              4.24.0          py310hecd8cb5_0  ',\n",
       " 'twisted                   22.2.0          py310hca72f7f_1  ',\n",
       " 'typer                     0.4.2              pyhd8ed1ab_0    conda-forge',\n",
       " 'typing-extensions         4.4.0           py310hecd8cb5_0  ',\n",
       " 'typing_extensions         4.4.0           py310hecd8cb5_0  ',\n",
       " 'tzdata                    2022g                h04d1e81_0  ',\n",
       " 'tzlocal                   4.3             py310h2ec42d9_0    conda-forge',\n",
       " 'ujson                     5.4.0           py310he9d5cce_0  ',\n",
       " 'unidecode                 1.2.0              pyhd3eb1b0_0  ',\n",
       " 'unixodbc                  2.3.11               hb456775_0  ',\n",
       " 'urllib3                   1.26.14         py310hecd8cb5_0  ',\n",
       " 'utf8proc                  2.6.1                h9ed2024_0  ',\n",
       " 'validators                0.20.0             pyhd8ed1ab_0    conda-forge',\n",
       " 'w3lib                     1.21.0             pyhd3eb1b0_0  ',\n",
       " 'wasabi                    0.10.1             pyhd8ed1ab_1    conda-forge',\n",
       " 'watchdog                  2.1.6           py310h999c104_0  ',\n",
       " 'wcwidth                   0.2.5              pyhd3eb1b0_0  ',\n",
       " 'webencodings              0.5.1           py310hecd8cb5_1  ',\n",
       " 'websocket-client          0.58.0          py310hecd8cb5_4  ',\n",
       " 'werkzeug                  2.2.2           py310hecd8cb5_0  ',\n",
       " 'whatthepatch              1.0.2           py310hecd8cb5_0  ',\n",
       " 'wheel                     0.38.4          py310hecd8cb5_0  ',\n",
       " 'widgetsnbextension        3.5.2           py310hecd8cb5_0  ',\n",
       " 'wrapt                     1.14.1          py310hca72f7f_0  ',\n",
       " 'wurlitzer                 3.0.2           py310hecd8cb5_0  ',\n",
       " 'xarray                    2022.11.0       py310hecd8cb5_0  ',\n",
       " 'xlwings                   0.29.1          py310hecd8cb5_0  ',\n",
       " 'xz                        5.2.10               h6c40b1e_1  ',\n",
       " 'yaml                      0.2.5                haf1e3a3_0  ',\n",
       " 'yapf                      0.31.0             pyhd3eb1b0_0  ',\n",
       " 'zeromq                    4.3.4                h23ab428_0  ',\n",
       " 'zfp                       0.5.5                he9d5cce_6  ',\n",
       " 'zict                      2.1.0           py310hecd8cb5_0  ',\n",
       " 'zipp                      3.11.0          py310hecd8cb5_0  ',\n",
       " 'zlib                      1.2.13               h4dc903c_0  ',\n",
       " 'zope                      1.0             py310hecd8cb5_1  ',\n",
       " 'zope.interface            5.4.0           py310hca72f7f_0  ',\n",
       " 'zstandard                 0.19.0          py310h6c40b1e_0  ',\n",
       " 'zstd                      1.5.2                hcb37349_0  ']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "packages = !conda list\n",
    "packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5150b85a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T16:39:48.084668Z",
     "start_time": "2023-05-21T16:39:46.854766Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                  Version\n",
      "------------------------ --------\n",
      "aiohttp                  3.8.4\n",
      "aiosignal                1.3.1\n",
      "anyio                    3.6.2\n",
      "appnope                  0.1.3\n",
      "argon2-cffi              21.3.0\n",
      "argon2-cffi-bindings     21.2.0\n",
      "arrow                    1.2.3\n",
      "asttokens                2.2.1\n",
      "async-timeout            4.0.2\n",
      "attrs                    23.1.0\n",
      "backcall                 0.2.0\n",
      "beautifulsoup4           4.12.2\n",
      "bleach                   6.0.0\n",
      "blis                     0.7.9\n",
      "catalogue                2.0.8\n",
      "certifi                  2023.5.7\n",
      "cffi                     1.15.1\n",
      "charset-normalizer       3.1.0\n",
      "click                    8.1.3\n",
      "comm                     0.1.3\n",
      "confection               0.0.4\n",
      "cymem                    2.0.7\n",
      "dataclasses-json         0.5.7\n",
      "debugpy                  1.6.7\n",
      "decorator                5.1.1\n",
      "defusedxml               0.7.1\n",
      "executing                1.2.0\n",
      "fastjsonschema           2.16.3\n",
      "fqdn                     1.5.1\n",
      "frozenlist               1.3.3\n",
      "greenlet                 2.0.2\n",
      "html5lib                 1.1\n",
      "idna                     3.4\n",
      "ipykernel                6.23.1\n",
      "ipython                  8.13.2\n",
      "ipython-genutils         0.2.0\n",
      "ipywidgets               8.0.6\n",
      "isoduration              20.11.0\n",
      "jedi                     0.18.2\n",
      "Jinja2                   3.1.2\n",
      "jsonpointer              2.3\n",
      "jsonschema               4.17.3\n",
      "jupyter                  1.0.0\n",
      "jupyter_client           8.2.0\n",
      "jupyter-console          6.6.3\n",
      "jupyter_core             5.3.0\n",
      "jupyter-events           0.6.3\n",
      "jupyter_server           2.5.0\n",
      "jupyter_server_terminals 0.4.4\n",
      "jupyterlab-pygments      0.2.2\n",
      "jupyterlab-widgets       3.0.7\n",
      "langchain                0.0.173\n",
      "langcodes                3.3.0\n",
      "MarkupSafe               2.1.2\n",
      "marshmallow              3.19.0\n",
      "marshmallow-enum         1.5.1\n",
      "matplotlib-inline        0.1.6\n",
      "mistune                  2.0.5\n",
      "multidict                6.0.4\n",
      "murmurhash               1.0.9\n",
      "mypy-extensions          1.0.0\n",
      "nbclassic                1.0.0\n",
      "nbclient                 0.7.4\n",
      "nbconvert                7.4.0\n",
      "nbformat                 5.8.0\n",
      "nest-asyncio             1.5.6\n",
      "notebook                 6.5.4\n",
      "notebook_shim            0.2.3\n",
      "numexpr                  2.8.4\n",
      "numpy                    1.24.3\n",
      "openapi-schema-pydantic  1.2.4\n",
      "packaging                23.1\n",
      "pandocfilters            1.5.0\n",
      "parso                    0.8.3\n",
      "pathy                    0.10.1\n",
      "pexpect                  4.8.0\n",
      "pickleshare              0.7.5\n",
      "pip                      23.1.2\n",
      "platformdirs             3.5.1\n",
      "preshed                  3.0.8\n",
      "prometheus-client        0.16.0\n",
      "prompt-toolkit           3.0.38\n",
      "protobuf                 4.21.12\n",
      "psutil                   5.9.5\n",
      "ptyprocess               0.7.0\n",
      "pure-eval                0.2.2\n",
      "pycparser                2.21\n",
      "pydantic                 1.10.7\n",
      "Pygments                 2.15.1\n",
      "PyMuPDF                  1.22.3\n",
      "PyPDF2                   3.0.1\n",
      "PyQt3D                   5.15.5\n",
      "PyQt5                    5.15.7\n",
      "PyQt5-sip                12.11.0\n",
      "PyQtChart                5.15.6\n",
      "PyQtDataVisualization    5.15.5\n",
      "PyQtNetworkAuth          5.15.5\n",
      "PyQtPurchasing           5.15.5\n",
      "PyQtWebEngine            5.15.6\n",
      "pyrsistent               0.19.3\n",
      "python-dateutil          2.8.2\n",
      "python-json-logger       2.0.7\n",
      "PyYAML                   6.0\n",
      "pyzmq                    25.0.2\n",
      "qtconsole                5.4.3\n",
      "QtPy                     2.3.1\n",
      "requests                 2.30.0\n",
      "rfc3339-validator        0.1.4\n",
      "rfc3986-validator        0.1.1\n",
      "Send2Trash               1.8.2\n",
      "setuptools               65.5.1\n",
      "six                      1.16.0\n",
      "smart-open               6.3.0\n",
      "sniffio                  1.3.0\n",
      "soupsieve                2.4.1\n",
      "spacy                    3.5.3\n",
      "spacy-legacy             3.0.12\n",
      "spacy-loggers            1.0.4\n",
      "SQLAlchemy               2.0.14\n",
      "srsly                    2.4.6\n",
      "stack-data               0.6.2\n",
      "tenacity                 8.2.2\n",
      "terminado                0.17.1\n",
      "thinc                    8.1.10\n",
      "tinycss2                 1.2.1\n",
      "tornado                  6.3.2\n",
      "tqdm                     4.65.0\n",
      "traitlets                5.9.0\n",
      "typer                    0.7.0\n",
      "typing_extensions        4.5.0\n",
      "typing-inspect           0.8.0\n",
      "uri-template             1.2.0\n",
      "urllib3                  2.0.2\n",
      "wasabi                   1.1.1\n",
      "wcwidth                  0.2.6\n",
      "webcolors                1.13\n",
      "webencodings             0.5.1\n",
      "websocket-client         1.5.1\n",
      "wheel                    0.38.4\n",
      "widgetsnbextension       4.0.7\n",
      "yarl                     1.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2885695",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## pdf to text sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd659ba",
   "metadata": {
    "hidden": true
   },
   "source": [
    " a Python function that can be used to read in a PDF using the PyMuPDF package and output a list of texts where each text is the text inside each subsection. The function also outputs a dictionary where each property is the subsection and the values of the property are subsection level and the text inside each subsection. The function is type annotated and documented using the Google doc-string style:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c83c9607",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T16:31:56.016953Z",
     "start_time": "2023-05-21T16:31:56.008094Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from typing import Union, Tuple, Dict, List\n",
    "import fitz\n",
    "import os\n",
    "\n",
    "def extract_subsections(pdf_paths: Union[str, List[str]]) -> \\\n",
    "                        Tuple[int, Dict[str, Tuple[int, str]], str]:\n",
    "    \"\"\"\n",
    "    Extracts the texts inside each subsection of one or more PDFs \n",
    "    and also builds a dictionary with subsections as keys\n",
    "    and a tuple of subsection level and subsection text as values.\n",
    "\n",
    "    Args:\n",
    "        pdf_paths (Union[str, List[str]]): The path(s) to the PDF file(s).\n",
    "\n",
    "    Returns:\n",
    "        Tuple[int, Dict[str, Tuple[int, str]], str]: A tuple with the \n",
    "        total number of extracted subsection texts, a dictionary where \n",
    "        each property is the subsection, and the values of the property \n",
    "        are subsection level and the text inside each subsection, \n",
    "        and a text corpus str these texts.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If the file(s) specified by pdf_paths do not exist or cannot be opened.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    if not isinstance(pdf_paths, list):\n",
    "        pdfs = [pdf_paths]\n",
    "    else:\n",
    "        pdfs = pdf_paths\n",
    "\n",
    "    section_texts = []\n",
    "    section_info = {}\n",
    "        \n",
    "    for pdf_path in pdfs:\n",
    "        # Error handling check\n",
    "        if not os.path.isfile(pdf_path):\n",
    "            raise FileNotFoundError(f\"No such file or directory: '{pdf_path}'\")\n",
    "        \n",
    "        try:\n",
    "            doc = fitz.open(pdf_path) \n",
    "        except RuntimeError as e:\n",
    "            raise RuntimeError(f\"Failed to open file '{pdf_path}': {e}\")\n",
    "\n",
    "        for page in doc:\n",
    "            blocks = page.get_text(\"blocks\")\n",
    "            for block in blocks:\n",
    "                text = block[4]\n",
    "                section_texts.append(text)\n",
    "                lines = text.splitlines()\n",
    "                for line in lines:\n",
    "                    if line.endswith(':'):  \n",
    "                        level = len(line.split())\n",
    "                        section_info[line[:-1]] = (level, '\\n'.join(lines[lines.index(line) + 1:]))\n",
    "\n",
    "    return len(section_texts), section_info, \" \".join(section_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1df1e0b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T17:44:04.812693Z",
     "start_time": "2023-05-20T17:44:04.809651Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# extract_subsections([\"../PDFs/Text and Code Embeddings.pdf\",\n",
    "#                     \"../PDFs/Place_Freeze_Success_PDF.pdf\",\n",
    "#                     \"../PDFs/0a6323ba-1642-45a6-9152-cd6eaaf9b4b0 (1).pdf\",\n",
    "#                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6f7dc71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T16:32:17.213555Z",
     "start_time": "2023-05-21T16:32:17.136518Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRUCE COTTMAN\n",
      "685 TECH CENTER PKWY APT 3219\n",
      "Newport News, VA 23606-1552\n",
      " April 24, 2023\n",
      " Dear BRUCE COTTMAN:\n",
      " We are writing to confirm that a security freeze was placed on your Equifax credit report. \n",
      " Before applying for credit, you must temporarily lift or permanently remove your security freeze by:\n",
      " l Logging in or creating an account at  www.myEquifax.com\n",
      " l Calling us at  1-888-298-0045\n",
      " l Or sending a written request that includes your name, address and social security number to:\n",
      "   Equifax Information Services LLC\n",
      "   P.O. Box 105788\n",
      "   Atlanta, GA 30348\n",
      " If you are sending your request by mail, please be sure to include copies of the following:\n",
      "            \n",
      "          One Government Issued ID\n",
      "                    One Item to Validate Your Address\n",
      "   \n",
      "Example: \n",
      "Driver's License\n",
      "State Issued ID Card\n",
      "Passport\n",
      "Birth Certificate\n",
      " \n",
      "  \n",
      "Example:\n",
      "Driver's License\n",
      "Utility Bill\n",
      "Pay Stub\n",
      "W2 or 1099 Form\n",
      " \n",
      "For requests to temporarily lift your security freeze, please be sure to specify the desired date range. \n",
      "Example: March 15, 20XX to March 21, 20XX\n",
      " Placing, lifting and removing a security freeze on your Equifax credit report is free.\n",
      " To learn how to make a security freeze request with the other two nationwide credit bureaus, visit:\n",
      " l TransUnion at  www.transunion.com\n",
      " l Experian at  www.experian.com\n",
      " Equifax Information Services LLC maintains your credit report and provides information to creditors and insurers,\n",
      "including credit card companies and other lenders, so that they may make pre−approved firm offers of credit as\n",
      "permitted by the Fair Credit Reporting Act. If you prefer not to receive such offers, you can opt out by visiting  \n",
      "www.optoutprescreen.com or calling toll-free: 1-888-5-OPT-OUT (or 1-888-567-8688).\n",
      " You may also send your request in writing to: \n",
      "Equifax Information Services LLC\n",
      "P.O. Box 740123\n",
      "Atlanta, GA 30374-0123\n",
      " Please include your complete name, address, social security number and signature. Equifax will remove your name \n",
      "from its pre-approved offer database and share your request with the other two nationwide credit reporting agencies.\n",
      " Thank you for the opportunity to assist you.\n",
      " Equifax Information Services LLC\n",
      " 3114544049-OFZ-0bdb010200000626-04242023\n",
      "000000001-FLT\n",
      " SARA COTTMAN\n",
      "Facility#3265\n",
      " <image: DeviceRGB, width: 200, height: 200, bpc: 8> Here are the benefits you have elected along with your cost per paycheck.\n",
      " Cost for medical coverage and life insurance depend on partly on the tobacco information you provided. You have elected to cover [0] of tobacco \n",
      "user(s).\n",
      " Plan\n",
      "Your Choices\n",
      "Your Coverage Level\n",
      "Cost Per Paycheck\n",
      " Medical\n",
      "Walmart Premier Plan\n",
      "Associate Only\n",
      "$33.00\n",
      " Vision\n",
      "Coverage\n",
      "Associate Only\n",
      "$2.76\n",
      " Dental\n",
      "Coverage\n",
      "Associate Only\n",
      "$8.30\n",
      " Accidental Death and \n",
      "Dismemberment\n",
      " $150,000\n",
      "Associate Only\n",
      "$0.97\n",
      " Optional Life Insurance*\n",
      "No Coverage\n",
      "$0.00\n",
      " Optional Child Life Insurance\n",
      "No Coverage\n",
      "$0.00\n",
      " Critical Illness Insurance\n",
      "No Coverage\n",
      "$0.00\n",
      " Accident Insurance\n",
      "Coverage\n",
      "Associate Only\n",
      "$0.68\n",
      " 401(k)\n",
      "Enrolled\n",
      "Regular Savings Rate  6% \n",
      "Catch-Up Savings Rate  1%\n",
      " $68.66\n",
      " Associate Stock Purchase Plan\n",
      "Enrolled\n",
      "$18.00\n",
      " Current Cost Per Paycheck\n",
      "New Cost Per Paycheck\n",
      " Total cost of all your \n",
      "benefits choices\n",
      " $0.00\n",
      "$132.37\n",
      " _______________________________________________________________________________________________\n",
      " I authorize Walmart to make payroll deductions from my wages each pay period to cover my premiums for the coverage I have elected under the \n",
      "Plan as well as any premiums for any automatic or default elections. Payroll deductions are taken from post-tax wages. I understand that my \n",
      "coverage and corresponding payroll deduction elections may not be changed during the year except as described in the Associate Benefits Book. \n",
      "However, if I am permitted to make changes to my coverage, my corresponding payroll deduction elections may increase or decrease, back to \n",
      "that effective date of such coverage.  Furthermore, I understand that in the event my wages are less than my premiums, I may experience a lapse \n",
      "in, or cancelation of, coverage.  If I am in arrears on my premiums, I authorize Walmart to make deductions from my wages to cover my unpaid \n",
      "premiums, up to the maximum amount allowable by law.  I understand that any authorization to deduct these extra amounts will be solely for my \n",
      "benefit as the deductions will be used to pay premiums so I may continue to participate in the Plan. I understand that if my wages are insufficient \n",
      "to pay any outstanding premiums, it is my responsibility to pay those premiums to ensure my coverage under the Plan continues without \n",
      "interruption or cancelation. I understand that if I have elected Long-Term Disability, Optional Associate Life or Optional Spouse/Partner Life, and \n",
      "Proof of Good Health is required, as described in the Associate Benefits Book, payroll deductions will not begin until I have met those \n",
      "requirements and my coverage has been approved. \n",
      " This is not a guarantee of benefits. All benefits are subject to plan terms and coverage is effective only when all requirements, including, but not \n",
      "limited to, waiting periods, proof of good health, current premium payments, have been satisfied. See the Associate Benefits Book for more \n",
      "details. \n",
      " To make investment changes log into your 401(k) Plan account at Benefits.ML.com.\n",
      " *Proof of Good Health is required when you enroll after your initial enrollment period, you increase coverage or you enroll for more than the \n",
      "guaranteed amount. You will receive a Proof of Good Health form electronically or by mail. Once you have turned in the form and answered all \n",
      "additional requests for information, you will receive a letter stating if your coverage is approved. If you need assistance with accessing the form \n",
      "or questions about the approval, please contact Prudential at 877-740-2116. If Proof of Good Health is required, coverage will not be effective \n",
      "until after The Prudential Insurance Company of America approves. Premiums will not be deducted until coverage is effective. See the Associate \n",
      "Benefits Book for coverage effective date and more details.\n",
      " **Proof of Good Health is required when you enroll after your initial enrollment period or you increase coverage. You will receive a Proof of Good \n",
      "Health form electronically or by mail. Once you have turned in the form, you will receive a letter stating if your coverage is approved. If you need \n",
      "assistance with accessing the form or questions about the approval, please contact Lincoln Financial Group at 877-353-6404. Coverage will not \n",
      "be effective until after Lincoln Financial Group approves. Premiums will not be deducted until coverage is effective. See the Associate Benefits \n",
      "Book for effective date and more details.\n",
      " ***Disability rates are based on your income. The amount displayed reflects what your cost per paycheck would be based on your wages from \n",
      "last pay period. Your actual cost per paycheck will vary based on your wages at the time the deductions are taken.\n",
      " If you have any questions about your enrollment, please call People Services at 800-421-1362.\n",
      " This document is generated on  : 05-11-2023 08:31:04.732000 CDT\n",
      " Text and Code Embeddings by Contrastive Pre-Training\n",
      " Arvind Neelakantan * 1 Tao Xu * 1 Raul Puri 1 Alec Radford 1 Jesse Michael Han 1 Jerry Tworek 1\n",
      " Qiming Yuan 1 Nikolas Tezak 1 Jong Wook Kim 1 Chris Hallacy 1 Johannes Heidecke 1 Pranav Shyam 1\n",
      " Boris Power 1 Tyna Eloundou Nekoul 1 Girish Sastry 1 Gretchen Krueger 1 David Schnurr 1\n",
      " Felipe Petroski Such 1 Kenny Hsu 1 Madeleine Thompson 1 Tabarak Khan 1 Toki Sherbakov 1 Joanne Jang 1\n",
      " Peter Welinder 1 Lilian Weng 1\n",
      " Abstract\n",
      " Text embeddings are useful features in many\n",
      "applications such as semantic search and com-\n",
      "puting text similarity. Previous work typically\n",
      "trains models customized for different use cases,\n",
      "varying in dataset choice, training objective and\n",
      "model architecture. In this work, we show that\n",
      "contrastive pre-training on unsupervised data at\n",
      "scale leads to high quality vector representations\n",
      "of text and code. The same unsupervised text em-\n",
      "beddings that achieve new state-of-the-art results\n",
      "in linear-probe classiﬁcation also display impres-\n",
      "sive semantic search capabilities and sometimes\n",
      "even perform competitively with ﬁne-tuned mod-\n",
      "els. On linear-probe classiﬁcation accuracy aver-\n",
      "aging over 7 tasks, our best unsupervised model\n",
      "achieves a relative improvement of 4% and 1.8%\n",
      "over previous best unsupervised and supervised\n",
      "text embedding models respectively. The same\n",
      "text embeddings when evaluated on large-scale\n",
      "semantic search attains a relative improvement\n",
      "of 23.4%, 14.7%, and 10.6% over previous best\n",
      "unsupervised methods on MSMARCO, Natural\n",
      "Questions and TriviaQA benchmarks, respec-\n",
      "tively.\n",
      "Similarly to text embeddings, we train\n",
      "code embedding models on (text, code) pairs, ob-\n",
      "taining a 20.8% relative improvement over prior\n",
      "best work on code search.\n",
      " 1. Introduction\n",
      " Deep unsupervised learning with generative and embed-\n",
      "ding models has seen dramatic success in the past few\n",
      "years. Generative models (Peters et al., 2018; Raffel et al.,\n",
      "2019; van den Oord et al., 2016; Ramesh et al., 2021;\n",
      "Brown et al., 2020; Chen et al., 2021) are trained to max-\n",
      " *Equal contribution\n",
      "1OpenAI. Correspondence to: Arvind\n",
      "Neelakantan <arvind@openai.com>.\n",
      " S-300M\n",
      "M-1.2B\n",
      "L-6B\n",
      "XL-175B\n",
      " Model Size\n",
      " 60\n",
      " 62\n",
      " 64\n",
      " 66\n",
      " 68\n",
      " 70\n",
      " Performance\n",
      " Average performance vs model size\n",
      " Figure 1. Average performance of unsupervised cpt-text\n",
      "models of different sizes across 22 tasks consisting of linear-probe\n",
      "classiﬁcation, text search, and sentence similarity tasks.\n",
      " imize the likelihood of observed data while embedding\n",
      "models are trained to distinguish observed data from noise\n",
      "(Sohn, 2016; van den Oord et al., 2018; Radford et al.,\n",
      "2021; Jia et al., 2021; Gao et al., 2021; Izacard et al., 2021).\n",
      "Generative models have been shown to produce realistic\n",
      "content and beneﬁt many downstream applications, reduc-\n",
      "ing the need for labeled training datasets. In generative\n",
      "models, the information about the input is typically dis-\n",
      "tributed over multiple hidden states of the model. While\n",
      "some generative models (Kingma & Welling, 2014; Kiros\n",
      "et al., 2015) can learn a single representation of the in-\n",
      "put, most autoregressive Transformer (Vaswani et al., 2017)\n",
      "models do not (Raffel et al., 2019; Brown et al., 2020; Chen\n",
      "et al., 2021; Ramesh et al., 2021). However, learning such a\n",
      "representation (or embedding) is necessary for many tasks.\n",
      "Systems that search over millions or billions of items re-\n",
      "quire each entry to be embedded as a dense representation\n",
      "and build an index in advance to save computational costs\n",
      "at query time. These embeddings are useful features for\n",
      "classiﬁcation tasks and can also enable data visualization\n",
      "applications via techniques such as clustering. Embedding\n",
      "models are explicitly optimized to learn a low dimensional\n",
      "representation that captures the semantic meaning of the\n",
      "input (Radford et al., 2021; Jia et al., 2021; Giorgi et al.,\n",
      "2020; Gao et al., 2021; Izacard et al., 2021).\n",
      " arXiv:2201.10005v1  [cs.CL]  24 Jan 2022\n",
      " Text and Code Embeddings by Contrastive Pre-Training\n",
      " In this work, we train embedding models using a con-\n",
      "trastive learning objective with in-batch negatives (Sohn,\n",
      "2016; Yih et al., 2011) on unlabeled data. The input is en-\n",
      "coded with a Transformer encoder (Vaswani et al., 2017)\n",
      "and we leverage naturally occurring paired data to con-\n",
      "struct training data with no explicit labels. Text embedding\n",
      "models are trained on paired text data where we consider\n",
      "neighboring pieces of text on the Internet as positive pairs.\n",
      "Code embedding models treat the top-level docstring in a\n",
      "function along with its implementation as a (text, code)\n",
      "pair. The training signal of the contrastive objective on\n",
      "its own is not sufﬁcient to learn useful representations and\n",
      "we overcome this by initializing our model with other pre-\n",
      "trained models (Brown et al., 2020; Chen et al., 2021). Fi-\n",
      "nally, we ﬁnd that it is critical to use a sufﬁciently large\n",
      "batch to achieve the optimal performance. We show that\n",
      "this simple recipe combining pre-trained model initializa-\n",
      "tion, large-batch contrastive learning and training at scale,\n",
      "can produce text and code embeddings that possess a broad\n",
      "range of capabilities.\n",
      " We train a series of unsupervised text embedding mod-\n",
      "els (cpt-text) of different sizes, ranging from 300M\n",
      "to 175B parameters, and observe a consistent perfor-\n",
      "mance improvement with increasing model sizes (Figure\n",
      "1). On classiﬁcation accuracy averaging across 7 linear-\n",
      "probe classiﬁcation tasks in SentEval (Conneau & Kiela,\n",
      "2018), our largest unsupervised model achieves new state-\n",
      "of-the-art results with a relative improvement of 4% and\n",
      "1.8% over the previous best unsupervised (Giorgi et al.,\n",
      "2020) and supervised (Gao et al., 2021) text embedding\n",
      "models, respectively.\n",
      " Text embedding in previous work was studied under differ-\n",
      "ent domains, varying in data, training objective and model\n",
      "architecture. Precisely, sentence embedding (Reimers &\n",
      "Gurevych, 2019; Gao et al., 2021; Giorgi et al., 2020)\n",
      "and neural information retrieval (Lee et al.; Guu et al.,\n",
      "2020; Karpukhin et al., 2020a; Sachan et al., 2021; Izac-\n",
      "ard et al., 2021) have remained different research topics\n",
      "evaluated on distinct benchmarks, even though both aim to\n",
      "learn high-quality text representation. However, we ﬁnd the\n",
      "same model that achieves good performance on sentence\n",
      "embedding benchmarks, as discussed above, is also able\n",
      "to obtain impressive results on large-scale information re-\n",
      "trieval. When evaluated on the MSMARCO passage rank-\n",
      "ing task (Nguyen et al., 2016) to search over 4M passages,\n",
      "cpt-text gets a relative improvement of 23.4% over pre-\n",
      "vious best unsupervised methods (Robertson, 2009). On\n",
      "the task of searching on 21M documents from Wikipedia,\n",
      "cpt-text obtains a relative improvement of 14.7%, and\n",
      "10.6% over previous unsupervised methods (Izacard et al.,\n",
      "2021) for Natural Questions (Kwiatkowski et al., 2019)\n",
      "and TriviaQA (Joshi et al., 2017), respectively. On Triv-\n",
      "iaQA, our unsupervised method is even competitive with\n",
      " ﬁne-tuned models.\n",
      " Next, we train code embedding models (cpt-code) using\n",
      "the same recipe. Our models learn via (text, code) pairs,\n",
      "extracted from open source code. We evaluate our model\n",
      "on CodeSearchNet (Husain et al., 2020), a commonly used\n",
      "code search benchmark, where the task is to ﬁnd the most\n",
      "relevant code snippet given a natural language query. Our\n",
      "models achieve new state-of-the-art results with a 20.8%\n",
      "relative improvement over the previous best result (Guo\n",
      "et al., 2021). Unlike text embedding models, we observe\n",
      "no performance improvement on code search when increas-\n",
      "ing the number of parameters of cpt-code from 300M to\n",
      "1.2B.\n",
      " Finally, we experiment with ﬁne-tuning our models on\n",
      "several supervised datasets and study the transfer learn-\n",
      "ing performance. When ﬁne-tuned on NLI (Natural Lan-\n",
      "guage Inference) datasets, we see a further boost in linear-\n",
      "probe classiﬁcation, outperforming the previous best trans-\n",
      "fer method (Gao et al., 2021) by 2.2%. On SST-2 senti-\n",
      "ment classiﬁcation (Socher et al., 2013), we ﬁnd that our\n",
      "representations are sufﬁciently descriptive that even a sim-\n",
      "ple k-NN classiﬁer achieves results comparable to a linear-\n",
      "probe classiﬁer. Interestingly, zero-shot performance with\n",
      "our embeddings outperforms the supervised neural network\n",
      "models introduced along with the release of the SST-2\n",
      "dataset. We also ﬁne-tune the unsupervised model on MS-\n",
      "MARCO and evaluate it on a suite of zero-shot search tasks\n",
      "in the BEIR benchmark (Thakur et al., 2021). In the trans-\n",
      "fer setting, our models achieve a 5.2% relative improve-\n",
      "ment over previous methods (Izacard et al., 2021) and is\n",
      "comparable even with methods (Santhanam et al., 2021;\n",
      "Formal et al., 2021; Wang et al., 2020) that demand sub-\n",
      "stantially more computation at test time.\n",
      " 2. Approach\n",
      " Our models are trained with a contrastive objective on\n",
      "paired data. In this section, we present more details on the\n",
      "model architecture and the training objective. The training\n",
      "set consists of paired samples, {(xi, yi)}N\n",
      "i=1, where (xi, yi)\n",
      "corresponds to a positive example pair, indicating that xi\n",
      "and yi are semantically similar or contextually relevant.\n",
      " 2.1. Model\n",
      " Given a training pair (x, y), a Transformer (Vaswani et al.,\n",
      "2017) encoder E is used to process x and y independently.\n",
      "The encoder maps the input to a dense vector representa-\n",
      "tion or embedding (Figure 2). We insert two special token\n",
      "delimiters, [SOS] and [EOS], to the start and end of the\n",
      "input sequence respectively. The hidden state from the last\n",
      "layer corresponding to the special token [EOS] is consid-\n",
      "ered as the embedding of the input sequence.\n",
      " Text and Code Embeddings by Contrastive Pre-Training\n",
      "  ENCODER\n",
      " <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 126, height: 74, bpc: 8> <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 64, height: 44, bpc: 8> INPUT\n",
      " <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 38, height: 34, bpc: 8> [EOS]\n",
      " <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 48, height: 50, bpc: 8> [SOS]\n",
      " <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 48, height: 50, bpc: 8>  ENCODER\n",
      " <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 126, height: 74, bpc: 8> <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 64, height: 44, bpc: 8> <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 644, height: 64, bpc: 8> <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 640, height: 74, bpc: 8> <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 62, height: 54, bpc: 8> <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 730, height: 76, bpc: 8> Figure 2. The encoder E maps input x to embedding vx. Special\n",
      "tokens, [SOS] and [EOS], are appended to the start and end\n",
      "of the input sequence respectively. The last layer hidden state\n",
      "corresponding to the token [EOS] is extracted as the embedding\n",
      "of the input sequence.\n",
      " [EOS]\n",
      "  ENCODER\n",
      " <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 126, height: 74, bpc: 8> <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 64, height: 44, bpc: 8> <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 644, height: 64, bpc: 8> <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 640, height: 74, bpc: 8> <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 62, height: 54, bpc: 8> <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 730, height: 76, bpc: 8> Figure 3. The encoder E maps inputs x and y, to embeddings,\n",
      "vx and vy independently. The similarity score between x and y\n",
      "is deﬁned as the cosine similarity between these two embedding\n",
      "vectors.\n",
      " The Transformer encoder maps the input, x and y, to em-\n",
      "beddings, vx and vy respectively and the similarity between\n",
      "two inputs is quantiﬁed by the cosine similarity between\n",
      "their embeddings, vx and vy (Figure 3).\n",
      " vx = E([SOS]x ⊕ x ⊕ [EOS]x)\n",
      " vy = E([SOS]y ⊕ y ⊕ [EOS]y)\n",
      " sim(x, y) =\n",
      "vx · vy\n",
      " ∥vx∥ · ∥vy∥\n",
      " where ⊕ is an operation to concatenate two strings to-\n",
      "gether. We found that using different delimiters leads to\n",
      "more stable training. For x, we use ‘[’ as [SOS]x and\n",
      "‘]’ as [EOS]x, while we use ‘{’ and ‘}’ as [SOS]y and\n",
      "[EOS]y respectively for y.\n",
      " 2.2. Training Objective\n",
      " The paired samples in the training set are contrasted against\n",
      "in-batch negatives (Yih et al., 2011; Sohn, 2016). Con-\n",
      "trastive learning with in-batch negatives has been widely\n",
      " Model\n",
      "Parameters\n",
      "Embed Dimensions\n",
      "Batch size\n",
      " S\n",
      "300M\n",
      "1024\n",
      "12288\n",
      "M\n",
      "1.2B\n",
      "2048\n",
      "6912\n",
      "L\n",
      "6B\n",
      "4096\n",
      "5896\n",
      "XL\n",
      "175B\n",
      "12288\n",
      "4976\n",
      " Table 1. Batch size used to train the models of different sizes.\n",
      " used for unsupervised representation learning in prior work\n",
      "(Radford et al., 2021; Jia et al., 2021; Chen et al., 2020;\n",
      "Izacard et al., 2021). For each example in a mini-batch\n",
      "of M examples, the other (M − 1) in the batch are used as\n",
      "negative examples. The usage of in-batch negatives enables\n",
      "re-use of computation both in the forward and the backward\n",
      "pass making training highly efﬁcient. The logits for one\n",
      "batch is a M × M matrix, where each entry logit(xi, yj) is\n",
      "given by,\n",
      " logit(xi,yj) = sim(xi, yj) · exp(τ),\n",
      " ∀(i, j), i, j ∈ {1, 2, . . . , M}\n",
      " where τ is a trainable temperature parameter.\n",
      " Only entries on the diagonal of the matrix are considered\n",
      "positive examples. The ﬁnal training loss is the sum of the\n",
      "cross entropy losses on the row and the column direction,\n",
      "as described in the following numpy style pseudo code.\n",
      " labels = np.arange(M)\n",
      "l_r = cross_entropy(logits, labels, axis=0)\n",
      "l_c = cross_entropy(logits, labels, axis=1)\n",
      "loss = (l_r + l_c) / 2\n",
      " We initialize our models with pre-trained generative lan-\n",
      "guage models. cpt-text is initialized with GPT mod-\n",
      "els (Brown et al., 2020) and cpt-code is initialized with\n",
      "Codex models (Chen et al., 2021). When ﬁne-tuning our\n",
      "models (Section 3), the supervised training data like NLI\n",
      "datasets contain explicit negative examples and they are\n",
      "used along with the in-batch negatives.\n",
      " 3. Results\n",
      " Our models are trained on naturally occurring paired data.\n",
      "cpt-text models are trained on Internet data with neigh-\n",
      "boring pieces of text as positive pairs for the contrastive ob-\n",
      "jective. The code embedding cpt-code models use (text,\n",
      "code) pairs extracted from open source code. As discussed\n",
      "in Section 3.4.1, sufﬁciently large batch size is crucial to\n",
      "achieve good performance with our setup. Table 1 lists the\n",
      "batch sizes used to train the models of different sizes.\n",
      " We evaluate our text embedding models on a broad range of\n",
      "tasks: linear-probe classiﬁcation, sentence similarity, and\n",
      " Text and Code Embeddings by Contrastive Pre-Training\n",
      " semantic search. While sentence embedding (Reimers &\n",
      "Gurevych, 2019; Gao et al., 2021; Giorgi et al., 2020) meth-\n",
      "ods report results only on embedding benchmarks and neu-\n",
      "ral information retrieval methods (Lee et al.; Guu et al.,\n",
      "2020; Karpukhin et al., 2020a; Sachan et al., 2021; Izacard\n",
      "et al., 2021) report results only on search benchmarks, we\n",
      "use the same unsupervised model across all these tasks.\n",
      " 3.1. Text Embedding\n",
      " The SentEval benchmark (Conneau & Kiela, 2018) is\n",
      "widely adopted to assess the quality of sentence embed-\n",
      "dings, consisting of a broad collection of tasks in the cate-\n",
      "gories of linear-probe classiﬁcation and sentence similarity,\n",
      "and we use the same to evaluate ours.\n",
      " 3.1.1. LINEAR PROBE CLASSIFICATION\n",
      " When evaluated on linear-probe classiﬁcation, the embed-\n",
      "dings are used as features to train a linear classiﬁer to\n",
      "solve a variety of downstream tasks. The results in Ta-\n",
      "ble 2 demonstrate a clear advantage of larger model sizes\n",
      "producing better features for improved classiﬁcation per-\n",
      "formance. In transfer learning setup, we ﬁne-tune unsuper-\n",
      "vised cpt-text models on SNLI (Bowman et al., 2015)\n",
      "and MNLI (Williams et al., 2018) datasets using entailment\n",
      "pairs as positive examples and contradiction pairs as nega-\n",
      "tive examples. On both unsupervised learning and transfer\n",
      "learning settings, we achieve state-of-the-art results.\n",
      " 3.1.2. ZERO-SHOT AND k-NN CLASSIFICATION\n",
      " In this section, we discuss results using zero-shot classiﬁ-\n",
      "cation and k-nearest neighbor classiﬁcation on the SST-2\n",
      "binary sentiment classiﬁcation task (Socher et al., 2013).\n",
      "We experiment with 6B (L) cpt-text model ﬁne-tuned\n",
      "on NLI data for this study. In the ﬁrst zero-shot experiment,\n",
      "each input text is assigned with one of the two labels (‘pos-\n",
      "itive’, ‘negative’) based on which label has its embedding\n",
      "closest to the input text embedding. The performance can\n",
      "be further improved by prompting, where we use a simple\n",
      "label description, ‘this is an example of a positive/negative\n",
      "movie review.’, instead of a single word. This zero-shot\n",
      "usage of embeddings is novel compared to prior work on\n",
      "embeddings and it is interesting to note that our zero-shot\n",
      "results are better than the supervised neural network results\n",
      "reported along with the release of the dataset (Socher et al.,\n",
      "2013).\n",
      "In the k-NN classiﬁcation experiment, given an\n",
      "input text, the prediction is the majority label among 256\n",
      "training examples closest to the test input in the embedding\n",
      "space. As shown in Table 3, the k-NN classiﬁer without\n",
      "any task-speciﬁc tuning of trainable parameters achieves\n",
      "results comparable to a linear classiﬁer.\n",
      " 3.1.3. SENTENCE SIMILARITY\n",
      " On sentence similarity tasks in SentEval, we ﬁnd that our\n",
      "models perform worse than previous SOTA methods (Ta-\n",
      "ble 4). Sentence similarity is not a completely well-deﬁned\n",
      "downstream task (e.g. are the sentences, ‘Jack loves Jill’\n",
      "and ‘Mary loves chocolates’, similar?).1,2\n",
      "For example,\n",
      " Goodman (1972) argue that two objects can be inﬁnitely\n",
      "similar or dissimilar (Vervaeke et al., 2012). A possible\n",
      "explanation for why our models perform better than prior\n",
      "work on search and classiﬁcation but not on these tasks is\n",
      "that our models might not be optimized for the speciﬁc def-\n",
      "inition used by these sentence similarity benchmarks. It\n",
      "is important to note that previous embedding search meth-\n",
      "ods do not report performance on sentence similarity tasks\n",
      "(Karpukhin et al., 2020a; Sachan et al., 2021; Izacard et al.,\n",
      "2021). More discussion on this phenomenon is presented\n",
      "in Section 3.4.2.\n",
      " 3.2. Text Search\n",
      " Previous work on training embedding methods for search\n",
      "typically requires ﬁne-tuning on a particular text search\n",
      "dataset (Karpukhin et al., 2020a; Sachan et al., 2021; Qu\n",
      "et al., 2021).\n",
      "It is also common to have a multi-step\n",
      "setup where ﬁne-tuned models rely on an expensive query\n",
      "and document cross-attention encoder in the ﬁnal step (Qu\n",
      "et al., 2021; Wang et al., 2020). In contrast, we push the\n",
      "limits of using a single embedding model for large-scale\n",
      "semantic search.\n",
      " 3.2.1. LARGE-SCALE SEARCH\n",
      " First, we evaluate our models on several large-scale text\n",
      "search benchmarks.\n",
      "MSMARCO (Nguyen et al., 2016)\n",
      "requires the model to search over 4M documents while\n",
      "Natural Questions (NQ) (Kwiatkowski et al., 2019) and\n",
      "TriviaQA (Joshi et al., 2017) involve searching over 21M\n",
      "Wikipedia documents. We use the FAISS library (John-\n",
      "son et al., 2019) to build the vector indices for approximate\n",
      "k-nearest neighbor search. The same unsupervised model\n",
      "discussed previously achieves impressive performance on\n",
      "semantic search. Table 5 demonstrates that cpt-text\n",
      "outperforms prior unsupervised approaches by a big mar-\n",
      "gin and larger model sizes consistently lead to improved\n",
      "performance. Surprisingly, on TriviaQA, our model is even\n",
      "competitive with ﬁne-tuned models.\n",
      " 1https://twitter.com/yoavgo/status/\n",
      "1431299645570011142\n",
      " 2https://twitter.com/yoavgo/status/\n",
      "1483565266575540225?s=20\n",
      " Text and Code Embeddings by Contrastive Pre-Training\n",
      " MR\n",
      "CR\n",
      "SUBJ\n",
      "MPQA\n",
      "SST\n",
      "TREC\n",
      "MRPC\n",
      "Avg.\n",
      " Unsupervised\n",
      " BERT (Devlin et al., 2019)\n",
      "78.7\n",
      "86.2\n",
      "94.4\n",
      "88.7\n",
      "84.4\n",
      "92.8\n",
      "69.4\n",
      "84.9\n",
      "SimCSE (Gao et al., 2021)\n",
      "84.7\n",
      "88.6\n",
      "95.4\n",
      "87.5\n",
      "89.5\n",
      "95.0\n",
      "72.4\n",
      "87.6\n",
      "DECLUTR (Giorgi et al., 2020)\n",
      "85.2\n",
      "90.7\n",
      "95.8\n",
      "88.5\n",
      "90.0\n",
      "93.2\n",
      "74.6\n",
      "88.3\n",
      " cpt-text S\n",
      "87.1\n",
      "90.1\n",
      "94.9\n",
      "88.3\n",
      "91.8\n",
      "95.2\n",
      "71.6\n",
      "88.4\n",
      "cpt-text M\n",
      "89.0\n",
      "90.9\n",
      "96.7\n",
      "89.6\n",
      "93.9\n",
      "96.6\n",
      "73.6\n",
      "89.9\n",
      "cpt-text L\n",
      "90.6\n",
      "92.6\n",
      "97.0\n",
      "90.6\n",
      "95.3\n",
      "97.0\n",
      "73.6\n",
      "90.9\n",
      "cpt-text XL\n",
      "92.2\n",
      "93.5\n",
      "97.4\n",
      "91.5\n",
      "96.2\n",
      "97.4\n",
      "74.1\n",
      "91.8\n",
      " Transfer from NLI data\n",
      " SBERT (Reimers & Gurevych, 2019)\n",
      "84.9\n",
      "90.1\n",
      "94.5\n",
      "90.3\n",
      "90.7\n",
      "87.4\n",
      "75.9\n",
      "87.7\n",
      "SimCSE (Gao et al., 2021)\n",
      "88.4\n",
      "92.5\n",
      "95.2\n",
      "90.1\n",
      "93.3\n",
      "93.8\n",
      "77.7\n",
      "90.2\n",
      " cpt-text S\n",
      "87.3\n",
      "91.0\n",
      "94.6\n",
      "90.5\n",
      "91.4\n",
      "95.0\n",
      "75.6\n",
      "89.3\n",
      "cpt-text M\n",
      "89.8\n",
      "92.7\n",
      "95.7\n",
      "91.3\n",
      "95.3\n",
      "96.6\n",
      "76.5\n",
      "91.1\n",
      "cpt-text L\n",
      "90.8\n",
      "93.5\n",
      "96.2\n",
      "91.2\n",
      "95.7\n",
      "96.0\n",
      "76.9\n",
      "91.5\n",
      "cpt-text XL\n",
      "92.4\n",
      "93.9\n",
      "97.0\n",
      "91.8\n",
      "95.8\n",
      "96.4\n",
      "78.1\n",
      "92.2\n",
      " Table 2. cpt-text models of different sizes, ranging from 300M (S) to 175B (XL), are compared to previous work on linear-probe\n",
      "classiﬁcation tasks in SentEval. We report performance of unsupervised models, as well as those ﬁne-tuned on NLI data.\n",
      " Method\n",
      "Accuracy\n",
      " Zero-shot\n",
      "88.1\n",
      "Zero-shot with prompting\n",
      "89.1\n",
      "k-NN\n",
      "93.3\n",
      "Linear-probe\n",
      "95.7\n",
      "Full ﬁne-tuned SOTA\n",
      "97.5\n",
      " Table 3. Comparison of different classiﬁcation strategies using\n",
      "the 6B cpt-text model ﬁne-tuned on NLI data for SST-2 bi-\n",
      "nary sentiment task (Socher et al., 2013). Our zero-shot results\n",
      "are better than the 85.4% accuracy obtained by supervised neural\n",
      "networks reported along with the release of the dataset (Socher\n",
      "et al., 2013).\n",
      " STS -12\n",
      "-13\n",
      "-14\n",
      "-15\n",
      "-16\n",
      "Avg\n",
      " Unsupervised\n",
      " SimCSE (Gao et al., 2021) 72.9 84.0 75.6 84.8 81.8 79.8\n",
      "cpt-text S\n",
      "62.1 60.0 62.0 71.8 73.7 65.9\n",
      "cpt-text M\n",
      "62.7 62.8 64.6 73.9 75.3 67.9\n",
      "cpt-text L\n",
      "62.4 66.4 67.6 76.0 77.5 70.0\n",
      "cpt-text XL\n",
      "64.1 67.5 68.4 76.7 78.7 71.1\n",
      " Transfer from NLI\n",
      " SimCSE (Gao et al., 2021) 77.5 87.3 82.4 86.7 83.9 83.6\n",
      "cpt-text S\n",
      "72.8 80.6 78.7 84.7 82.0 79.8\n",
      "cpt-text M\n",
      "73.7 80.2 78.9 85.0 82.8 80.1\n",
      "cpt-text L\n",
      "71.8 79.7 79.0 85.8 84.0 80.1\n",
      "cpt-text XL\n",
      "72.3 80.3 78.9 85.1 85.1 80.3\n",
      " Table 4. cpt-text performs worse than the previous best sen-\n",
      "tence embedding method on sentence similarity tasks. We inves-\n",
      "tigate this result in more detail in Section 3.4.2.\n",
      " MSMARCO\n",
      "NQ\n",
      "TriviaQA\n",
      " Fine-tuned SOTA\n",
      "44.3\n",
      "84.8, 89.8\n",
      "84.1, 87.8\n",
      " Unsupervised\n",
      " BM25\n",
      "18.4\n",
      "62.9, 78.3\n",
      "76.4, 83.2\n",
      "ICT\n",
      "-\n",
      "50.9, 66.8\n",
      "57.5, 73.6\n",
      "MSS\n",
      "-\n",
      "59.8, 74.9\n",
      "68.2, 79.4\n",
      "Contriever\n",
      "-\n",
      "67.2, 81.3\n",
      "74.2, 83.2\n",
      " cpt-text S\n",
      "19.9\n",
      "65.5, 77.2\n",
      "75.1, 81.7\n",
      "cpt-text M\n",
      "20.6\n",
      "68.7, 79.6\n",
      "78.0, 83.8\n",
      "cpt-text L\n",
      "21.5\n",
      "73.0, 83.4\n",
      "80.0, 86.8\n",
      "cpt-text XL\n",
      "22.7\n",
      "78.8, 86.8\n",
      "82.1, 86.9\n",
      " Table 5. Evaluation of unsupervised cpt-text models of differ-\n",
      "ent sizes on several large-scale text search benchmarks. We report\n",
      "MRR@10 on MSMARCO and Recall@20, Recall@100 for NQ\n",
      "and TriviaQA as done in prior work. Results for training with\n",
      "Inverse Cloze Task (ICT) and masked salient spans (MSS) objec-\n",
      "tives are taken from Sachan et al. (2021). cpt-text achieves the\n",
      "best results among unsupervised methods, surpassing keyword\n",
      "search methods on MSMARCO (Robertson, 2009) and embed-\n",
      "ding based methods (Izacard et al., 2021) on NQ and TriviaQA.\n",
      " Text and Code Embeddings by Contrastive Pre-Training\n",
      " 3.2.2. BEIR SEARCH\n",
      " Next, we evaluate our models on 11 zero-shot search tasks\n",
      "in the BEIR evaluation suite (Thakur et al., 2021). First,\n",
      "we observe that our unsupervised model performs compet-\n",
      "itively even with some previous embedding methods that\n",
      "leverage supervised MSMARCO data (Xiong et al., 2020;\n",
      "Hofst¨atter et al., 2021). Keyword-based BM25 (Robertson,\n",
      "2009) achieves the best results in the unsupervised setting\n",
      "while cpt-text achieves the best transfer learning re-\n",
      "sults.\n",
      " In the transfer setting, our models achieve a 5.2% relative\n",
      "improvement over the previous best embedding method\n",
      "(Izacard et al., 2021).\n",
      "It also outperforms docT5query\n",
      "(Nogueira et al., 2019a) that relies on a ﬁne-tuned T5 model\n",
      "(Raffel et al., 2019) for document expansion. cpt-text\n",
      "results are competitive even with methods that use sub-\n",
      "stantially more compute at test time. BM25+CE (Wang\n",
      "et al., 2020) uses keyword search to select top 100 docu-\n",
      "ments which are then re-ranked by a cross-attention neural\n",
      "network encoder. The ranking encoder network performs\n",
      "computationally expensive joint query and document atten-\n",
      "tion and cannot exploit indexing and approximate nearest\n",
      "neighbor algorithms for fast and efﬁcient search at query\n",
      "time. Several other existing work take this approach of\n",
      "leveraging more computation resources at query time to ob-\n",
      "tain better search performance. ColBERT v2 (Santhanam\n",
      "et al., 2021) is a multi-vector method that represents the\n",
      "query and the documents as a set of vectors, and employs\n",
      "a multi-step retrieval procedure to obtain relevant docu-\n",
      "ments. Splade v2 (Formal et al., 2021) represents queries\n",
      "and documents as sparse vectors of size equivalent to the\n",
      "vocabulary of the BERT encoder (Devlin et al., 2019). Our\n",
      "cpt-text models compute only one dense embedding\n",
      "per document which are indexed ofﬂine and does not de-\n",
      "pend on any cross-attention re-ranker at query time.\n",
      " 3.3. Code Search\n",
      " We evaluate our code embedding models on the code\n",
      "search task using the CodeSearchNet benchmark (Husain\n",
      "et al., 2020). Given a natural language query, the model\n",
      "is expected to retrieve the relevant code block among 1K\n",
      "candidates. The models are evaluated on 6 programming\n",
      "languages and our model achieves state-of-the-art results\n",
      "(Table 7). Unlike with text embeddings, we do not see a\n",
      "performance improvement with increased model size for\n",
      "code embeddings.\n",
      " We also evaluate on a harder setting of ﬁnding the relevant\n",
      "code block among 10K candidates instead of 1K. Here, we\n",
      "compare the performance of cpt-text models against\n",
      "cpt-code models (Table 8). It is interesting to see that\n",
      "text embedding performs fairly well in code search espe-\n",
      "cially in Python. We see a drop in performance for code\n",
      " embedding models with increased distractors and still don’t\n",
      "see bigger models giving a boost in search performance.\n",
      " 3.4. Analysis\n",
      " 3.4.1. EFFECT OF BATCH SIZE\n",
      " Our ablation study highlights the effect of the model’s\n",
      "batch size on the ﬁnal performance. Table 9 compares the\n",
      "performance of S (300M) cpt-text model trained with\n",
      "different batch sizes on the NQ development set. Since we\n",
      "train with in-batch negatives, a larger batch increases the\n",
      "chances of having hard negatives in a batch, resulting in a\n",
      "signiﬁcant performance boost.\n",
      " 3.4.2. TRAINING BEHAVIOR\n",
      " We observe that as we train our models for longer, the\n",
      "performance on search and classiﬁcation tasks increases\n",
      "while the performance on sentence similarity tasks de-\n",
      "creases (Figure 4). As discussed previously, sentence simi-\n",
      "larity is not a well deﬁned task. A hypothesis is that search\n",
      "tasks and sentence similarity tasks might have contradict-\n",
      "ing deﬁnitions. For example, a sentence and its negation\n",
      "could be considered as relevant during search, but not “sim-\n",
      "ilar” in sentence similarity tasks. It is also important to\n",
      "note that previous embedding search methods do not report\n",
      "performance on sentence similarity tasks (Karpukhin et al.,\n",
      "2020a; Sachan et al., 2021; Izacard et al., 2021) and previ-\n",
      "ous sentence embedding methods do not evaluate on search\n",
      "tasks (Reimers & Gurevych, 2019; Giorgi et al., 2020; Gao\n",
      "et al., 2021). When deciding the model checkpoints to use\n",
      "for evaluation, we assigned higher importance to search\n",
      "and classiﬁcation tasks as they are commonly associated\n",
      "with clearly deﬁned real-world applications while sentence\n",
      "similarity tasks are less so.\n",
      " 4. Related Work\n",
      " The goal of representation learning (Bengio et al., 2012)\n",
      "is to learn an embedding space in which similar examples\n",
      "stay close to each other while dissimilar ones are far apart\n",
      "(Hadsell et al., 2006). In contrastive learning, the learning\n",
      "procedure is formulated as a classiﬁcation problem given\n",
      "similar and dissimilar candidates (Chopra et al., 2005; Gut-\n",
      "mann & Hyv¨arinen, 2010; Schroff et al., 2015; Sohn, 2016;\n",
      "van den Oord et al., 2018). Recent work relies on con-\n",
      "trastive objective to learn representations for images (Wu\n",
      "et al., 2018; He et al., 2020; Chen et al., 2020; Zbontar\n",
      "et al., 2021), text, or both jointly (Lu et al., 2019; Sun\n",
      "et al., 2019; Kim et al., 2021; Radford et al., 2021; Khosla\n",
      "et al., 2020). In self-supervised contrastive learning, pos-\n",
      "itive samples can be collected in various approaches in-\n",
      "cluding by creating an augmented version of the origi-\n",
      "nal input without modifying the semantic meaning (Gao\n",
      " Text and Code Embeddings by Contrastive Pre-Training\n",
      " covid nfc\n",
      "ﬁqa\n",
      "arg. touche quora scifact climate dbp. hotpot fever Avg.\n",
      " Unsupervised\n",
      " BM25 (Robertson, 2009)\n",
      "65.6 32.5 23.6 31.5\n",
      "36.7\n",
      "78.9\n",
      "66.5\n",
      "21.3\n",
      "31.3\n",
      "60.3\n",
      "75.3 47.6\n",
      "Contriever (Izacard et al., 2021)\n",
      "27.4 31.7 24.5 37.9\n",
      "19.3\n",
      "83.5\n",
      "64.9\n",
      "15.5\n",
      "29.2\n",
      "48.1\n",
      "68.2 40.9\n",
      " cpt-text S\n",
      "52.9 32.0 34.1 38.7\n",
      "21.0\n",
      "68.1\n",
      "65.4\n",
      "15.8\n",
      "27.2\n",
      "51.5\n",
      "57.1 42.2\n",
      "cpt-text M\n",
      "44.3 34.5 37.3 41.2\n",
      "23.3\n",
      "70.3\n",
      "68.3\n",
      "15.6\n",
      "29.6\n",
      "53.0\n",
      "58.2 43.2\n",
      "cpt-text L\n",
      "42.7 36.9 39.7 39.2\n",
      "22.8\n",
      "68.7\n",
      "71.2\n",
      "16.1\n",
      "31.2\n",
      "54.3\n",
      "63.8 44.2\n",
      " Transfer from MSMARCO\n",
      " TAS-B (Hofst¨atter et al., 2021)\n",
      "48.1 31.9 30.0 42.9\n",
      "16.2\n",
      "83.5\n",
      "64.3\n",
      "22.8\n",
      "38.4\n",
      "58.4\n",
      "70.0 46.0\n",
      "ANCE (Xiong et al., 2020)\n",
      "65.4 23.7 29.5 41.5\n",
      "24.0\n",
      "85.2\n",
      "50.7\n",
      "19.8\n",
      "28.1\n",
      "45.6\n",
      "66.9 43.7\n",
      "Contriever (Izacard et al., 2021)\n",
      "59.6 32.8 32.9 44.6\n",
      "23.0\n",
      "86.5\n",
      "67.7\n",
      "23.7\n",
      "41.3\n",
      "63.8\n",
      "75.8 50.2\n",
      " cpt-text S\n",
      "67.9 33.2 38.4 47.0\n",
      "28.5\n",
      "70.6\n",
      "67.2\n",
      "18.5\n",
      "36.2\n",
      "59.4\n",
      "72.1 49.0\n",
      "cpt-text M\n",
      "58.5 36.7 42.2 49.2\n",
      "29.7\n",
      "69.7\n",
      "70.4\n",
      "19.9\n",
      "38.6\n",
      "63.1\n",
      "77.0 50.5\n",
      "cpt-text L\n",
      "56.2 38.0 45.2 46.9\n",
      "30.9\n",
      "67.7\n",
      "74.4\n",
      "19.4\n",
      "41.2\n",
      "64.8\n",
      "75.6 50.9\n",
      "cpt-text XL\n",
      "64.9 40.7 51.2 43.5\n",
      "29.1\n",
      "63.8\n",
      "75.4\n",
      "22.3\n",
      "43.2\n",
      "68.8\n",
      "77.5 52.8\n",
      " docT5query (Nogueira et al., 2019a)\n",
      "71.3 32.8 29.1 34.9\n",
      "34.7\n",
      "80.2\n",
      "67.5\n",
      "20.1\n",
      "33.1\n",
      "58.0\n",
      "71.4 48.5\n",
      "BM25+CE (Wang et al., 2020)\n",
      "75.7 35.0 34.7 31.1\n",
      "27.1\n",
      "82.5\n",
      "68.8\n",
      "25.3\n",
      "39.2\n",
      "70.7\n",
      "81.9 52.0\n",
      "ColBERT v2 (Santhanam et al., 2021) 73.8 33.8 35.6 46.3\n",
      "26.3\n",
      "85.2\n",
      "69.3\n",
      "17.6\n",
      "44.6\n",
      "66.7\n",
      "78.5 52.5\n",
      "Splade v2 (Formal et al., 2021)\n",
      "71.0 33.4 33.6 47.9\n",
      "27.2\n",
      "83.8\n",
      "69.3\n",
      "23.5\n",
      "43.5\n",
      "68.4\n",
      "78.6 52.7\n",
      " Table 6. Comparison of cpt-text to previous methods on 11 zero-shot search tasks in the BEIR evaluation suite (Thakur et al.,\n",
      "2021). Results are reported both in the unsupervised data setting and in the transfer data setting. cpt-text outperforms previous best\n",
      "embedding methods (Xiong et al., 2020; Hofst¨atter et al., 2021; Izacard et al., 2021) in both the settings. In the unsupervised setting,\n",
      "BM25 (Robertson, 2009) still achieves the best performance while in the transfer setting cpt-text is competitive with methods that\n",
      "use substantially more compute at test time (Wang et al., 2020; Santhanam et al., 2021; Formal et al., 2021).\n",
      " Go\n",
      "Ruby Python Java\n",
      "JS\n",
      "PHP Avg.\n",
      " CodeBERT\n",
      "69.3\n",
      "70.6\n",
      "84.0\n",
      "86.8 74.8 70.6 76.0\n",
      "GraphCodeBERT 84.1\n",
      "73.2\n",
      "87.9\n",
      "75.7 71.1 72.5 77.4\n",
      " cpt-code S\n",
      "97.7\n",
      "86.3\n",
      "99.8\n",
      "94.0 86.0 96.7 93.4\n",
      "cpt-code M\n",
      "97.5\n",
      "85.5\n",
      "99.9\n",
      "94.4 86.5 97.2 93.5\n",
      " Table 7. Comparison of cpt-code on code search across 6 pro-\n",
      "gramming languages (Husain et al., 2020) with CodeBERT (Feng\n",
      "et al., 2020) and GraphCodeBERT (Guo et al., 2021). The task re-\n",
      "quires ﬁnding the relevant code block among 1K candidates for a\n",
      "given natural language query. cpt-code performs substantially\n",
      "better than previous methods on all the languages.\n",
      " Go\n",
      "Ruby Python Java\n",
      "JS\n",
      "PHP Avg.\n",
      " cpt-text S\n",
      "60.6\n",
      "58.9\n",
      "92.6\n",
      "48.4 52.8 47.6\n",
      "60.1\n",
      "cpt-text M 65.4\n",
      "63.1\n",
      "91.4\n",
      "47.9 53.5 43.1\n",
      "60.7\n",
      " cpt-code S\n",
      "90.4\n",
      "80.6\n",
      "98.8\n",
      "81.9 76.1 85.3\n",
      "85.5\n",
      "cpt-code M 90.0\n",
      "89.1\n",
      "98.9\n",
      "81.1 75.6 85.1\n",
      "85.0\n",
      " Table 8. Comparison of cpt-code vs cpt-text on large scale\n",
      "code search (Husain et al., 2020). The task is to retrieve the rel-\n",
      "evant code block among 10K candidates for a given natural lan-\n",
      "guage query. It is interesting to note that cpt-text performs\n",
      "quite well on Python code search without explicitly training on\n",
      "(text, code) pairs.\n",
      " Batch Size\n",
      "MRR@10\n",
      " 1536\n",
      "71.4\n",
      "12288\n",
      "84.7\n",
      " Table 9. Performance of the cpt-text 300M model on NQ dev\n",
      "set given different training batch sizes.\n",
      " et al., 2021), by grouping samples within the same context\n",
      "(Giorgi et al., 2020; Izacard et al., 2021), or by collecting\n",
      "data about the same object from different views (Tian et al.,\n",
      "2019).\n",
      " Learning word embeddings is a well studied research area\n",
      "(Brown et al., 1992; Gutmann & Hyv¨arinen, 2010; Mikolov\n",
      "et al., 2013; Pennington et al., 2014).\n",
      "Learning low-\n",
      "dimensional representations of larger text pieces, denser\n",
      "than raw term-based vectors, has been studied extensively\n",
      "as well (Deerwester et al., 1990; Yih et al., 2011). Most\n",
      "of the recent models for learning sentence embeddings rely\n",
      "on supervised NLI datasets, using entailment pairs as pos-\n",
      "itive examples and contradiction pairs as (hard) negatives.\n",
      "SBERT (Reimers & Gurevych, 2019) trained a siamese net-\n",
      "work to learn a representation where sentence similarity is\n",
      "estimated by the cosine similarity between embeddings. Li\n",
      "et al. (2020) improves the embedding space to be isotropic\n",
      " Text and Code Embeddings by Contrastive Pre-Training\n",
      " 85\n",
      " 86\n",
      " 87\n",
      " 88\n",
      " 89\n",
      " 90\n",
      " Performance\n",
      " Senteval\n",
      "NQ\n",
      " 0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      " Training Steps\n",
      " 65\n",
      " 70\n",
      " 75\n",
      " Performance\n",
      " sts12\n",
      "sts13\n",
      "sts14\n",
      "sts15\n",
      "sts16\n",
      " Training Behavior\n",
      " Figure 4. Performance of M (1.2B) cpt-text model on classi-\n",
      "ﬁcation, search and sentence similarity tasks at different training\n",
      "steps. While the performance on search and classiﬁcation im-\n",
      "proves with longer training, the performance on sentence similar-\n",
      "ity degrades.\n",
      " via normalizing ﬂows. The whitening operation is another\n",
      "alternative operation to improve the isotropy of the embed-\n",
      "ding space (Su et al., 2021). It is typical to initialize such\n",
      "models with a pre-trained language model (Devlin et al.,\n",
      "2019) before training on NLI datasets.\n",
      " Several methods have been studied for unsupervised or\n",
      "self-supervised sentence embedding learning (Logeswaran\n",
      "& Lee, 2018; Zhang et al., 2020; Gao et al., 2021). Com-\n",
      "mon approaches consider sentences within the same con-\n",
      "text as semantically similar samples (Kiros et al., 2015; Lo-\n",
      "geswaran & Lee, 2018). To create positive training pairs\n",
      "with augmented samples, a diverse set of text augmen-\n",
      "tation operations have been explored, including lexicon-\n",
      "based distortion (Wei & Zou, 2019), synonym replacement\n",
      "(Kobayashi, 2018), back-translation (Fang & Xie, 2020),\n",
      "cut-off (Shen et al., 2020) and dropout (Gao et al., 2021).\n",
      "However, unsupervised sentence embedding models still\n",
      "perform notably worse than supervised sentence encoders.\n",
      " Large-scale text search based on dense embeddings and\n",
      "neural information retrieval (neural IR) have the poten-\n",
      "tial to generalize better than keyword matching in classic\n",
      "IR systems. Neural IR systems encode documents at the\n",
      "indexing stage and then perform nearest neighbor search\n",
      "(Johnson et al., 2019) at query time (Lin et al., 2021).\n",
      "Neural IR models are usually learned by ﬁne-tuning a pre-\n",
      "trained language model on supervised search corpus (Lee\n",
      "et al.; Guu et al., 2020; Karpukhin et al., 2020b; Lewis\n",
      "et al., 2020). Many SOTA search models combine classical\n",
      "IR with neural IR in a staged setup, where the candidates\n",
      "are ﬁrst narrowed down by BM25 keyword search (Robert-\n",
      "son, 2009) and then re-ranked by joint query and document\n",
      " neural encoders (Nogueira et al., 2019b; Qu et al., 2021).\n",
      "Xiong et al. (2020) proposed ANCE, a contrastive learn-\n",
      "ing framework for learning text representations for dense\n",
      "retrieval using mined hard negatives. Other unsupervised\n",
      "retriever methods use the Inverse Cloze Task or masked\n",
      "salient spans to achieve signiﬁcant improvement on ODQA\n",
      "tasks (Sachan et al., 2021). In comparison to most prior\n",
      "work, we ﬁnd that with a large enough batch size, it is\n",
      "possible to achieve good search performance without us-\n",
      "ing supervised data. Finally, the recently published Con-\n",
      "triever (Izacard et al., 2021) is most similar to our work on\n",
      "learning text embeddings for text search using contrastive\n",
      "learning on unlabeled data.\n",
      " Semantic code search refers to the task of retrieving code\n",
      "relevant to a query in natural language. The CodeSearch-\n",
      "Net challenge (Husain et al., 2020) presents a set of bench-\n",
      "mark code search tasks in different programming lan-\n",
      "guages, as well as a simple baseline model to predict em-\n",
      "beddings of query and code via contrastive learning on a\n",
      "dataset of (text, code) pairs. ContraCode (Jain et al., 2021)\n",
      "uses a contrastive learning task of identifying functionally\n",
      "similar programs, where the functionally similar samples\n",
      "are generated via source-to-source compiler transforma-\n",
      "tions. CodeBERT (Feng et al., 2020) learns to predict se-\n",
      "mantic similarity with a pre-trained language model and\n",
      "GraphCodeBERT (Guo et al., 2021) further improves the\n",
      "performance on the CodeSearchNet benchmark by adding\n",
      "pre-training tasks on code structure.\n",
      " 5. Broader Impacts\n",
      " Prior research has shown that text representation models\n",
      "encode the biases present in their training data, including\n",
      "those which are discriminatory towards protected groups\n",
      "such as Black people or women (Bolukbasi et al., 2016;\n",
      "Caliskan et al., 2017; May et al., 2019; Zhao et al., 2018;\n",
      "Rudinger et al., 2018). Biases encoded in embedding mod-\n",
      "els may cause representational harms3 by reinforcing exis-\n",
      "tent societal biases in the text corpus, and further propagat-\n",
      "ing them in downstream tasks of embedding models.\n",
      " Therefore, we encourage further research on two research\n",
      "agendas: (a) developing robust evaluation methodologies\n",
      "for multiple classes of bias in training data and pre-trained\n",
      "models, and (b) developing and improving methods for\n",
      "mitigating encoded bias, including ﬁne-tuning to reduce\n",
      "bias in pre-trained models (Caliskan et al., 2017; May et al.,\n",
      "2019; Bolukbasi et al., 2016; Liang et al., 2020; Park et al.,\n",
      "2018; Solaiman & Dennison, 2021). Until we have robust\n",
      "evaluation methodology, it is important to restrict and mon-\n",
      "itor the use of the model in downstream applications. Par-\n",
      " 3Representational harms occur when systems reinforce the\n",
      "subordination of some groups along the lines of identity, e.g.\n",
      "stereotyping or denigration (Crawford, 2017).\n",
      " Text and Code Embeddings by Contrastive Pre-Training\n",
      " ticularly for those where risk of representational harm is\n",
      "great and those where biased representations may inﬂuence\n",
      "the allocation of resources and opportunities to people.\n",
      " Our embedding models are trained with large batch sizes\n",
      "and require substantial computation resources. While this\n",
      "training regime is environmentally and computationally\n",
      "costly, there are promising paths forward to amortize and\n",
      "offset these costs while allowing users to beneﬁts from\n",
      "the capabilities of these models. For example, safe public\n",
      "access to large pre-trained language models, and efﬁcient\n",
      "training pipelines that leverage improved model architec-\n",
      "tures and training schemes. We encourage further research\n",
      "and implementation efforts in these areas.\n",
      " 6. Conclusion\n",
      " We showed that contrastive pre-training on unsupervised\n",
      "data with a sufﬁciently large batch size can lead to high\n",
      "quality vector representations of text and code. Our models\n",
      "achieved new state-of-the-art results in linear-probe classi-\n",
      "ﬁcation, text search and code search. We ﬁnd that our mod-\n",
      "els underperformed on sentence similarity tasks and ob-\n",
      "served unexpected training behavior with respect to these\n",
      "tasks. Finally, we discussed the broader impact of our work\n",
      "on society.\n",
      " References\n",
      " Bengio, Y., Courville, A. C., and Vincent, P. Representa-\n",
      "tion learning: A review and new perspectives. Transac-\n",
      "tions on pattern analysis and machine intelligence, 35\n",
      "(8), 2012.\n",
      " Bolukbasi, T., Chang, K., Zou, J. Y., Saligrama, V., and\n",
      "Kalai, A. Man is to computer programmer as woman is\n",
      "to homemaker? debiasing word embeddings. 29, 2016.\n",
      " Bowman, S. R., Angeli, G., Potts, C., and Manning, C. D.\n",
      "A large annotated corpus for learning natural language\n",
      "inference. In Conference on Empirical Methods in Nat-\n",
      "ural Language Processing (EMNLP). ACL, 2015.\n",
      " Brown, P. F., Della Pietra, V. J., deSouza, P. V., Lai, J. C.,\n",
      "and Mercer, R. L. Class-based n-gram models of nat-\n",
      "ural language. Computational Linguistics, 18(4):467–\n",
      "480, 1992.\n",
      " Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan,\n",
      "J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry,\n",
      "G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger,\n",
      "G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.,\n",
      "Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E.,\n",
      "Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C.,\n",
      "McCandlish, S., Radford, A., Sutskever, I., and Amodei,\n",
      " D. Language models are few-shot learners. In Advances\n",
      "in Neural Information Processing Systems, 2020.\n",
      " Caliskan, A., Bryson, J. J., and Narayanan, A.\n",
      "Seman-\n",
      "tics derived automatically from language corpora contain\n",
      "human-like biases. Science, 356(6334):183–186, 2017.\n",
      " Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto,\n",
      "H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N.,\n",
      "Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov,\n",
      "M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray,\n",
      "S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavar-\n",
      "ian, M., Winter, C., Tillet, P., Such, F. P., Cummings,\n",
      "D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss,\n",
      "A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang,\n",
      "J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W.,\n",
      "Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra,\n",
      "V., Morikawa, E., Radford, A., Knight, M., Brundage,\n",
      "M., Murati, M., Mayer, K., Welinder, P., McGrew, B.,\n",
      "Amodei, D., McCandlish, S., Sutskever, I., and Zaremba,\n",
      "W. Evaluating large language models trained on code.\n",
      "arXiv preprint arXiv:2107.03374, 2021.\n",
      " Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. E.\n",
      "A simple framework for contrastive learning of visual\n",
      "representations. In International conference on machine\n",
      "learning (ICML), 2020.\n",
      " Chopra, S., Hadsell, R., and LeCun, Y. Learning a similar-\n",
      "ity metric discriminatively, with application to face ver-\n",
      "iﬁcation. In Computer Vision and Pattern Recognition\n",
      "(CVPR). IEEE, 2005.\n",
      " Conneau, A. and Kiela, D. Senteval: An evaluation toolkit\n",
      "for universal sentence representations.\n",
      "arXiv preprint\n",
      "arXiv:1803.05449, 2018.\n",
      " Crawford, K. The trouble with bias. Keynote at NeurIPS,\n",
      "2017.\n",
      " Deerwester, S., Dumais, S., Furnas, G., Landauer, T., and\n",
      "Harshman, R.\n",
      "Indexing by latent semantic analysis.\n",
      "Journal of the American society for information science,\n",
      "41(6):391–407, 1990.\n",
      " Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.\n",
      "BERT: Pre-training of deep bidirectional transformers\n",
      "for language understanding. In Conference of the North\n",
      "American Chapter of the Association for Computational\n",
      "Linguistics (NAACL). ACL, 2019.\n",
      " Fang, H. and Xie, P.\n",
      "CERT: contrastive self-supervised\n",
      "learning for language understanding.\n",
      "arXiv preprint\n",
      "arXiv:2005.12766, 2020.\n",
      " Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M.,\n",
      "Shou, L., Qin, B., Liu, T., Jiang, D., and Zhou, M. Code-\n",
      "bert: A pre-trained model for programming and natural\n",
      " Text and Code Embeddings by Contrastive Pre-Training\n",
      " languages. In Conference on Empirical Methods in Nat-\n",
      "ural Language Processing (EMNLP), 2020.\n",
      " Formal, T., Lassance, C., Piwowarski, B., and Clinchant,\n",
      "S. SPLADE v2: Sparse lexical and expansion model for\n",
      "information retrieval. arXiv preprint arXiv:2109.10086,\n",
      "2021.\n",
      " Gao, T., Yao, X., and Chen, D.\n",
      "SimCSE: Simple con-\n",
      "trastive learning of sentence embeddings. In Conference\n",
      "on Empirical Methods in Natural Language Processing\n",
      "(EMNLP), 2021.\n",
      " Giorgi, J. M., Nitski, O., Bader, G. D., and Wang, B. De-\n",
      "clutr: Deep contrastive learning for unsupervised textual\n",
      "representations. In Proceedings of ACL/IJCNLP, 2020.\n",
      " Goodman, N. Seven strictures on similarity. Bobbs Merrill,\n",
      "1972.\n",
      " Guo, D., Ren, S., Lu, S., Feng, Z., Tang, D., Liu, S.,\n",
      "Zhou, L., Duan, N., Svyatkovskiy, A., Fu, S., Tufano,\n",
      "M., Deng, S. K., Clement, C. B., Drain, D., Sundaresan,\n",
      "N., Yin, J., Jiang, D., and Zhou, M. Graphcodebert: Pre-\n",
      "training code representations with data ﬂow. In Interna-\n",
      "tional Conference on Learning Representation (ICLR),\n",
      "2021.\n",
      " Gutmann, M. and Hyv¨arinen, A. Noise-contrastive estima-\n",
      "tion: A new estimation principle for unnormalized sta-\n",
      "tistical models. In Conference on Artiﬁcial Intelligence\n",
      "and Statistics. PMLR, 2010.\n",
      " Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang,\n",
      "M. REALM: retrieval-augmented language model pre-\n",
      "training. arXiv preprint arXiv:2002.08909, 2020.\n",
      " Hadsell, R., Chopra, S., and LeCun, Y.\n",
      "Dimensionality\n",
      "reduction by learning an invariant mapping. In Computer\n",
      "Vision and Pattern Recognition (CVPR), volume 2, pp.\n",
      "1735–1742. IEEE, 2006.\n",
      " He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. B. Mo-\n",
      "mentum contrast for unsupervised visual representation\n",
      "learning. In Computer Vision and Pattern Recognition\n",
      "(CVPR), 2020.\n",
      " Hofst¨atter, S., Lin, S., Yang, J., Lin, J., and Hanbury,\n",
      "A.\n",
      "Efﬁciently teaching an effective dense retriever\n",
      "with balanced topic aware sampling.\n",
      "arXiv preprint\n",
      "arXiv:2104.06967, 2021.\n",
      " Husain, H., Wu, H.-H., Gazit, T., Allamanis, M., and\n",
      "Brockschmidt, M. CodeSearchNet challenge: Evaluat-\n",
      "ing the state of semantic code search.\n",
      "arXiv preprint\n",
      "arXiv:1909.09436, 2020.\n",
      " Izacard, G., Caron, M., Hosseini, L., Riedel, S., Bo-\n",
      "janowski, P., Joulin, A., and Grave, E. Towards unsuper-\n",
      "vised dense information retrieval with contrastive learn-\n",
      "ing. arXiv preprint arXiv:2112.09118, 2021.\n",
      " Jain, P., Jain, A., Zhang, T., Abbeel, P., Gonzalez, J. E., and\n",
      "Stoica, I. Contrastive code representation learning. In\n",
      "Conference on Empirical Methods in Natural Language\n",
      "Processing (EMNLP), 2021.\n",
      " Jia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham,\n",
      "H., Le, Q. V., Sung, Y., Li, Z., and Duerig, T. Scaling up\n",
      "visual and vision-language representation learning with\n",
      "noisy text supervision. In International Conference on\n",
      "Machine Learning (ICML), 2021.\n",
      " Johnson, J., Douze, M., and J´egou, H. Billion-scale simi-\n",
      "larity search with gpus. IEEE Transactions on Big Data,\n",
      "2019.\n",
      " Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L.\n",
      "TriviaQA: A large scale distantly supervised challenge\n",
      "dataset for reading comprehension. In Conference of the\n",
      "Association for Computational Linguistics (ACL). ACL,\n",
      "2017.\n",
      " Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L.,\n",
      "Edunov, S., Chen, D., and Yih, W.-t.\n",
      "Dense passage\n",
      "retrieval for open-domain question answering. In Con-\n",
      "ference on Empirical Methods in Natural Language Pro-\n",
      "cessing (EMNLP), 2020a.\n",
      " Karpukhin, V., Oguz, B., Min, S., Wu, L., Edunov, S.,\n",
      "Chen, D., and Yih, W. Dense passage retrieval for open-\n",
      "domain question answering. In Conference on Empiri-\n",
      "cal Methods in Natural Language Processing (EMNLP),\n",
      "2020b.\n",
      " Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian,\n",
      "Y., Isola, P., Maschinot, A., Liu, C., and Krishnan,\n",
      "D.\n",
      "Supervised contrastive learning.\n",
      "arXiv preprint\n",
      "arXiv:2004.11362, 2020.\n",
      " Kim, W., Son, B., and Kim, I.\n",
      "Vilt:\n",
      "Vision-and-\n",
      "language transformer without convolution or region su-\n",
      "pervision.\n",
      "In International Conference on Machine\n",
      "Learning (ICML), 2021.\n",
      " Kingma, D. P. and Welling, M. Auto-Encoding Variational\n",
      "Bayes. In International Conference on Learning Repre-\n",
      "sentation (ICLR), 2014.\n",
      " Kiros, J., Zhu, Y., Salakhutdinov, R., Zemel, R. S., Tor-\n",
      "ralba, A., Urtasun, R., and Fidler, S. Skip-thought vec-\n",
      "tors. In Advances in Neural Information Processing Sys-\n",
      "tems (NeuriPS), 2015.\n",
      " Text and Code Embeddings by Contrastive Pre-Training\n",
      " Kobayashi, S.\n",
      "Contextual augmentation: Data augmen-\n",
      "tation by words with paradigmatic relations.\n",
      "arXiv\n",
      "preprint arXiv:1805.06201, 2018.\n",
      " Kwiatkowski, T., Palomaki, J., Redﬁeld, O., Collins, M.,\n",
      "Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Kel-\n",
      "cey, M., Devlin, J., Lee, K., Toutanova, K. N., Jones,\n",
      "L., Chang, M.-W., Dai, A., Uszkoreit, J., Le, Q., and\n",
      "Petrov, S. Natural questions: a benchmark for question\n",
      "answering research. Transactions of the Association of\n",
      "Computational Linguistics, 2019.\n",
      " Lee, K., Chang, M., and Toutanova, K. Latent retrieval\n",
      "for weakly supervised open domain question answering.\n",
      "In Korhonen, A., Traum, D. R., and M`arquez, L. (eds.),\n",
      "Conference of the Association for Computational Lin-\n",
      "guistics (ACL), pp. 6086–6096. ACL.\n",
      " Lewis, P. S. H., Perez, E., Piktus, A., Petroni, F.,\n",
      "Karpukhin, V., Goyal, N., K¨uttler, H., Lewis, M., Yih,\n",
      "W., Rockt¨aschel, T., Riedel, S., and Kiela, D. Retrieval-\n",
      "augmented generation for knowledge-intensive NLP\n",
      "tasks.\n",
      "In Advances in Neural Information Processing\n",
      "Systems (NeuriPS), 2020.\n",
      " Li, B., Zhou, H., He, J., Wang, M., Yang, Y., and Li, L.\n",
      "On the sentence embeddings from pre-trained language\n",
      "models. In Conference on Empirical Methods in Natural\n",
      "Language Processing (EMNLP), 2020.\n",
      " Liang, P. P., Li, I. M., Zheng, E., Lim, Y. C., Salakhutdinov,\n",
      "R., and Morency, L. Towards debiasing sentence repre-\n",
      "sentations. In Conference of the Association for Compu-\n",
      "tational Linguistics (ACL), 2020.\n",
      " Lin, J., Nogueira, R., and Yates, A. Pretrained transformers\n",
      "for text ranking: BERT and beyond. Synthesis Lectures\n",
      "on Human Language Technologies, 14(4):1–325, 2021.\n",
      " Logeswaran, L. and Lee, H. An efﬁcient framework for\n",
      "learning sentence representations. In International Con-\n",
      "ference on Learning Representation (ICLR), 2018.\n",
      " Lu, J., Batra, D., Parikh, D., and Lee, S.\n",
      "Vil-\n",
      "bert: Pretraining task-agnostic visiolinguistic represen-\n",
      "tations for vision-and-language tasks.\n",
      "arXiv preprint\n",
      "arXiv:1908.02265, 2019.\n",
      " May, C., Wang, A., Bordia, S., Bowman, S. R., and\n",
      "Rudinger, R. On measuring social biases in sentence en-\n",
      "coders. In Conference of the North American Chapter of\n",
      "the Association for Computational Linguistics (NAACL),\n",
      "2019.\n",
      " Mikolov, T., Chen, K., Corrado, G. S., and Dean, J. Efﬁ-\n",
      "cient estimation of word representations in vector space.\n",
      "arXiv preprint arXiv:1301.3781, 2013.\n",
      " Nguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary,\n",
      "S., Majumder, R., and Deng, L. MS MARCO: A hu-\n",
      "man generated machine reading comprehension dataset.\n",
      "arXiv preprint arXiv:1611.09268, 2016.\n",
      " Nogueira, R., Lin, J., and Epistemic, A. From doc2query\n",
      "to doctttttquery. Online preprint, 2019a.\n",
      " Nogueira, R., Yang, W., Cho, K., and Lin, J.\n",
      "Multi-\n",
      "stage document ranking with BERT.\n",
      "arXiv preprint\n",
      "arXiv:1910.14424, 2019b.\n",
      " Park, J. H., Shin, J., and Fung, P. Reducing gender bias in\n",
      "abusive language detection. In Conference on Empiri-\n",
      "cal Methods in Natural Language Processing (EMNLP),\n",
      "2018.\n",
      " Pennington, J., Socher, R., and Manning, C.\n",
      "GloVe:\n",
      "Global vectors for word representation. In Conference\n",
      "on Empirical Methods in Natural Language Processing\n",
      "(EMNLP), 2014.\n",
      " Peters, M. E., Neumann, M., Iyyer, M., Gardner, M.,\n",
      "Clark, C., Lee, K., and Zettlemoyer, L.\n",
      "Deep con-\n",
      "textualized word representations.\n",
      "In Proceedings of\n",
      "NCAAL/IJCNLP, 2018.\n",
      " Qu, Y., Ding, Y., Liu, J., Liu, K., Ren, R., Zhao, X., Dong,\n",
      "D., Wu, H., and Wang, H. Rocketqa: An optimized train-\n",
      "ing approach to dense passage retrieval for open-domain\n",
      "question answering. In Conference of the Association for\n",
      "Computational Linguistics (ACL), 2021.\n",
      " Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\n",
      "Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark,\n",
      "J., Krueger, G., and Sutskever, I. Learning transferable\n",
      "visual models from natural language supervision. arXiv\n",
      "preprint arXiv:2103.00020, 2021.\n",
      " Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\n",
      "Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring\n",
      "the limits of transfer learning with a uniﬁed text-to-text\n",
      "transformer. arXiv preprint arXiv:1910.10683, 2019.\n",
      " Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Rad-\n",
      "ford, A., Chen, M., and Sutskever, I.\n",
      "Zero-shot text-\n",
      "to-image generation. arXiv preprint arXiv:2102.12092,\n",
      "2021.\n",
      " Reimers, N. and Gurevych, I. Sentence-bert: Sentence em-\n",
      "beddings using siamese bert-networks. In Conference\n",
      "on Empirical Methods in Natural Language Processing\n",
      "(EMNLP), 2019.\n",
      " Robertson, S.\n",
      "The Probabilistic Relevance Framework:\n",
      "BM25 and Beyond. Foundations and Trends® in Infor-\n",
      "mation Retrieval, 2009.\n",
      " Text and Code Embeddings by Contrastive Pre-Training\n",
      " Rudinger, R., Naradowsky, J., Leonard, B., and Durme,\n",
      "B. V.\n",
      "Gender bias in coreference resolution.\n",
      "arXiv\n",
      "preprint arXiv:1804.09301, 2018.\n",
      " Sachan, D. S., Patwary, M., Shoeybi, M., Kant, N., Ping,\n",
      "W., Hamilton, W. L., and Catanzaro, B.\n",
      "End-to-end\n",
      "training of neural retrievers for open-domain question\n",
      "answering. In Zong, C., Xia, F., Li, W., and Navigli,\n",
      "R. (eds.), Proceedings of ACL/IJCNLP, pp. 6648–6662.\n",
      "ACL, 2021.\n",
      " Santhanam, K., Khattab, O., Saad-Falcon, J., Potts, C.,\n",
      "and Zaharia, M. Colbertv2: Effective and efﬁcient re-\n",
      "trieval via lightweight late interaction.\n",
      "arXiv preprint\n",
      "arXiv:2112.01488, 2021.\n",
      " Schroff, F., Kalenichenko, D., and Philbin, J. Facenet: A\n",
      "uniﬁed embedding for face recognition and clustering.\n",
      "In Computer Vision and Pattern Recognition (CVPR),\n",
      "2015.\n",
      " Shen, D., Zheng, M., Shen, Y., Qu, Y., and Chen, W. A\n",
      "simple but tough-to-beat data augmentation approach for\n",
      "natural language understanding and generation. arXiv\n",
      "preprint arXiv:2009.13818, 2020.\n",
      " Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning,\n",
      "C. D., Ng, A., and Potts, C. Recursive deep models for\n",
      "semantic compositionality over a sentiment treebank. In\n",
      "Conference on Empirical Methods in Natural Language\n",
      "Processing (EMNLP), 2013.\n",
      " Sohn, K. Improved deep metric learning with multi-class\n",
      "n-pair loss objective. In Advances in Neural Information\n",
      "Processing Systems (NeuriPS), 2016.\n",
      " Solaiman, I. and Dennison, C. Process for adapting lan-\n",
      "guage models to society (PALMS) with values-targeted\n",
      "datasets. arXiv preprint arXiv:2106.10328, 2021.\n",
      " Su, J., Cao, J., Liu, W., and Ou, Y. Whitening sentence\n",
      "representations for better semantics and faster retrieval.\n",
      "arXiv preprint arXiv:2103.15316, 2021.\n",
      " Sun, C., Myers, A., Vondrick, C., Murphy, K., and Schmid,\n",
      "C.\n",
      "Videobert: A joint model for video and language\n",
      "representation learning. In International Conference on\n",
      "Computer Vision (ICCV), 2019.\n",
      " Thakur, N., Reimers, N., R¨uckl´e, A., Srivastava, A., and\n",
      "Gurevych, I.\n",
      "BEIR: A heterogenous benchmark for\n",
      "zero-shot evaluation of information retrieval models.\n",
      "In Advances in Neural Information Processing Systems\n",
      "(NeuriPS), 2021.\n",
      " Tian, Y., Krishnan, D., and Isola, P.\n",
      "Contrastive multi-\n",
      "view coding. European Conference on Computer Vision\n",
      "(ECCV), 2019.\n",
      " van den Oord, A., Dieleman, S., Zen, H., Simonyan, K.,\n",
      "Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A.,\n",
      "and Kavukcuoglu, K. Wavenet: A generative model for\n",
      "raw audio. arXiv preprint arXiv:1609.03499, 2016.\n",
      " van den Oord, A., Li, Y., and Vinyals, O.\n",
      "Representa-\n",
      "tion learning with contrastive predictive coding. arXiv\n",
      "preprint arXiv:1807.03748, 2018.\n",
      " Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\n",
      "L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Atten-\n",
      "tion is all you need. In Advances in Neural Information\n",
      "Processing Systems (NeuriPS), 2017.\n",
      " Vervaeke, J., Lillicrap, T. P., and Richards, B. A. Relevance\n",
      "realization and the emerging framework in cognitive sci-\n",
      "ence. Journal of logic and computation, 22(1):79–99,\n",
      "2012.\n",
      " Wang, W., Wei, F., Dong, L., Bao, H., Yang, N., and Zhou,\n",
      "M.\n",
      "Minilm: Deep self-attention distillation for task-\n",
      "agnostic compression of pre-trained transformers. arXiv\n",
      "preprint arXiv:2002.10957, 2020.\n",
      " Wei, J. W. and Zou, K. EDA: easy data augmentation tech-\n",
      "niques for boosting performance on text classiﬁcation\n",
      "tasks. arXiv preprint arXiv:1901.11196, 2019.\n",
      " Williams, A., Nangia, N., and Bowman, S.\n",
      "A broad-\n",
      "coverage challenge corpus for sentence understanding\n",
      "through inference. In Conference of the North American\n",
      "Chapter of the Association for Computational Linguis-\n",
      "tics (NAACL). ACL, 2018.\n",
      " Wu, Z., Xiong, Y., Yu, S. X., and Lin, D. Unsupervised\n",
      "feature learning via non-parametric instance-level dis-\n",
      "crimination. In Computer Vision and Pattern Recogni-\n",
      "tion (CVPR), 2018.\n",
      " Xiong, L., Xiong, C., Li, Y., Tang, K., Liu, J., Bennett,\n",
      "P. N., Ahmed, J., and Overwijk, A. Approximate near-\n",
      "est neighbor negative contrastive learning for dense text\n",
      "retrieval. arXiv preprint arXiv:2007.00808, 2020.\n",
      " Yih, W.-t., Toutanova, K., Platt, J. C., and Meek, C. Learn-\n",
      "ing discriminative projections for text similarity mea-\n",
      "sures.\n",
      "In Conference on Computational Natural Lan-\n",
      "guage Learning (CoNLL). ACL, 2011.\n",
      " Zbontar, J., Jing, L., Misra, I., LeCun, Y., and Deny, S. Bar-\n",
      "low twins: Self-supervised learning via redundancy re-\n",
      "duction. In International Conference on Machine Learn-\n",
      "ing (ICML), 2021.\n",
      " Zhang, Y., He, R., Liu, Z., Lim, K. H., and Bing, L.\n",
      "An unsupervised sentence embedding method by mutual\n",
      "information maximization.\n",
      "In Conference on Empiri-\n",
      "cal Methods in Natural Language Processing (EMNLP),\n",
      "2020.\n",
      " Text and Code Embeddings by Contrastive Pre-Training\n",
      " Zhao, J., Wang, T., Yatskar, M., Ordonez, V., and Chang,\n",
      "K. Gender bias in coreference resolution: Evaluation and\n",
      "debiasing methods.\n",
      "arXiv preprint arXiv:1804.06876,\n",
      "2018.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ll,section_info,corpus= extract_subsections([\n",
    "                    \"../PDFs/Place_Freeze_Success_PDF.pdf\",\n",
    "                    \"../PDFs/0a6323ba-1642-45a6-9152-cd6eaaf9b4b0 (1).pdf\",\n",
    "                    \"../PDFs/Text and Code Embeddings.pdf\",\n",
    "                    ])\n",
    "\n",
    "# print('n=',ll)\n",
    "# for i in listy:\n",
    "#      print(i)\n",
    "#      print(\"\\n\")\n",
    "print(corpus)\n",
    "\n",
    "#section_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4b67d9",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## contractions dictonary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eac763e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T16:20:57.203882Z",
     "start_time": "2023-05-21T16:20:57.193766Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "contractions = {\n",
    "    \"'bout\": \"about\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"'em\": \"them\",\n",
    "    \"'til\": \"until\",\n",
    "    \"'ve\": \"have\",\n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"e'er\": \"ever\",\n",
    "    \"gonna\": \"going to\",\n",
    "    \"gotta\": \"got to\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he had\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"here's\": \"here is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I had\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it had\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"o'er\": \"over\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"she'd\": \"she had\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so is\",\n",
    "    \"that'd\": \"that had\",\n",
    "    \"that'll\": \"that will\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there had\",\n",
    "    \"there'll\": \"there will\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they had\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"this'll\": \"this will\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we had\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'd\": \"what did\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"when'd\": \"when did\",\n",
    "    \"when'll\": \"when will\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where'll\": \"where will\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"who'd\": \"who had\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"why'd\": \"why did\",\n",
    "    \"why'll\": \"why will\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"you'd\": \"you had\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "183dbfa3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T16:32:30.868547Z",
     "start_time": "2023-05-21T16:32:30.863641Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def expand_contractions(text: str, contractions: Dict[str, str]) -> str:\n",
    "  \"\"\"Expands contractions in a corpus text.\n",
    "\n",
    "  Args:\n",
    "    text: The corpus text.\n",
    "    contractions: A dictionary that maps contractions to their expanded forms.\n",
    "\n",
    "  Returns:\n",
    "    The expanded corpus text.\n",
    "  \"\"\"\n",
    "\n",
    "  expanded_text = []\n",
    "  for word in text.split():\n",
    "    if word in contractions:\n",
    "      expanded_text.append(contractions[word])\n",
    "    else:\n",
    "      expanded_text.append(word)\n",
    "\n",
    "  return \" \".join(expanded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "42e79416",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T16:33:15.245780Z",
     "start_time": "2023-05-21T16:33:15.239173Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRUCE COTTMAN 685 TECH CENTER PKWY APT 3219 Newport News, VA 23606-1552 April 24, 2023 Dear BRUCE COTTMAN: We are writing to confirm that a security freeze was placed on your Equifax credit report. Before applying for credit, you must temporarily lift or permanently remove your security freeze by: l Logging in or creating an account at www.myEquifax.com l Calling us at 1-888-298-0045 l Or sending a written request that includes your name, address and social security number to: Equifax Information Services LLC P.O. Box 105788 Atlanta, GA 30348 If you are sending your request by mail, please be sure to include copies of the following: One Government Issued ID One Item to Validate Your Address Example: Driver's License State Issued ID Card Passport Birth Certificate Example: Driver's License Utility Bill Pay Stub W2 or 1099 Form For requests to temporarily lift your security freeze, please be sure to specify the desired date range. Example: March 15, 20XX to March 21, 20XX Placing, lifting and removing a security freeze on your Equifax credit report is free. To learn how to make a security freeze request with the other two nationwide credit bureaus, visit: l TransUnion at www.transunion.com l Experian at www.experian.com Equifax Information Services LLC maintains your credit report and provides information to creditors and insurers, including credit card companies and other lenders, so that they may make pre−approved firm offers of credit as permitted by the Fair Credit Reporting Act. If you prefer not to receive such offers, you can opt out by visiting www.optoutprescreen.com or calling toll-free: 1-888-5-OPT-OUT (or 1-888-567-8688). You may also send your request in writing to: Equifax Information Services LLC P.O. Box 740123 Atlanta, GA 30374-0123 Please include your complete name, address, social security number and signature. Equifax will remove your name from its pre-approved offer database and share your request with the other two nationwide credit reporting agencies. Thank you for the opportunity to assist you. Equifax Information Services LLC 3114544049-OFZ-0bdb010200000626-04242023 000000001-FLT SARA COTTMAN Facility#3265 <image: DeviceRGB, width: 200, height: 200, bpc: 8> Here are the benefits you have elected along with your cost per paycheck. Cost for medical coverage and life insurance depend on partly on the tobacco information you provided. You have elected to cover [0] of tobacco user(s). Plan Your Choices Your Coverage Level Cost Per Paycheck Medical Walmart Premier Plan Associate Only $33.00 Vision Coverage Associate Only $2.76 Dental Coverage Associate Only $8.30 Accidental Death and Dismemberment $150,000 Associate Only $0.97 Optional Life Insurance* No Coverage $0.00 Optional Child Life Insurance No Coverage $0.00 Critical Illness Insurance No Coverage $0.00 Accident Insurance Coverage Associate Only $0.68 401(k) Enrolled Regular Savings Rate 6% Catch-Up Savings Rate 1% $68.66 Associate Stock Purchase Plan Enrolled $18.00 Current Cost Per Paycheck New Cost Per Paycheck Total cost of all your benefits choices $0.00 $132.37 _______________________________________________________________________________________________ I authorize Walmart to make payroll deductions from my wages each pay period to cover my premiums for the coverage I have elected under the Plan as well as any premiums for any automatic or default elections. Payroll deductions are taken from post-tax wages. I understand that my coverage and corresponding payroll deduction elections may not be changed during the year except as described in the Associate Benefits Book. However, if I am permitted to make changes to my coverage, my corresponding payroll deduction elections may increase or decrease, back to that effective date of such coverage. Furthermore, I understand that in the event my wages are less than my premiums, I may experience a lapse in, or cancelation of, coverage. If I am in arrears on my premiums, I authorize Walmart to make deductions from my wages to cover my unpaid premiums, up to the maximum amount allowable by law. I understand that any authorization to deduct these extra amounts will be solely for my benefit as the deductions will be used to pay premiums so I may continue to participate in the Plan. I understand that if my wages are insufficient to pay any outstanding premiums, it is my responsibility to pay those premiums to ensure my coverage under the Plan continues without interruption or cancelation. I understand that if I have elected Long-Term Disability, Optional Associate Life or Optional Spouse/Partner Life, and Proof of Good Health is required, as described in the Associate Benefits Book, payroll deductions will not begin until I have met those requirements and my coverage has been approved. This is not a guarantee of benefits. All benefits are subject to plan terms and coverage is effective only when all requirements, including, but not limited to, waiting periods, proof of good health, current premium payments, have been satisfied. See the Associate Benefits Book for more details. To make investment changes log into your 401(k) Plan account at Benefits.ML.com. *Proof of Good Health is required when you enroll after your initial enrollment period, you increase coverage or you enroll for more than the guaranteed amount. You will receive a Proof of Good Health form electronically or by mail. Once you have turned in the form and answered all additional requests for information, you will receive a letter stating if your coverage is approved. If you need assistance with accessing the form or questions about the approval, please contact Prudential at 877-740-2116. If Proof of Good Health is required, coverage will not be effective until after The Prudential Insurance Company of America approves. Premiums will not be deducted until coverage is effective. See the Associate Benefits Book for coverage effective date and more details. **Proof of Good Health is required when you enroll after your initial enrollment period or you increase coverage. You will receive a Proof of Good Health form electronically or by mail. Once you have turned in the form, you will receive a letter stating if your coverage is approved. If you need assistance with accessing the form or questions about the approval, please contact Lincoln Financial Group at 877-353-6404. Coverage will not be effective until after Lincoln Financial Group approves. Premiums will not be deducted until coverage is effective. See the Associate Benefits Book for effective date and more details. ***Disability rates are based on your income. The amount displayed reflects what your cost per paycheck would be based on your wages from last pay period. Your actual cost per paycheck will vary based on your wages at the time the deductions are taken. If you have any questions about your enrollment, please call People Services at 800-421-1362. This document is generated on : 05-11-2023 08:31:04.732000 CDT Text and Code Embeddings by Contrastive Pre-Training Arvind Neelakantan * 1 Tao Xu * 1 Raul Puri 1 Alec Radford 1 Jesse Michael Han 1 Jerry Tworek 1 Qiming Yuan 1 Nikolas Tezak 1 Jong Wook Kim 1 Chris Hallacy 1 Johannes Heidecke 1 Pranav Shyam 1 Boris Power 1 Tyna Eloundou Nekoul 1 Girish Sastry 1 Gretchen Krueger 1 David Schnurr 1 Felipe Petroski Such 1 Kenny Hsu 1 Madeleine Thompson 1 Tabarak Khan 1 Toki Sherbakov 1 Joanne Jang 1 Peter Welinder 1 Lilian Weng 1 Abstract Text embeddings are useful features in many applications such as semantic search and com- puting text similarity. Previous work typically trains models customized for different use cases, varying in dataset choice, training objective and model architecture. In this work, we show that contrastive pre-training on unsupervised data at scale leads to high quality vector representations of text and code. The same unsupervised text em- beddings that achieve new state-of-the-art results in linear-probe classiﬁcation also display impres- sive semantic search capabilities and sometimes even perform competitively with ﬁne-tuned mod- els. On linear-probe classiﬁcation accuracy aver- aging over 7 tasks, our best unsupervised model achieves a relative improvement of 4% and 1.8% over previous best unsupervised and supervised text embedding models respectively. The same text embeddings when evaluated on large-scale semantic search attains a relative improvement of 23.4%, 14.7%, and 10.6% over previous best unsupervised methods on MSMARCO, Natural Questions and TriviaQA benchmarks, respec- tively. Similarly to text embeddings, we train code embedding models on (text, code) pairs, ob- taining a 20.8% relative improvement over prior best work on code search. 1. Introduction Deep unsupervised learning with generative and embed- ding models has seen dramatic success in the past few years. Generative models (Peters et al., 2018; Raffel et al., 2019; van den Oord et al., 2016; Ramesh et al., 2021; Brown et al., 2020; Chen et al., 2021) are trained to max- *Equal contribution 1OpenAI. Correspondence to: Arvind Neelakantan <arvind@openai.com>. S-300M M-1.2B L-6B XL-175B Model Size 60 62 64 66 68 70 Performance Average performance vs model size Figure 1. Average performance of unsupervised cpt-text models of different sizes across 22 tasks consisting of linear-probe classiﬁcation, text search, and sentence similarity tasks. imize the likelihood of observed data while embedding models are trained to distinguish observed data from noise (Sohn, 2016; van den Oord et al., 2018; Radford et al., 2021; Jia et al., 2021; Gao et al., 2021; Izacard et al., 2021). Generative models have been shown to produce realistic content and beneﬁt many downstream applications, reduc- ing the need for labeled training datasets. In generative models, the information about the input is typically dis- tributed over multiple hidden states of the model. While some generative models (Kingma & Welling, 2014; Kiros et al., 2015) can learn a single representation of the in- put, most autoregressive Transformer (Vaswani et al., 2017) models do not (Raffel et al., 2019; Brown et al., 2020; Chen et al., 2021; Ramesh et al., 2021). However, learning such a representation (or embedding) is necessary for many tasks. Systems that search over millions or billions of items re- quire each entry to be embedded as a dense representation and build an index in advance to save computational costs at query time. These embeddings are useful features for classiﬁcation tasks and can also enable data visualization applications via techniques such as clustering. Embedding models are explicitly optimized to learn a low dimensional representation that captures the semantic meaning of the input (Radford et al., 2021; Jia et al., 2021; Giorgi et al., 2020; Gao et al., 2021; Izacard et al., 2021). arXiv:2201.10005v1 [cs.CL] 24 Jan 2022 Text and Code Embeddings by Contrastive Pre-Training In this work, we train embedding models using a con- trastive learning objective with in-batch negatives (Sohn, 2016; Yih et al., 2011) on unlabeled data. The input is en- coded with a Transformer encoder (Vaswani et al., 2017) and we leverage naturally occurring paired data to con- struct training data with no explicit labels. Text embedding models are trained on paired text data where we consider neighboring pieces of text on the Internet as positive pairs. Code embedding models treat the top-level docstring in a function along with its implementation as a (text, code) pair. The training signal of the contrastive objective on its own is not sufﬁcient to learn useful representations and we overcome this by initializing our model with other pre- trained models (Brown et al., 2020; Chen et al., 2021). Fi- nally, we ﬁnd that it is critical to use a sufﬁciently large batch to achieve the optimal performance. We show that this simple recipe combining pre-trained model initializa- tion, large-batch contrastive learning and training at scale, can produce text and code embeddings that possess a broad range of capabilities. We train a series of unsupervised text embedding mod- els (cpt-text) of different sizes, ranging from 300M to 175B parameters, and observe a consistent perfor- mance improvement with increasing model sizes (Figure 1). On classiﬁcation accuracy averaging across 7 linear- probe classiﬁcation tasks in SentEval (Conneau & Kiela, 2018), our largest unsupervised model achieves new state- of-the-art results with a relative improvement of 4% and 1.8% over the previous best unsupervised (Giorgi et al., 2020) and supervised (Gao et al., 2021) text embedding models, respectively. Text embedding in previous work was studied under differ- ent domains, varying in data, training objective and model architecture. Precisely, sentence embedding (Reimers & Gurevych, 2019; Gao et al., 2021; Giorgi et al., 2020) and neural information retrieval (Lee et al.; Guu et al., 2020; Karpukhin et al., 2020a; Sachan et al., 2021; Izac- ard et al., 2021) have remained different research topics evaluated on distinct benchmarks, even though both aim to learn high-quality text representation. However, we ﬁnd the same model that achieves good performance on sentence embedding benchmarks, as discussed above, is also able to obtain impressive results on large-scale information re- trieval. When evaluated on the MSMARCO passage rank- ing task (Nguyen et al., 2016) to search over 4M passages, cpt-text gets a relative improvement of 23.4% over pre- vious best unsupervised methods (Robertson, 2009). On the task of searching on 21M documents from Wikipedia, cpt-text obtains a relative improvement of 14.7%, and 10.6% over previous unsupervised methods (Izacard et al., 2021) for Natural Questions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017), respectively. On Triv- iaQA, our unsupervised method is even competitive with ﬁne-tuned models. Next, we train code embedding models (cpt-code) using the same recipe. Our models learn via (text, code) pairs, extracted from open source code. We evaluate our model on CodeSearchNet (Husain et al., 2020), a commonly used code search benchmark, where the task is to ﬁnd the most relevant code snippet given a natural language query. Our models achieve new state-of-the-art results with a 20.8% relative improvement over the previous best result (Guo et al., 2021). Unlike text embedding models, we observe no performance improvement on code search when increas- ing the number of parameters of cpt-code from 300M to 1.2B. Finally, we experiment with ﬁne-tuning our models on several supervised datasets and study the transfer learn- ing performance. When ﬁne-tuned on NLI (Natural Lan- guage Inference) datasets, we see a further boost in linear- probe classiﬁcation, outperforming the previous best trans- fer method (Gao et al., 2021) by 2.2%. On SST-2 senti- ment classiﬁcation (Socher et al., 2013), we ﬁnd that our representations are sufﬁciently descriptive that even a sim- ple k-NN classiﬁer achieves results comparable to a linear- probe classiﬁer. Interestingly, zero-shot performance with our embeddings outperforms the supervised neural network models introduced along with the release of the SST-2 dataset. We also ﬁne-tune the unsupervised model on MS- MARCO and evaluate it on a suite of zero-shot search tasks in the BEIR benchmark (Thakur et al., 2021). In the trans- fer setting, our models achieve a 5.2% relative improve- ment over previous methods (Izacard et al., 2021) and is comparable even with methods (Santhanam et al., 2021; Formal et al., 2021; Wang et al., 2020) that demand sub- stantially more computation at test time. 2. Approach Our models are trained with a contrastive objective on paired data. In this section, we present more details on the model architecture and the training objective. The training set consists of paired samples, {(xi, yi)}N i=1, where (xi, yi) corresponds to a positive example pair, indicating that xi and yi are semantically similar or contextually relevant. 2.1. Model Given a training pair (x, y), a Transformer (Vaswani et al., 2017) encoder E is used to process x and y independently. The encoder maps the input to a dense vector representa- tion or embedding (Figure 2). We insert two special token delimiters, [SOS] and [EOS], to the start and end of the input sequence respectively. The hidden state from the last layer corresponding to the special token [EOS] is consid- ered as the embedding of the input sequence. Text and Code Embeddings by Contrastive Pre-Training ENCODER <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 126, height: 74, bpc: 8> <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 64, height: 44, bpc: 8> INPUT <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 38, height: 34, bpc: 8> [EOS] <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 48, height: 50, bpc: 8> [SOS] <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 48, height: 50, bpc: 8> ENCODER <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 126, height: 74, bpc: 8> <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 64, height: 44, bpc: 8> <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 644, height: 64, bpc: 8> <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 640, height: 74, bpc: 8> <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 62, height: 54, bpc: 8> <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 730, height: 76, bpc: 8> Figure 2. The encoder E maps input x to embedding vx. Special tokens, [SOS] and [EOS], are appended to the start and end of the input sequence respectively. The last layer hidden state corresponding to the token [EOS] is extracted as the embedding of the input sequence. [EOS] ENCODER <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 126, height: 74, bpc: 8> <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 64, height: 44, bpc: 8> <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 644, height: 64, bpc: 8> <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 640, height: 74, bpc: 8> <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 62, height: 54, bpc: 8> <image: ICCBased(RGB,sRGB IEC61966-2.1), width: 730, height: 76, bpc: 8> Figure 3. The encoder E maps inputs x and y, to embeddings, vx and vy independently. The similarity score between x and y is deﬁned as the cosine similarity between these two embedding vectors. The Transformer encoder maps the input, x and y, to em- beddings, vx and vy respectively and the similarity between two inputs is quantiﬁed by the cosine similarity between their embeddings, vx and vy (Figure 3). vx = E([SOS]x ⊕ x ⊕ [EOS]x) vy = E([SOS]y ⊕ y ⊕ [EOS]y) sim(x, y) = vx · vy ∥vx∥ · ∥vy∥ where ⊕ is an operation to concatenate two strings to- gether. We found that using different delimiters leads to more stable training. For x, we use ‘[’ as [SOS]x and ‘]’ as [EOS]x, while we use ‘{’ and ‘}’ as [SOS]y and [EOS]y respectively for y. 2.2. Training Objective The paired samples in the training set are contrasted against in-batch negatives (Yih et al., 2011; Sohn, 2016). Con- trastive learning with in-batch negatives has been widely Model Parameters Embed Dimensions Batch size S 300M 1024 12288 M 1.2B 2048 6912 L 6B 4096 5896 XL 175B 12288 4976 Table 1. Batch size used to train the models of different sizes. used for unsupervised representation learning in prior work (Radford et al., 2021; Jia et al., 2021; Chen et al., 2020; Izacard et al., 2021). For each example in a mini-batch of M examples, the other (M − 1) in the batch are used as negative examples. The usage of in-batch negatives enables re-use of computation both in the forward and the backward pass making training highly efﬁcient. The logits for one batch is a M × M matrix, where each entry logit(xi, yj) is given by, logit(xi,yj) = sim(xi, yj) · exp(τ), ∀(i, j), i, j ∈ {1, 2, . . . , M} where τ is a trainable temperature parameter. Only entries on the diagonal of the matrix are considered positive examples. The ﬁnal training loss is the sum of the cross entropy losses on the row and the column direction, as described in the following numpy style pseudo code. labels = np.arange(M) l_r = cross_entropy(logits, labels, axis=0) l_c = cross_entropy(logits, labels, axis=1) loss = (l_r + l_c) / 2 We initialize our models with pre-trained generative lan- guage models. cpt-text is initialized with GPT mod- els (Brown et al., 2020) and cpt-code is initialized with Codex models (Chen et al., 2021). When ﬁne-tuning our models (Section 3), the supervised training data like NLI datasets contain explicit negative examples and they are used along with the in-batch negatives. 3. Results Our models are trained on naturally occurring paired data. cpt-text models are trained on Internet data with neigh- boring pieces of text as positive pairs for the contrastive ob- jective. The code embedding cpt-code models use (text, code) pairs extracted from open source code. As discussed in Section 3.4.1, sufﬁciently large batch size is crucial to achieve good performance with our setup. Table 1 lists the batch sizes used to train the models of different sizes. We evaluate our text embedding models on a broad range of tasks: linear-probe classiﬁcation, sentence similarity, and Text and Code Embeddings by Contrastive Pre-Training semantic search. While sentence embedding (Reimers & Gurevych, 2019; Gao et al., 2021; Giorgi et al., 2020) meth- ods report results only on embedding benchmarks and neu- ral information retrieval methods (Lee et al.; Guu et al., 2020; Karpukhin et al., 2020a; Sachan et al., 2021; Izacard et al., 2021) report results only on search benchmarks, we use the same unsupervised model across all these tasks. 3.1. Text Embedding The SentEval benchmark (Conneau & Kiela, 2018) is widely adopted to assess the quality of sentence embed- dings, consisting of a broad collection of tasks in the cate- gories of linear-probe classiﬁcation and sentence similarity, and we use the same to evaluate ours. 3.1.1. LINEAR PROBE CLASSIFICATION When evaluated on linear-probe classiﬁcation, the embed- dings are used as features to train a linear classiﬁer to solve a variety of downstream tasks. The results in Ta- ble 2 demonstrate a clear advantage of larger model sizes producing better features for improved classiﬁcation per- formance. In transfer learning setup, we ﬁne-tune unsuper- vised cpt-text models on SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018) datasets using entailment pairs as positive examples and contradiction pairs as nega- tive examples. On both unsupervised learning and transfer learning settings, we achieve state-of-the-art results. 3.1.2. ZERO-SHOT AND k-NN CLASSIFICATION In this section, we discuss results using zero-shot classiﬁ- cation and k-nearest neighbor classiﬁcation on the SST-2 binary sentiment classiﬁcation task (Socher et al., 2013). We experiment with 6B (L) cpt-text model ﬁne-tuned on NLI data for this study. In the ﬁrst zero-shot experiment, each input text is assigned with one of the two labels (‘pos- itive’, ‘negative’) based on which label has its embedding closest to the input text embedding. The performance can be further improved by prompting, where we use a simple label description, ‘this is an example of a positive/negative movie review.’, instead of a single word. This zero-shot usage of embeddings is novel compared to prior work on embeddings and it is interesting to note that our zero-shot results are better than the supervised neural network results reported along with the release of the dataset (Socher et al., 2013). In the k-NN classiﬁcation experiment, given an input text, the prediction is the majority label among 256 training examples closest to the test input in the embedding space. As shown in Table 3, the k-NN classiﬁer without any task-speciﬁc tuning of trainable parameters achieves results comparable to a linear classiﬁer. 3.1.3. SENTENCE SIMILARITY On sentence similarity tasks in SentEval, we ﬁnd that our models perform worse than previous SOTA methods (Ta- ble 4). Sentence similarity is not a completely well-deﬁned downstream task (e.g. are the sentences, ‘Jack loves Jill’ and ‘Mary loves chocolates’, similar?).1,2 For example, Goodman (1972) argue that two objects can be inﬁnitely similar or dissimilar (Vervaeke et al., 2012). A possible explanation for why our models perform better than prior work on search and classiﬁcation but not on these tasks is that our models might not be optimized for the speciﬁc def- inition used by these sentence similarity benchmarks. It is important to note that previous embedding search meth- ods do not report performance on sentence similarity tasks (Karpukhin et al., 2020a; Sachan et al., 2021; Izacard et al., 2021). More discussion on this phenomenon is presented in Section 3.4.2. 3.2. Text Search Previous work on training embedding methods for search typically requires ﬁne-tuning on a particular text search dataset (Karpukhin et al., 2020a; Sachan et al., 2021; Qu et al., 2021). It is also common to have a multi-step setup where ﬁne-tuned models rely on an expensive query and document cross-attention encoder in the ﬁnal step (Qu et al., 2021; Wang et al., 2020). In contrast, we push the limits of using a single embedding model for large-scale semantic search. 3.2.1. LARGE-SCALE SEARCH First, we evaluate our models on several large-scale text search benchmarks. MSMARCO (Nguyen et al., 2016) requires the model to search over 4M documents while Natural Questions (NQ) (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017) involve searching over 21M Wikipedia documents. We use the FAISS library (John- son et al., 2019) to build the vector indices for approximate k-nearest neighbor search. The same unsupervised model discussed previously achieves impressive performance on semantic search. Table 5 demonstrates that cpt-text outperforms prior unsupervised approaches by a big mar- gin and larger model sizes consistently lead to improved performance. Surprisingly, on TriviaQA, our model is even competitive with ﬁne-tuned models. 1https://twitter.com/yoavgo/status/ 1431299645570011142 2https://twitter.com/yoavgo/status/ 1483565266575540225?s=20 Text and Code Embeddings by Contrastive Pre-Training MR CR SUBJ MPQA SST TREC MRPC Avg. Unsupervised BERT (Devlin et al., 2019) 78.7 86.2 94.4 88.7 84.4 92.8 69.4 84.9 SimCSE (Gao et al., 2021) 84.7 88.6 95.4 87.5 89.5 95.0 72.4 87.6 DECLUTR (Giorgi et al., 2020) 85.2 90.7 95.8 88.5 90.0 93.2 74.6 88.3 cpt-text S 87.1 90.1 94.9 88.3 91.8 95.2 71.6 88.4 cpt-text M 89.0 90.9 96.7 89.6 93.9 96.6 73.6 89.9 cpt-text L 90.6 92.6 97.0 90.6 95.3 97.0 73.6 90.9 cpt-text XL 92.2 93.5 97.4 91.5 96.2 97.4 74.1 91.8 Transfer from NLI data SBERT (Reimers & Gurevych, 2019) 84.9 90.1 94.5 90.3 90.7 87.4 75.9 87.7 SimCSE (Gao et al., 2021) 88.4 92.5 95.2 90.1 93.3 93.8 77.7 90.2 cpt-text S 87.3 91.0 94.6 90.5 91.4 95.0 75.6 89.3 cpt-text M 89.8 92.7 95.7 91.3 95.3 96.6 76.5 91.1 cpt-text L 90.8 93.5 96.2 91.2 95.7 96.0 76.9 91.5 cpt-text XL 92.4 93.9 97.0 91.8 95.8 96.4 78.1 92.2 Table 2. cpt-text models of different sizes, ranging from 300M (S) to 175B (XL), are compared to previous work on linear-probe classiﬁcation tasks in SentEval. We report performance of unsupervised models, as well as those ﬁne-tuned on NLI data. Method Accuracy Zero-shot 88.1 Zero-shot with prompting 89.1 k-NN 93.3 Linear-probe 95.7 Full ﬁne-tuned SOTA 97.5 Table 3. Comparison of different classiﬁcation strategies using the 6B cpt-text model ﬁne-tuned on NLI data for SST-2 bi- nary sentiment task (Socher et al., 2013). Our zero-shot results are better than the 85.4% accuracy obtained by supervised neural networks reported along with the release of the dataset (Socher et al., 2013). STS -12 -13 -14 -15 -16 Avg Unsupervised SimCSE (Gao et al., 2021) 72.9 84.0 75.6 84.8 81.8 79.8 cpt-text S 62.1 60.0 62.0 71.8 73.7 65.9 cpt-text M 62.7 62.8 64.6 73.9 75.3 67.9 cpt-text L 62.4 66.4 67.6 76.0 77.5 70.0 cpt-text XL 64.1 67.5 68.4 76.7 78.7 71.1 Transfer from NLI SimCSE (Gao et al., 2021) 77.5 87.3 82.4 86.7 83.9 83.6 cpt-text S 72.8 80.6 78.7 84.7 82.0 79.8 cpt-text M 73.7 80.2 78.9 85.0 82.8 80.1 cpt-text L 71.8 79.7 79.0 85.8 84.0 80.1 cpt-text XL 72.3 80.3 78.9 85.1 85.1 80.3 Table 4. cpt-text performs worse than the previous best sen- tence embedding method on sentence similarity tasks. We inves- tigate this result in more detail in Section 3.4.2. MSMARCO NQ TriviaQA Fine-tuned SOTA 44.3 84.8, 89.8 84.1, 87.8 Unsupervised BM25 18.4 62.9, 78.3 76.4, 83.2 ICT - 50.9, 66.8 57.5, 73.6 MSS - 59.8, 74.9 68.2, 79.4 Contriever - 67.2, 81.3 74.2, 83.2 cpt-text S 19.9 65.5, 77.2 75.1, 81.7 cpt-text M 20.6 68.7, 79.6 78.0, 83.8 cpt-text L 21.5 73.0, 83.4 80.0, 86.8 cpt-text XL 22.7 78.8, 86.8 82.1, 86.9 Table 5. Evaluation of unsupervised cpt-text models of differ- ent sizes on several large-scale text search benchmarks. We report MRR@10 on MSMARCO and Recall@20, Recall@100 for NQ and TriviaQA as done in prior work. Results for training with Inverse Cloze Task (ICT) and masked salient spans (MSS) objec- tives are taken from Sachan et al. (2021). cpt-text achieves the best results among unsupervised methods, surpassing keyword search methods on MSMARCO (Robertson, 2009) and embed- ding based methods (Izacard et al., 2021) on NQ and TriviaQA. Text and Code Embeddings by Contrastive Pre-Training 3.2.2. BEIR SEARCH Next, we evaluate our models on 11 zero-shot search tasks in the BEIR evaluation suite (Thakur et al., 2021). First, we observe that our unsupervised model performs compet- itively even with some previous embedding methods that leverage supervised MSMARCO data (Xiong et al., 2020; Hofst¨atter et al., 2021). Keyword-based BM25 (Robertson, 2009) achieves the best results in the unsupervised setting while cpt-text achieves the best transfer learning re- sults. In the transfer setting, our models achieve a 5.2% relative improvement over the previous best embedding method (Izacard et al., 2021). It also outperforms docT5query (Nogueira et al., 2019a) that relies on a ﬁne-tuned T5 model (Raffel et al., 2019) for document expansion. cpt-text results are competitive even with methods that use sub- stantially more compute at test time. BM25+CE (Wang et al., 2020) uses keyword search to select top 100 docu- ments which are then re-ranked by a cross-attention neural network encoder. The ranking encoder network performs computationally expensive joint query and document atten- tion and cannot exploit indexing and approximate nearest neighbor algorithms for fast and efﬁcient search at query time. Several other existing work take this approach of leveraging more computation resources at query time to ob- tain better search performance. ColBERT v2 (Santhanam et al., 2021) is a multi-vector method that represents the query and the documents as a set of vectors, and employs a multi-step retrieval procedure to obtain relevant docu- ments. Splade v2 (Formal et al., 2021) represents queries and documents as sparse vectors of size equivalent to the vocabulary of the BERT encoder (Devlin et al., 2019). Our cpt-text models compute only one dense embedding per document which are indexed ofﬂine and does not de- pend on any cross-attention re-ranker at query time. 3.3. Code Search We evaluate our code embedding models on the code search task using the CodeSearchNet benchmark (Husain et al., 2020). Given a natural language query, the model is expected to retrieve the relevant code block among 1K candidates. The models are evaluated on 6 programming languages and our model achieves state-of-the-art results (Table 7). Unlike with text embeddings, we do not see a performance improvement with increased model size for code embeddings. We also evaluate on a harder setting of ﬁnding the relevant code block among 10K candidates instead of 1K. Here, we compare the performance of cpt-text models against cpt-code models (Table 8). It is interesting to see that text embedding performs fairly well in code search espe- cially in Python. We see a drop in performance for code embedding models with increased distractors and still don’t see bigger models giving a boost in search performance. 3.4. Analysis 3.4.1. EFFECT OF BATCH SIZE Our ablation study highlights the effect of the model’s batch size on the ﬁnal performance. Table 9 compares the performance of S (300M) cpt-text model trained with different batch sizes on the NQ development set. Since we train with in-batch negatives, a larger batch increases the chances of having hard negatives in a batch, resulting in a signiﬁcant performance boost. 3.4.2. TRAINING BEHAVIOR We observe that as we train our models for longer, the performance on search and classiﬁcation tasks increases while the performance on sentence similarity tasks de- creases (Figure 4). As discussed previously, sentence simi- larity is not a well deﬁned task. A hypothesis is that search tasks and sentence similarity tasks might have contradict- ing deﬁnitions. For example, a sentence and its negation could be considered as relevant during search, but not “sim- ilar” in sentence similarity tasks. It is also important to note that previous embedding search methods do not report performance on sentence similarity tasks (Karpukhin et al., 2020a; Sachan et al., 2021; Izacard et al., 2021) and previ- ous sentence embedding methods do not evaluate on search tasks (Reimers & Gurevych, 2019; Giorgi et al., 2020; Gao et al., 2021). When deciding the model checkpoints to use for evaluation, we assigned higher importance to search and classiﬁcation tasks as they are commonly associated with clearly deﬁned real-world applications while sentence similarity tasks are less so. 4. Related Work The goal of representation learning (Bengio et al., 2012) is to learn an embedding space in which similar examples stay close to each other while dissimilar ones are far apart (Hadsell et al., 2006). In contrastive learning, the learning procedure is formulated as a classiﬁcation problem given similar and dissimilar candidates (Chopra et al., 2005; Gut- mann & Hyv¨arinen, 2010; Schroff et al., 2015; Sohn, 2016; van den Oord et al., 2018). Recent work relies on con- trastive objective to learn representations for images (Wu et al., 2018; He et al., 2020; Chen et al., 2020; Zbontar et al., 2021), text, or both jointly (Lu et al., 2019; Sun et al., 2019; Kim et al., 2021; Radford et al., 2021; Khosla et al., 2020). In self-supervised contrastive learning, pos- itive samples can be collected in various approaches in- cluding by creating an augmented version of the origi- nal input without modifying the semantic meaning (Gao Text and Code Embeddings by Contrastive Pre-Training covid nfc ﬁqa arg. touche quora scifact climate dbp. hotpot fever Avg. Unsupervised BM25 (Robertson, 2009) 65.6 32.5 23.6 31.5 36.7 78.9 66.5 21.3 31.3 60.3 75.3 47.6 Contriever (Izacard et al., 2021) 27.4 31.7 24.5 37.9 19.3 83.5 64.9 15.5 29.2 48.1 68.2 40.9 cpt-text S 52.9 32.0 34.1 38.7 21.0 68.1 65.4 15.8 27.2 51.5 57.1 42.2 cpt-text M 44.3 34.5 37.3 41.2 23.3 70.3 68.3 15.6 29.6 53.0 58.2 43.2 cpt-text L 42.7 36.9 39.7 39.2 22.8 68.7 71.2 16.1 31.2 54.3 63.8 44.2 Transfer from MSMARCO TAS-B (Hofst¨atter et al., 2021) 48.1 31.9 30.0 42.9 16.2 83.5 64.3 22.8 38.4 58.4 70.0 46.0 ANCE (Xiong et al., 2020) 65.4 23.7 29.5 41.5 24.0 85.2 50.7 19.8 28.1 45.6 66.9 43.7 Contriever (Izacard et al., 2021) 59.6 32.8 32.9 44.6 23.0 86.5 67.7 23.7 41.3 63.8 75.8 50.2 cpt-text S 67.9 33.2 38.4 47.0 28.5 70.6 67.2 18.5 36.2 59.4 72.1 49.0 cpt-text M 58.5 36.7 42.2 49.2 29.7 69.7 70.4 19.9 38.6 63.1 77.0 50.5 cpt-text L 56.2 38.0 45.2 46.9 30.9 67.7 74.4 19.4 41.2 64.8 75.6 50.9 cpt-text XL 64.9 40.7 51.2 43.5 29.1 63.8 75.4 22.3 43.2 68.8 77.5 52.8 docT5query (Nogueira et al., 2019a) 71.3 32.8 29.1 34.9 34.7 80.2 67.5 20.1 33.1 58.0 71.4 48.5 BM25+CE (Wang et al., 2020) 75.7 35.0 34.7 31.1 27.1 82.5 68.8 25.3 39.2 70.7 81.9 52.0 ColBERT v2 (Santhanam et al., 2021) 73.8 33.8 35.6 46.3 26.3 85.2 69.3 17.6 44.6 66.7 78.5 52.5 Splade v2 (Formal et al., 2021) 71.0 33.4 33.6 47.9 27.2 83.8 69.3 23.5 43.5 68.4 78.6 52.7 Table 6. Comparison of cpt-text to previous methods on 11 zero-shot search tasks in the BEIR evaluation suite (Thakur et al., 2021). Results are reported both in the unsupervised data setting and in the transfer data setting. cpt-text outperforms previous best embedding methods (Xiong et al., 2020; Hofst¨atter et al., 2021; Izacard et al., 2021) in both the settings. In the unsupervised setting, BM25 (Robertson, 2009) still achieves the best performance while in the transfer setting cpt-text is competitive with methods that use substantially more compute at test time (Wang et al., 2020; Santhanam et al., 2021; Formal et al., 2021). Go Ruby Python Java JS PHP Avg. CodeBERT 69.3 70.6 84.0 86.8 74.8 70.6 76.0 GraphCodeBERT 84.1 73.2 87.9 75.7 71.1 72.5 77.4 cpt-code S 97.7 86.3 99.8 94.0 86.0 96.7 93.4 cpt-code M 97.5 85.5 99.9 94.4 86.5 97.2 93.5 Table 7. Comparison of cpt-code on code search across 6 pro- gramming languages (Husain et al., 2020) with CodeBERT (Feng et al., 2020) and GraphCodeBERT (Guo et al., 2021). The task re- quires ﬁnding the relevant code block among 1K candidates for a given natural language query. cpt-code performs substantially better than previous methods on all the languages. Go Ruby Python Java JS PHP Avg. cpt-text S 60.6 58.9 92.6 48.4 52.8 47.6 60.1 cpt-text M 65.4 63.1 91.4 47.9 53.5 43.1 60.7 cpt-code S 90.4 80.6 98.8 81.9 76.1 85.3 85.5 cpt-code M 90.0 89.1 98.9 81.1 75.6 85.1 85.0 Table 8. Comparison of cpt-code vs cpt-text on large scale code search (Husain et al., 2020). The task is to retrieve the rel- evant code block among 10K candidates for a given natural lan- guage query. It is interesting to note that cpt-text performs quite well on Python code search without explicitly training on (text, code) pairs. Batch Size MRR@10 1536 71.4 12288 84.7 Table 9. Performance of the cpt-text 300M model on NQ dev set given different training batch sizes. et al., 2021), by grouping samples within the same context (Giorgi et al., 2020; Izacard et al., 2021), or by collecting data about the same object from different views (Tian et al., 2019). Learning word embeddings is a well studied research area (Brown et al., 1992; Gutmann & Hyv¨arinen, 2010; Mikolov et al., 2013; Pennington et al., 2014). Learning low- dimensional representations of larger text pieces, denser than raw term-based vectors, has been studied extensively as well (Deerwester et al., 1990; Yih et al., 2011). Most of the recent models for learning sentence embeddings rely on supervised NLI datasets, using entailment pairs as pos- itive examples and contradiction pairs as (hard) negatives. SBERT (Reimers & Gurevych, 2019) trained a siamese net- work to learn a representation where sentence similarity is estimated by the cosine similarity between embeddings. Li et al. (2020) improves the embedding space to be isotropic Text and Code Embeddings by Contrastive Pre-Training 85 86 87 88 89 90 Performance Senteval NQ 0 10000 20000 30000 40000 50000 Training Steps 65 70 75 Performance sts12 sts13 sts14 sts15 sts16 Training Behavior Figure 4. Performance of M (1.2B) cpt-text model on classi- ﬁcation, search and sentence similarity tasks at different training steps. While the performance on search and classiﬁcation im- proves with longer training, the performance on sentence similar- ity degrades. via normalizing ﬂows. The whitening operation is another alternative operation to improve the isotropy of the embed- ding space (Su et al., 2021). It is typical to initialize such models with a pre-trained language model (Devlin et al., 2019) before training on NLI datasets. Several methods have been studied for unsupervised or self-supervised sentence embedding learning (Logeswaran & Lee, 2018; Zhang et al., 2020; Gao et al., 2021). Com- mon approaches consider sentences within the same con- text as semantically similar samples (Kiros et al., 2015; Lo- geswaran & Lee, 2018). To create positive training pairs with augmented samples, a diverse set of text augmen- tation operations have been explored, including lexicon- based distortion (Wei & Zou, 2019), synonym replacement (Kobayashi, 2018), back-translation (Fang & Xie, 2020), cut-off (Shen et al., 2020) and dropout (Gao et al., 2021). However, unsupervised sentence embedding models still perform notably worse than supervised sentence encoders. Large-scale text search based on dense embeddings and neural information retrieval (neural IR) have the poten- tial to generalize better than keyword matching in classic IR systems. Neural IR systems encode documents at the indexing stage and then perform nearest neighbor search (Johnson et al., 2019) at query time (Lin et al., 2021). Neural IR models are usually learned by ﬁne-tuning a pre- trained language model on supervised search corpus (Lee et al.; Guu et al., 2020; Karpukhin et al., 2020b; Lewis et al., 2020). Many SOTA search models combine classical IR with neural IR in a staged setup, where the candidates are ﬁrst narrowed down by BM25 keyword search (Robert- son, 2009) and then re-ranked by joint query and document neural encoders (Nogueira et al., 2019b; Qu et al., 2021). Xiong et al. (2020) proposed ANCE, a contrastive learn- ing framework for learning text representations for dense retrieval using mined hard negatives. Other unsupervised retriever methods use the Inverse Cloze Task or masked salient spans to achieve signiﬁcant improvement on ODQA tasks (Sachan et al., 2021). In comparison to most prior work, we ﬁnd that with a large enough batch size, it is possible to achieve good search performance without us- ing supervised data. Finally, the recently published Con- triever (Izacard et al., 2021) is most similar to our work on learning text embeddings for text search using contrastive learning on unlabeled data. Semantic code search refers to the task of retrieving code relevant to a query in natural language. The CodeSearch- Net challenge (Husain et al., 2020) presents a set of bench- mark code search tasks in different programming lan- guages, as well as a simple baseline model to predict em- beddings of query and code via contrastive learning on a dataset of (text, code) pairs. ContraCode (Jain et al., 2021) uses a contrastive learning task of identifying functionally similar programs, where the functionally similar samples are generated via source-to-source compiler transforma- tions. CodeBERT (Feng et al., 2020) learns to predict se- mantic similarity with a pre-trained language model and GraphCodeBERT (Guo et al., 2021) further improves the performance on the CodeSearchNet benchmark by adding pre-training tasks on code structure. 5. Broader Impacts Prior research has shown that text representation models encode the biases present in their training data, including those which are discriminatory towards protected groups such as Black people or women (Bolukbasi et al., 2016; Caliskan et al., 2017; May et al., 2019; Zhao et al., 2018; Rudinger et al., 2018). Biases encoded in embedding mod- els may cause representational harms3 by reinforcing exis- tent societal biases in the text corpus, and further propagat- ing them in downstream tasks of embedding models. Therefore, we encourage further research on two research agendas: (a) developing robust evaluation methodologies for multiple classes of bias in training data and pre-trained models, and (b) developing and improving methods for mitigating encoded bias, including ﬁne-tuning to reduce bias in pre-trained models (Caliskan et al., 2017; May et al., 2019; Bolukbasi et al., 2016; Liang et al., 2020; Park et al., 2018; Solaiman & Dennison, 2021). Until we have robust evaluation methodology, it is important to restrict and mon- itor the use of the model in downstream applications. Par- 3Representational harms occur when systems reinforce the subordination of some groups along the lines of identity, e.g. stereotyping or denigration (Crawford, 2017). Text and Code Embeddings by Contrastive Pre-Training ticularly for those where risk of representational harm is great and those where biased representations may inﬂuence the allocation of resources and opportunities to people. Our embedding models are trained with large batch sizes and require substantial computation resources. While this training regime is environmentally and computationally costly, there are promising paths forward to amortize and offset these costs while allowing users to beneﬁts from the capabilities of these models. For example, safe public access to large pre-trained language models, and efﬁcient training pipelines that leverage improved model architec- tures and training schemes. We encourage further research and implementation efforts in these areas. 6. Conclusion We showed that contrastive pre-training on unsupervised data with a sufﬁciently large batch size can lead to high quality vector representations of text and code. Our models achieved new state-of-the-art results in linear-probe classi- ﬁcation, text search and code search. We ﬁnd that our mod- els underperformed on sentence similarity tasks and ob- served unexpected training behavior with respect to these tasks. Finally, we discussed the broader impact of our work on society. References Bengio, Y., Courville, A. C., and Vincent, P. Representa- tion learning: A review and new perspectives. Transac- tions on pattern analysis and machine intelligence, 35 (8), 2012. Bolukbasi, T., Chang, K., Zou, J. Y., Saligrama, V., and Kalai, A. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. 29, 2016. Bowman, S. R., Angeli, G., Potts, C., and Manning, C. D. A large annotated corpus for learning natural language inference. In Conference on Empirical Methods in Nat- ural Language Processing (EMNLP). ACL, 2015. Brown, P. F., Della Pietra, V. J., deSouza, P. V., Lai, J. C., and Mercer, R. L. Class-based n-gram models of nat- ural language. Computational Linguistics, 18(4):467– 480, 1992. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Advances in Neural Information Processing Systems, 2020. Caliskan, A., Bryson, J. J., and Narayanan, A. Seman- tics derived automatically from language corpora contain human-like biases. Science, 356(6334):183–186, 2017. Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavar- ian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. E. A simple framework for contrastive learning of visual representations. In International conference on machine learning (ICML), 2020. Chopra, S., Hadsell, R., and LeCun, Y. Learning a similar- ity metric discriminatively, with application to face ver- iﬁcation. In Computer Vision and Pattern Recognition (CVPR). IEEE, 2005. Conneau, A. and Kiela, D. Senteval: An evaluation toolkit for universal sentence representations. arXiv preprint arXiv:1803.05449, 2018. Crawford, K. The trouble with bias. Keynote at NeurIPS, 2017. Deerwester, S., Dumais, S., Furnas, G., Landauer, T., and Harshman, R. Indexing by latent semantic analysis. Journal of the American society for information science, 41(6):391–407, 1990. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL). ACL, 2019. Fang, H. and Xie, P. CERT: contrastive self-supervised learning for language understanding. arXiv preprint arXiv:2005.12766, 2020. Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., Qin, B., Liu, T., Jiang, D., and Zhou, M. Code- bert: A pre-trained model for programming and natural Text and Code Embeddings by Contrastive Pre-Training languages. In Conference on Empirical Methods in Nat- ural Language Processing (EMNLP), 2020. Formal, T., Lassance, C., Piwowarski, B., and Clinchant, S. SPLADE v2: Sparse lexical and expansion model for information retrieval. arXiv preprint arXiv:2109.10086, 2021. Gao, T., Yao, X., and Chen, D. SimCSE: Simple con- trastive learning of sentence embeddings. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2021. Giorgi, J. M., Nitski, O., Bader, G. D., and Wang, B. De- clutr: Deep contrastive learning for unsupervised textual representations. In Proceedings of ACL/IJCNLP, 2020. Goodman, N. Seven strictures on similarity. Bobbs Merrill, 1972. Guo, D., Ren, S., Lu, S., Feng, Z., Tang, D., Liu, S., Zhou, L., Duan, N., Svyatkovskiy, A., Fu, S., Tufano, M., Deng, S. K., Clement, C. B., Drain, D., Sundaresan, N., Yin, J., Jiang, D., and Zhou, M. Graphcodebert: Pre- training code representations with data ﬂow. In Interna- tional Conference on Learning Representation (ICLR), 2021. Gutmann, M. and Hyv¨arinen, A. Noise-contrastive estima- tion: A new estimation principle for unnormalized sta- tistical models. In Conference on Artiﬁcial Intelligence and Statistics. PMLR, 2010. Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M. REALM: retrieval-augmented language model pre- training. arXiv preprint arXiv:2002.08909, 2020. Hadsell, R., Chopra, S., and LeCun, Y. Dimensionality reduction by learning an invariant mapping. In Computer Vision and Pattern Recognition (CVPR), volume 2, pp. 1735–1742. IEEE, 2006. He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. B. Mo- mentum contrast for unsupervised visual representation learning. In Computer Vision and Pattern Recognition (CVPR), 2020. Hofst¨atter, S., Lin, S., Yang, J., Lin, J., and Hanbury, A. Efﬁciently teaching an effective dense retriever with balanced topic aware sampling. arXiv preprint arXiv:2104.06967, 2021. Husain, H., Wu, H.-H., Gazit, T., Allamanis, M., and Brockschmidt, M. CodeSearchNet challenge: Evaluat- ing the state of semantic code search. arXiv preprint arXiv:1909.09436, 2020. Izacard, G., Caron, M., Hosseini, L., Riedel, S., Bo- janowski, P., Joulin, A., and Grave, E. Towards unsuper- vised dense information retrieval with contrastive learn- ing. arXiv preprint arXiv:2112.09118, 2021. Jain, P., Jain, A., Zhang, T., Abbeel, P., Gonzalez, J. E., and Stoica, I. Contrastive code representation learning. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2021. Jia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H., Le, Q. V., Sung, Y., Li, Z., and Duerig, T. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning (ICML), 2021. Johnson, J., Douze, M., and J´egou, H. Billion-scale simi- larity search with gpus. IEEE Transactions on Big Data, 2019. Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Conference of the Association for Computational Linguistics (ACL). ACL, 2017. Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W.-t. Dense passage retrieval for open-domain question answering. In Con- ference on Empirical Methods in Natural Language Pro- cessing (EMNLP), 2020a. Karpukhin, V., Oguz, B., Min, S., Wu, L., Edunov, S., Chen, D., and Yih, W. Dense passage retrieval for open- domain question answering. In Conference on Empiri- cal Methods in Natural Language Processing (EMNLP), 2020b. Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., Maschinot, A., Liu, C., and Krishnan, D. Supervised contrastive learning. arXiv preprint arXiv:2004.11362, 2020. Kim, W., Son, B., and Kim, I. Vilt: Vision-and- language transformer without convolution or region su- pervision. In International Conference on Machine Learning (ICML), 2021. Kingma, D. P. and Welling, M. Auto-Encoding Variational Bayes. In International Conference on Learning Repre- sentation (ICLR), 2014. Kiros, J., Zhu, Y., Salakhutdinov, R., Zemel, R. S., Tor- ralba, A., Urtasun, R., and Fidler, S. Skip-thought vec- tors. In Advances in Neural Information Processing Sys- tems (NeuriPS), 2015. Text and Code Embeddings by Contrastive Pre-Training Kobayashi, S. Contextual augmentation: Data augmen- tation by words with paradigmatic relations. arXiv preprint arXiv:1805.06201, 2018. Kwiatkowski, T., Palomaki, J., Redﬁeld, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Kel- cey, M., Devlin, J., Lee, K., Toutanova, K. N., Jones, L., Chang, M.-W., Dai, A., Uszkoreit, J., Le, Q., and Petrov, S. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics, 2019. Lee, K., Chang, M., and Toutanova, K. Latent retrieval for weakly supervised open domain question answering. In Korhonen, A., Traum, D. R., and M`arquez, L. (eds.), Conference of the Association for Computational Lin- guistics (ACL), pp. 6086–6096. ACL. Lewis, P. S. H., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., K¨uttler, H., Lewis, M., Yih, W., Rockt¨aschel, T., Riedel, S., and Kiela, D. Retrieval- augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems (NeuriPS), 2020. Li, B., Zhou, H., He, J., Wang, M., Yang, Y., and Li, L. On the sentence embeddings from pre-trained language models. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020. Liang, P. P., Li, I. M., Zheng, E., Lim, Y. C., Salakhutdinov, R., and Morency, L. Towards debiasing sentence repre- sentations. In Conference of the Association for Compu- tational Linguistics (ACL), 2020. Lin, J., Nogueira, R., and Yates, A. Pretrained transformers for text ranking: BERT and beyond. Synthesis Lectures on Human Language Technologies, 14(4):1–325, 2021. Logeswaran, L. and Lee, H. An efﬁcient framework for learning sentence representations. In International Con- ference on Learning Representation (ICLR), 2018. Lu, J., Batra, D., Parikh, D., and Lee, S. Vil- bert: Pretraining task-agnostic visiolinguistic represen- tations for vision-and-language tasks. arXiv preprint arXiv:1908.02265, 2019. May, C., Wang, A., Bordia, S., Bowman, S. R., and Rudinger, R. On measuring social biases in sentence en- coders. In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2019. Mikolov, T., Chen, K., Corrado, G. S., and Dean, J. Efﬁ- cient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013. Nguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Majumder, R., and Deng, L. MS MARCO: A hu- man generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268, 2016. Nogueira, R., Lin, J., and Epistemic, A. From doc2query to doctttttquery. Online preprint, 2019a. Nogueira, R., Yang, W., Cho, K., and Lin, J. Multi- stage document ranking with BERT. arXiv preprint arXiv:1910.14424, 2019b. Park, J. H., Shin, J., and Fung, P. Reducing gender bias in abusive language detection. In Conference on Empiri- cal Methods in Natural Language Processing (EMNLP), 2018. Pennington, J., Socher, R., and Manning, C. GloVe: Global vectors for word representation. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014. Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. Deep con- textualized word representations. In Proceedings of NCAAL/IJCNLP, 2018. Qu, Y., Ding, Y., Liu, J., Liu, K., Ren, R., Zhao, X., Dong, D., Wu, H., and Wang, H. Rocketqa: An optimized train- ing approach to dense passage retrieval for open-domain question answering. In Conference of the Association for Computational Linguistics (ACL), 2021. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019. Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Rad- ford, A., Chen, M., and Sutskever, I. Zero-shot text- to-image generation. arXiv preprint arXiv:2102.12092, 2021. Reimers, N. and Gurevych, I. Sentence-bert: Sentence em- beddings using siamese bert-networks. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2019. Robertson, S. The Probabilistic Relevance Framework: BM25 and Beyond. Foundations and Trends® in Infor- mation Retrieval, 2009. Text and Code Embeddings by Contrastive Pre-Training Rudinger, R., Naradowsky, J., Leonard, B., and Durme, B. V. Gender bias in coreference resolution. arXiv preprint arXiv:1804.09301, 2018. Sachan, D. S., Patwary, M., Shoeybi, M., Kant, N., Ping, W., Hamilton, W. L., and Catanzaro, B. End-to-end training of neural retrievers for open-domain question answering. In Zong, C., Xia, F., Li, W., and Navigli, R. (eds.), Proceedings of ACL/IJCNLP, pp. 6648–6662. ACL, 2021. Santhanam, K., Khattab, O., Saad-Falcon, J., Potts, C., and Zaharia, M. Colbertv2: Effective and efﬁcient re- trieval via lightweight late interaction. arXiv preprint arXiv:2112.01488, 2021. Schroff, F., Kalenichenko, D., and Philbin, J. Facenet: A uniﬁed embedding for face recognition and clustering. In Computer Vision and Pattern Recognition (CVPR), 2015. Shen, D., Zheng, M., Shen, Y., Qu, Y., and Chen, W. A simple but tough-to-beat data augmentation approach for natural language understanding and generation. arXiv preprint arXiv:2009.13818, 2020. Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A., and Potts, C. Recursive deep models for semantic compositionality over a sentiment treebank. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2013. Sohn, K. Improved deep metric learning with multi-class n-pair loss objective. In Advances in Neural Information Processing Systems (NeuriPS), 2016. Solaiman, I. and Dennison, C. Process for adapting lan- guage models to society (PALMS) with values-targeted datasets. arXiv preprint arXiv:2106.10328, 2021. Su, J., Cao, J., Liu, W., and Ou, Y. Whitening sentence representations for better semantics and faster retrieval. arXiv preprint arXiv:2103.15316, 2021. Sun, C., Myers, A., Vondrick, C., Murphy, K., and Schmid, C. Videobert: A joint model for video and language representation learning. In International Conference on Computer Vision (ICCV), 2019. Thakur, N., Reimers, N., R¨uckl´e, A., Srivastava, A., and Gurevych, I. BEIR: A heterogenous benchmark for zero-shot evaluation of information retrieval models. In Advances in Neural Information Processing Systems (NeuriPS), 2021. Tian, Y., Krishnan, D., and Isola, P. Contrastive multi- view coding. European Conference on Computer Vision (ECCV), 2019. van den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A., and Kavukcuoglu, K. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499, 2016. van den Oord, A., Li, Y., and Vinyals, O. Representa- tion learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Atten- tion is all you need. In Advances in Neural Information Processing Systems (NeuriPS), 2017. Vervaeke, J., Lillicrap, T. P., and Richards, B. A. Relevance realization and the emerging framework in cognitive sci- ence. Journal of logic and computation, 22(1):79–99, 2012. Wang, W., Wei, F., Dong, L., Bao, H., Yang, N., and Zhou, M. Minilm: Deep self-attention distillation for task- agnostic compression of pre-trained transformers. arXiv preprint arXiv:2002.10957, 2020. Wei, J. W. and Zou, K. EDA: easy data augmentation tech- niques for boosting performance on text classiﬁcation tasks. arXiv preprint arXiv:1901.11196, 2019. Williams, A., Nangia, N., and Bowman, S. A broad- coverage challenge corpus for sentence understanding through inference. In Conference of the North American Chapter of the Association for Computational Linguis- tics (NAACL). ACL, 2018. Wu, Z., Xiong, Y., Yu, S. X., and Lin, D. Unsupervised feature learning via non-parametric instance-level dis- crimination. In Computer Vision and Pattern Recogni- tion (CVPR), 2018. Xiong, L., Xiong, C., Li, Y., Tang, K., Liu, J., Bennett, P. N., Ahmed, J., and Overwijk, A. Approximate near- est neighbor negative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808, 2020. Yih, W.-t., Toutanova, K., Platt, J. C., and Meek, C. Learn- ing discriminative projections for text similarity mea- sures. In Conference on Computational Natural Lan- guage Learning (CoNLL). ACL, 2011. Zbontar, J., Jing, L., Misra, I., LeCun, Y., and Deny, S. Bar- low twins: Self-supervised learning via redundancy re- duction. In International Conference on Machine Learn- ing (ICML), 2021. Zhang, Y., He, R., Liu, Z., Lim, K. H., and Bing, L. An unsupervised sentence embedding method by mutual information maximization. In Conference on Empiri- cal Methods in Natural Language Processing (EMNLP), 2020. Text and Code Embeddings by Contrastive Pre-Training Zhao, J., Wang, T., Yatskar, M., Ordonez, V., and Chang, K. Gender bias in coreference resolution: Evaluation and debiasing methods. arXiv preprint arXiv:1804.06876, 2018.\n"
     ]
    }
   ],
   "source": [
    "print(expand_contractions(corpus,contractions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c20c6b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## various text cleaners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ee81105f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T19:47:38.105932Z",
     "start_time": "2023-05-21T19:47:36.768914Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import spacy\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "from googletrans import Translator\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def remove_numbers(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes numbers from the input text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Text from which to remove numbers.\n",
    "\n",
    "    Returns:\n",
    "        str: Text without numbers.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return ''.join([token.text for token in doc if not token.is_digit])\n",
    "\n",
    "\n",
    "def remove_urls(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes URLs from the input text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Text from which to remove URLs.\n",
    "\n",
    "    Returns:\n",
    "        str: Text without URLs.\n",
    "    \"\"\"\n",
    "    return re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "\n",
    "def remove_emails(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes email addresses from the input text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Text from which to remove email addresses.\n",
    "\n",
    "    Returns:\n",
    "        str: Text without email addresses.\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "def remove_excess_whitespace(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes excess whitespace from a text string.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to clean.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with excess whitespace removed.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    cleaned_text = \" \".join(token.text for token in doc if not token.is_space)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def correct_spelling(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Corrects the spelling of a text string.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to correct.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with corrected spelling.\n",
    "    \"\"\"\n",
    "    corrected_text = TextBlob(text).correct()\n",
    "    return str(corrected_text)\n",
    "\n",
    "def translate_text(text: str, dest_language: str = 'en') -> str:\n",
    "    \"\"\"\n",
    "    Translates a text string to a specified language.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to translate.\n",
    "        dest_language (str, optional): The language to translate to. Defaults to 'en'.\n",
    "\n",
    "    Returns:\n",
    "        str: The translated text.\n",
    "    \"\"\"\n",
    "    translator = Translator()\n",
    "    translated_text = translator.translate(text, dest=dest_language)\n",
    "    return translated_text.text\n",
    "\n",
    "\n",
    "def remove_phone_numbers(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes phone numbers from a text string using regular expressions.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to clean.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with phone numbers removed.\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\b\\d{10}\\b|\\b\\d{3}-\\d{3}-\\d{4}\\b|\\(\\d{3}\\)\\s*\\d{3}-\\d{4}\\b', '', text)\n",
    "\n",
    "def remove_html_tags(text: str) -> str:\n",
    "  \"\"\"\n",
    "  Removes HTML tags from a text string using regular expressions.\n",
    "\n",
    "  Args:\n",
    "    text (str): The text to clean.\n",
    "\n",
    "  Returns:\n",
    "    str: The text with HTML tags removed.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: If the input text is not a string.\n",
    "\n",
    "  Note: \n",
    "    The remove_html_tags function uses a basic regular \n",
    "    expression to remove HTML tags from the text. \n",
    "    This should work for most HTML, but could fail \n",
    "    in some edge cases (such as comments or scripts embedded within the HTML). \n",
    "    For more complex HTML parsing and cleaning, \n",
    "    consider using a library designed for that purpose, like BeautifulSoup.\n",
    "  \"\"\"\n",
    "\n",
    "  if not isinstance(text, str):\n",
    "    raise ValueError(\"The input text must be a string.\")\n",
    "\n",
    "  return re.sub(r\"<[^>]+>\", \"\", text)\n",
    "\n",
    "def to_lowercase(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts the input text to lowercase.\n",
    "\n",
    "    Args:\n",
    "        text (str): Text to convert to lowercase.\n",
    "\n",
    "    Returns:\n",
    "        str: Text in lowercase.\n",
    "    \"\"\"\n",
    "    return text.lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139f0b7f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Your task is to break down strings that are longer than 1500 words\n",
    "into sub-strings where each string is less than 1500 words. Here's a way to do that by using nltk.tokenize to split the text into words:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eca9f139",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T20:56:30.319150Z",
     "start_time": "2023-05-21T20:56:30.312913Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from typing import List, Union\n",
    "\n",
    "def break_down_long_strings(text_list: List[str]) -> List[Union[str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Breaks down strings that are longer than 1500 words into sub-strings\n",
    "    where each string is less than 1500 words.\n",
    "\n",
    "    Args:\n",
    "        text_list (List[str]): A list of strings.\n",
    "\n",
    "    Returns:\n",
    "        List[Union[str, List[str]]]: A list of strings and lists of strings\n",
    "        (for the original strings that were longer than 1500 words).\n",
    "    \"\"\"\n",
    "    new_list = []\n",
    "    \n",
    "    for text in text_list:\n",
    "        words = nltk.word_tokenize(text)\n",
    "        \n",
    "        if len(words) <= 1500:\n",
    "            new_list.append(text)\n",
    "        else:\n",
    "            sub_text_list = []\n",
    "            for i in range(0, len(words), 1500):\n",
    "                sub_text = \" \".join(words[i: i + 1500])\n",
    "                sub_text_list.append(sub_text)\n",
    "            new_list.append(sub_text_list)\n",
    "\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023c8052",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3507ef21",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1912237b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660a3cc5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17445ed1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7b191e",
   "metadata": {},
   "source": [
    "## argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2cb7d8",
   "metadata": {},
   "source": [
    "Please note that each file path should not contain spaces, or if they do, the paths should be quoted, like -pdf \"file 1.pdf\" \"file 2.pdf\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c136723f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T18:31:32.851200Z",
     "start_time": "2023-05-20T18:31:32.847114Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse \n",
    "def get_args():\n",
    "    # Create the parser\n",
    "    parser = argparse.ArgumentParser(description=\"This is our description\")\n",
    "\n",
    "    # Add 'pdf-files' argument\n",
    "    parser.add_argument('-pdf', '--pdf-files', nargs='*', type=str, help='pdf file path')\n",
    "\n",
    "    # Parse the arguments\n",
    "    args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "510e2140",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T18:41:45.463559Z",
     "start_time": "2023-05-20T18:41:45.456442Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-pdf [PDF_FILES ...]]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/brucecottman/Library/Jupyter/runtime/kernel-862b326d-adad-4078-88c5-a3e8735207e8.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "parser = get_args()\n",
    "parser.add_argument('-h', type=str, help='The name of the person.')\n",
    "\n",
    "args = parser.parse_args()\n",
    "#args = parser.parse_args('-pdf \"../PDFs/Place_Freeze_Success_PDF.pdf\"')\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c9f88ff3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T18:45:20.628015Z",
     "start_time": "2023-05-20T18:45:20.619814Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--name NAME]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/brucecottman/Library/Jupyter/runtime/kernel-862b326d-adad-4078-88c5-a3e8735207e8.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--name', type=str, help='The name of the person.')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(f'Hello, {args.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ea7fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92b67e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf0f133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7548ad79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102d618b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e1ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
